#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:nil todo:t |:t
#+title: Evaluación LLM
#+date: 2025-01-14
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export: biblatex
#+latex_class: article
#+latex_class_options:
#+latex_header:
#+latex_header_extra:
#+description:
#+keywords:
#+subtitle:
#+latex_footnote_command: \footnote{%s%s}
#+latex_engraved_theme:
#+latex_compiler: pdflatex
#+bibliography: bibliography.bib
#+LATEX_HEADER: \usepackage[a4paper, margin=2.5cm]{geometry}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[spanish]{babel}
#+LATEX_HEADER: \usepackage[backend=biber,style=ieee,autolang=other,maxcitenames=99, maxbibnames=99]{biblatex}
#+LATEX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \setminted{breaklines=true}



* Métricas para clasificación
#+attr_latex: :float nil
#+begin_src python :session :results output :exports both 
from pprint import pprint
from evaluate import load

accuracy = load("accuracy")
precision = load("precision")
recall = load("recall")
f1 = load("f1")

real_labels = [0,1,0,1,1]
predicted_labels = [0,0,0,1,1]
acc_val = accuracy.compute(references=real_labels, predictions=predicted_labels)
precision_val = precision.compute(references=real_labels, predictions=predicted_labels)
recall_val = recall.compute(references=real_labels, predictions=predicted_labels)
f1_val = f1.compute(references=real_labels, predictions=predicted_labels)

print(f"acc: {acc_val}")
print(f"precision: {precision_val}")
print(f"recall: {recall_val}")
print(f"f1: {f1_val}")
#+end_src

#+RESULTS:
: 2025-01-15 09:16:55.005124: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
: 2025-01-15 09:16:55.005216: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
: 2025-01-15 09:16:55.005242: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
: 2025-01-15 09:16:55.112401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
: To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
: acc: {'accuracy': 0.8}
: precision: {'precision': 1.0}
: recall: {'recall': 0.6666666666666666}
: f1: {'f1': 0.8}

Se presenta la descripción y las features de ~Accuracy~

#+begin_src python :session :results output :exports both :width \textwidth
print("Accuracy description")
print(accuracy.description)
print("Accuracy Features")
print(accuracy.features)

#+end_src

#+RESULTS:
#+begin_minted
Accuracy description

Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
 Where:
TP: True positive
TN: True negative
FP: False positive
FN: False negative

Accuracy Features
{'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}
#+end_minted

Se presenta las features y la descripción de la métrica ~Precision~
#+attr_latex: 
#+begin_src python :session :results output :exports both 
print("Description")
print(precision.description)
print("Features")
print(precision.features)

#+end_src

#+RESULTS:
: Description
: 
: Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:
: Precision = TP / (TP + FP)
: where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).
: 
: Features
: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}

Se presenta las features y la descripcion de ~Recall~
#+attr_latex: 
#+begin_src python :session :results output :exports both 
print("Description")
print(recall.description)
print("Features")
print(recall.features)

#+end_src

#+RESULTS:
: Description
: 
: Recall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:
: Recall = TP / (TP + FN)
: Where TP is the true positives and FN is the false negatives.
: 
: Features
: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}

* Ejercicio Métricas de Clasificación en Pipeline
#+begin_src python :session :results output :exports both
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from evaluate import load

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
model = model.to(device)
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device=0)

new_data = ["this movie was terrible", "best movie ever"]

predictions = classifier(new_data)
predicted_labels = [1 if pred["label"]=="POSITIVE" else 0 for pred in predictions]
print(predicted_labels)
# tokenizar dato de entrada
new_input = tokenizer(new_data, return_tensors="pt", padding=True, truncation=True, max_length=64)
new_input = new_input.to(device)
with torch.no_grad():
    outputs = model(**new_input)

predicted = torch.argmax(outputs.logits, dim=1).tolist()
print(predicted)
# etiquetas ground truth

real = [0,1]

accuracy = load("accuracy")
precision = load("precision")
recall = load("recall")
f1 = load("f1")

acc_val = accuracy.compute(references=real, predictions=predicted)
precision_val = precision.compute(references=real, predictions=predicted)
recall_val = recall.compute(references=real, predictions=predicted)
f1_val = f1.compute(references=real, predictions=predicted)

print(f"acc: {acc_val}")
print(f"precision: {precision_val}")
print(f"recall: {recall_val}")
print(f"f1: {f1_val}")
#+end_src

#+RESULTS:
: [0, 1]
: [0, 1]
: acc: {'accuracy': 1.0}
: precision: {'precision': 1.0}
: recall: {'recall': 1.0}
: f1: {'f1': 1.0}
* Perplexity
#+begin_src python :session :results output :exports both
import torch
from evaluate import load
from transformers import AutoModelForCausalLM, AutoTokenizer

# revisando si la GPU esta disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

tokenizer = AutoTokenizer.from_pretrained(model_name)

# configurando el padding token a eos_token
tokenizer.pad_token = tokenizer.eos_token
# Preparar el texto semilla
prompt = "Latest research findings in Antartica show"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
attention_mask = torch.ones(input_ids.shape, device=device)
# Generacion de texto
output = model.generate(input_ids,
                        max_length=45,
                        num_return_sequences=1)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

# Probando el Perplexity Score
# se requiere tokenizar el texto generado
inputs = tokenizer(generated_text,
                   return_tensors="pt",
                   padding=True,
                   truncation=True).to(device)
inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, device=device)
# cargando el perplexity score
perplexity = load("perplexity", module_type="metric")

# results = perplexity.compute(predictions=generated_text, model_id="gpt2")
results = perplexity.compute(model=model,
                             input_ids=inputs['input_ids'],
                             attention_mask=inputs['attention_mask'],
                             pad_token_id=tokenizer.pad_token_id)
print(results)
print(results["mean_perplexity"])


#+end_src

#+RESULTS:
: The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
: Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
: The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
: Latest research findings in Antartica show that the presence of a single molecule in the brain is associated with a number of neurodegenerative diseases, including Alzheimer's disease, Parkinson's disease, and Huntington's disease.

* Ejercicio para BLEU
#+begin_src python :session :results output :exports both
from pprint import pprint
from evaluate import load
bleu = load("bleu")
pprint(bleu.description)

input_text = "Latest research findings in Antarctica show"
references = [["Latest research findings in Antartica show significant ice loss due to climate change.", "Latest research findings in Antarctica show that the ice sheet is melting faster than previously thought."]]

generated_text = "Latest research findings in Antarctica show that the ice sheet is melting faster than previously thought."
result = bleu.compute(predictions=[generated_text], references=references)
pprint(result)
#+end_src

#+RESULTS:
#+begin_example
('BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the '
 'quality of text which has been machine-translated from one natural language '
 'to another.\n'
 "Quality is considered to be the correspondence between a machine's output "
 'and that of a human: "the closer a machine translation is to a professional '
 'human translation, the better it is"\n'
 '– this is the central idea behind BLEU. BLEU was one of the first metrics to '
 'claim a high correlation with human judgements of quality, and remains one '
 'of the most popular automated and inexpensive metrics.\n'
 '\n'
 'Scores are calculated for individual translated segments—generally '
 'sentences—by comparing them with a set of good quality reference '
 'translations.\n'
 'Those scores are then averaged over the whole corpus to reach an estimate of '
 "the translation's overall quality.\n"
 'Neither intelligibility nor grammatical correctness are not taken into '
 'account.\n')
{'bleu': 1.0,
 'brevity_penalty': 1.0,
 'length_ratio': 1.2142857142857142,
 'precisions': [1.0, 1.0, 1.0, 1.0],
 'reference_length': 14,
 'translation_length': 17}
#+end_example

* Ejercicio BLEU Traducción
#+begin_src python :session :results output :exports both
from pprint import pprint
from evaluate import load
bleu = load("bleu")

input_sentences = ["Hola, ¿cómo estás?", "Estoy genial, gracias"]
references = [["Hello, how are you?", "Hi, how are you?"],
              ["I'm great, thanks", "I'm great, thank you"]]
result = bleu.compute(predictions=input_sentences, references=references)
pprint(result)
pprint(bleu.description)
#+end_src

#+RESULTS:
#+begin_example
{'bleu': 0.0,
 'brevity_penalty': 0.8948393168143697,
 'length_ratio': 0.9,
 'precisions': [0.3333333333333333, 0.0, 0.0, 0.0],
 'reference_length': 10,
 'translation_length': 9}
('BLEU (Bilingual Evaluation Understudy) is an algorithm for evaluating the '
 'quality of text which has been machine-translated from one natural language '
 'to another.\n'
 "Quality is considered to be the correspondence between a machine's output "
 'and that of a human: "the closer a machine translation is to a professional '
 'human translation, the better it is"\n'
 '– this is the central idea behind BLEU. BLEU was one of the first metrics to '
 'claim a high correlation with human judgements of quality, and remains one '
 'of the most popular automated and inexpensive metrics.\n'
 '\n'
 'Scores are calculated for individual translated segments—generally '
 'sentences—by comparing them with a set of good quality reference '
 'translations.\n'
 'Those scores are then averaged over the whole corpus to reach an estimate of '
 "the translation's overall quality.\n"
 'Neither intelligibility nor grammatical correctness are not taken into '
 'account.\n')
#+end_example

- brevity penalty :: Es un factor que se aplica en la puntuación BLEU
  para penalizar las traducciones que son más cortas que las oraciones
  de referencia. Si la traducción es significativamente más corta que
  la referencia, la puntuación BLEU se reduce. Esto ayuda a evitar que
  las traducciones excesivamente breves obtengan puntuaciones
  artificialmente altas.
- length ratio :: Es la relación entre la longitud de la traducción y
  la longitud de la referencia. Se calcula dividiendo la longitud (el
  número de palabras) de la traducción por la longitud de la
  referencia. Un valor cercano a 1 indica que la longitud de la
  traducción y la referencia es similar, mientras que valores muy
  altos o muy bajos indican una diferencia significativa en la
  longitud.
- translation length :: Es el número de palabras en la oración
  traducida. Es uno de los factores que se utilizan para calcular la
  relación de longitud y la penalización por brevedad.
- reference length :: número de palabras en la oración de referencia
- precisions :: proporción de n-gramas que aparecen en la referencia hasta 4 n-gramas.


* ROUGE
#+begin_src python :session :results output :exports both
from evaluate import load

rouge = load('rouge')

predictions = ["The cat sat on the mat."]
references = ["The cat is sitting on the mat."]

results = rouge.compute(predictions=predictions,
                        references=references)
pprint(results)
#+end_src

#+RESULTS:
: {'rouge1': 0.7692307692307692,
:  'rouge2': 0.5454545454545454,
:  'rougeL': 0.7692307692307692,
:  'rougeLsum': 0.7692307692307692}

#+begin_src python :session :results output :exports both
predictions = ["As we learn more about the frequency and size distribution of exoplanets, we are discovering that terrestrial planets are exceedingly common."]
references = ["The more we learn about the frequency and size distribution of exoplanets, the more confident we are that they are exceedingly common."]

results = rouge.compute(predictions=predictions,
                        references=references)
pprint(results)
#+end_src

#+RESULTS:
: {'rouge1': 0.7906976744186046,
:  'rouge2': 0.5365853658536585,
:  'rougeL': 0.7441860465116279,
:  'rougeLsum': 0.7441860465116279}

#+begin_src python :session :results output :exports both
references = ["""Un autómata finito (AF) o máquina de estado finito es un modelo computacional 
que realiza cómputos en forma automática sobre una entrada para producir una salida."""]
predictions = ["""Un autómata finito (AF) es un modelo computacional que procesa entradas 
de manera automática para generar una salida."""]
results = rouge.compute(predictions=predictions, references=references)
pprint(results)
#+end_src

#+RESULTS:
: {'rouge1': 0.64, 'rouge2': 0.4166666666666667, 'rougeL': 0.6, 'rougeLsum': 0.64}

Se observa que ~rouge1~ y ~rougeL~ dan resultados similares en los
ejemplos. ~rouge2~, por su parte, parece más bajo que las otras
métricas. Esto se debe a la comparación a nivel de n-gramas realizada
por la métrica.

* METEOR y BLEU

#+begin_src python :session :results output :exports both
from evaluate import load

bleu_metric = load('bleu')
meteor_metric = load('meteor')

predictions = ["He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures."]
references = ["He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures."]

results_bleu = bleu_metric.compute(predictions=predictions, references=references)
results_meteor = meteor_metric.compute(predictions=predictions, references=references)
pprint(f"BLEU: {results_bleu}")
pprint(f"Meteor: {results_meteor}")
#+end_src

#+RESULTS:
#+begin_example
[nltk_data] Downloading package wordnet to /home/leningfe/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/leningfe/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /home/leningfe/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
("BLEU: {'bleu': 0.25928256340208583, 'precisions': [0.7, 0.3684210526315789, "
 "0.2222222222222222, 0.11764705882352941], 'brevity_penalty': "
 "0.9048374180359595, 'length_ratio': 0.9090909090909091, "
 "'translation_length': 20, 'reference_length': 22}")
"Meteor: {'meteor': 0.6531090723751274}"
#+end_example
Se observa que los scores de BLEU son más bajos comparados con METEOR
* Exact Match

#+begin_src python :session :results output :exports both
from evaluate import load
exact_match = load("exact_match")

predictions = ["The cat sat on the mat.",
               "Theaters are great.",
               "Like comparing oranges and apples."]
references = ["The cat sat on the mat?",
              "Theaters are great.",
              "Like comparing apples and oranges."]

results = exact_match.compute(predictions=predictions,
                              references=references)

pprint(f"Exact Match: {results}")

#+end_src

#+RESULTS:
: "Exact Match: {'exact_match': 0.3333333333333333}"
Se observa que la métrica busca establecer relaciones de identidad
entre las oraciones generadas y las de referencia. Por ejemplo, si en
la oración del medio, que es la única idéntica en ambos sets,
alteramos y eliminamos el punto final, el ~EM~ se hace 0

* Similitud de texto
** Bert Score
#+begin_src python :session :results output :exports both
from evaluate import load
bertscore = load("bertscore")
predictions = ["The burrow stretched forward like a narrow corridor for a while, then plunged abruptly downward, so quickly that Alice had no chance to stop herself before she was tumbling into an extremely deep shaft."]
references = ["The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well."]

results = bertscore.compute(predictions=predictions,
                            references=references,
                            model_type="roberta-large")
pprint(f"Bert-Score: {results}")

# para meteor
meteor_score = load("meteor")
results_meteor = meteor_score.compute(predictions=predictions,
                                      references=references)
pprint(f"Meteor-Score: {results_meteor}")
#+end_src 

#+RESULTS:
#+begin_example
Downloading builder script:   0%|                                         | 0.00/7.95k [00:00<?, ?B/s]Downloading builder script: 100%|████████████████████████████████| 7.95k/7.95k [00:00<00:00, 5.29MB/s]
tokenizer_config.json:   0%|                                               | 0.00/25.0 [00:00<?, ?B/s]tokenizer_config.json: 100%|███████████████████████████████████████| 25.0/25.0 [00:00<00:00, 4.54kB/s]
config.json:   0%|                                                          | 0.00/482 [00:00<?, ?B/s]config.json: 100%|███████████████████████████████████████████████████| 482/482 [00:00<00:00, 1.17MB/s]
vocab.json:   0%|                                                          | 0.00/899k [00:00<?, ?B/s]vocab.json: 100%|██████████████████████████████████████████████████| 899k/899k [00:00<00:00, 3.53MB/s]vocab.json: 100%|██████████████████████████████████████████████████| 899k/899k [00:00<00:00, 3.50MB/s]
merges.txt:   0%|                                                          | 0.00/456k [00:00<?, ?B/s]merges.txt: 100%|██████████████████████████████████████████████████| 456k/456k [00:00<00:00, 6.79MB/s]
tokenizer.json:   0%|                                                     | 0.00/1.36M [00:00<?, ?B/s]tokenizer.json: 100%|████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 15.9MB/s]
model.safetensors:   0%|                                                  | 0.00/1.42G [00:00<?, ?B/s]model.safetensors:   1%|▎                                        | 10.5M/1.42G [00:01<02:27, 9.57MB/s]model.safetensors:   1%|▌                                        | 21.0M/1.42G [00:01<01:25, 16.3MB/s]model.safetensors:   2%|▉                                        | 31.5M/1.42G [00:01<01:02, 22.2MB/s]model.safetensors:   3%|█▏                                       | 41.9M/1.42G [00:01<00:53, 25.9MB/s]model.safetensors:   4%|█▌                                       | 52.4M/1.42G [00:02<00:50, 27.3MB/s]model.safetensors:   4%|█▊                                       | 62.9M/1.42G [00:02<00:48, 28.0MB/s]model.safetensors:   5%|██                                       | 73.4M/1.42G [00:03<00:46, 28.8MB/s]model.safetensors:   6%|██▍                                      | 83.9M/1.42G [00:03<00:47, 28.0MB/s]model.safetensors:   7%|██▋                                      | 94.4M/1.42G [00:03<00:46, 28.8MB/s]model.safetensors:   7%|███                                       | 105M/1.42G [00:04<00:44, 29.9MB/s]model.safetensors:   8%|███▍                                      | 115M/1.42G [00:04<00:42, 30.4MB/s]model.safetensors:   9%|███▋                                      | 126M/1.42G [00:04<00:39, 32.6MB/s]model.safetensors:  10%|████                                      | 136M/1.42G [00:04<00:37, 33.9MB/s]model.safetensors:  10%|████▎                                     | 147M/1.42G [00:05<00:38, 33.2MB/s]model.safetensors:  11%|████▋                                     | 157M/1.42G [00:05<00:36, 35.0MB/s]model.safetensors:  12%|████▉                                     | 168M/1.42G [00:05<00:32, 38.0MB/s]model.safetensors:  13%|█████▎                                    | 178M/1.42G [00:06<00:32, 38.0MB/s]model.safetensors:  13%|█████▌                                    | 189M/1.42G [00:06<00:29, 42.3MB/s]model.safetensors:  14%|█████▉                                    | 199M/1.42G [00:06<00:27, 44.6MB/s]model.safetensors:  15%|██████▏                                   | 210M/1.42G [00:06<00:29, 41.1MB/s]model.safetensors:  15%|██████▌                                   | 220M/1.42G [00:07<00:29, 40.9MB/s]model.safetensors:  16%|██████▊                                   | 231M/1.42G [00:07<00:31, 38.0MB/s]model.safetensors:  17%|███████                                   | 241M/1.42G [00:07<00:32, 36.7MB/s]model.safetensors:  18%|███████▍                                  | 252M/1.42G [00:07<00:30, 38.9MB/s]model.safetensors:  18%|███████▋                                  | 262M/1.42G [00:08<00:29, 38.8MB/s]model.safetensors:  19%|████████                                  | 273M/1.42G [00:08<00:31, 36.2MB/s]model.safetensors:  20%|████████▎                                 | 283M/1.42G [00:08<00:32, 35.4MB/s]model.safetensors:  21%|████████▋                                 | 294M/1.42G [00:09<00:29, 37.8MB/s]model.safetensors:  21%|████████▉                                 | 304M/1.42G [00:09<00:29, 37.6MB/s]model.safetensors:  22%|█████████▎                                | 315M/1.42G [00:09<00:29, 38.0MB/s]model.safetensors:  23%|█████████▌                                | 325M/1.42G [00:09<00:26, 41.5MB/s]model.safetensors:  24%|█████████▉                                | 336M/1.42G [00:09<00:23, 45.7MB/s]model.safetensors:  24%|██████████▏                               | 346M/1.42G [00:10<00:24, 43.5MB/s]model.safetensors:  25%|██████████▌                               | 357M/1.42G [00:10<00:24, 43.0MB/s]model.safetensors:  26%|██████████▊                               | 367M/1.42G [00:10<00:22, 46.5MB/s]model.safetensors:  27%|███████████▏                              | 377M/1.42G [00:11<00:29, 35.9MB/s]model.safetensors:  27%|███████████▍                              | 388M/1.42G [00:11<00:29, 35.3MB/s]model.safetensors:  28%|███████████▊                              | 398M/1.42G [00:11<00:25, 40.1MB/s]model.safetensors:  29%|████████████                              | 409M/1.42G [00:11<00:28, 35.8MB/s]model.safetensors:  30%|████████████▍                             | 419M/1.42G [00:12<00:27, 36.6MB/s]model.safetensors:  30%|████████████▋                             | 430M/1.42G [00:12<00:29, 34.1MB/s]model.safetensors:  31%|█████████████                             | 440M/1.42G [00:12<00:27, 35.3MB/s]model.safetensors:  32%|█████████████▎                            | 451M/1.42G [00:13<00:28, 34.7MB/s]model.safetensors:  32%|█████████████▋                            | 461M/1.42G [00:13<00:27, 34.8MB/s]model.safetensors:  33%|█████████████▉                            | 472M/1.42G [00:13<00:25, 36.9MB/s]model.safetensors:  34%|██████████████▏                           | 482M/1.42G [00:13<00:24, 37.9MB/s]model.safetensors:  35%|██████████████▌                           | 493M/1.42G [00:14<00:26, 35.5MB/s]model.safetensors:  35%|██████████████▊                           | 503M/1.42G [00:14<00:24, 37.1MB/s]model.safetensors:  36%|███████████████▏                          | 514M/1.42G [00:14<00:26, 34.6MB/s]model.safetensors:  37%|███████████████▍                          | 524M/1.42G [00:15<00:25, 35.6MB/s]model.safetensors:  38%|███████████████▊                          | 535M/1.42G [00:15<00:25, 35.2MB/s]model.safetensors:  38%|████████████████                          | 545M/1.42G [00:15<00:24, 35.5MB/s]model.safetensors:  39%|████████████████▍                         | 556M/1.42G [00:15<00:21, 39.6MB/s]model.safetensors:  40%|████████████████▋                         | 566M/1.42G [00:16<00:23, 37.1MB/s]model.safetensors:  41%|█████████████████                         | 577M/1.42G [00:16<00:21, 39.5MB/s]model.safetensors:  41%|█████████████████▎                        | 587M/1.42G [00:16<00:21, 38.9MB/s]model.safetensors:  42%|█████████████████▋                        | 598M/1.42G [00:17<00:19, 42.0MB/s]model.safetensors:  43%|█████████████████▉                        | 608M/1.42G [00:17<00:21, 38.3MB/s]model.safetensors:  44%|██████████████████▎                       | 619M/1.42G [00:17<00:21, 37.0MB/s]model.safetensors:  44%|██████████████████▌                       | 629M/1.42G [00:17<00:22, 34.9MB/s]model.safetensors:  45%|██████████████████▉                       | 640M/1.42G [00:18<00:21, 35.8MB/s]model.safetensors:  46%|███████████████████▏                      | 650M/1.42G [00:18<00:22, 34.1MB/s]model.safetensors:  46%|███████████████████▌                      | 661M/1.42G [00:18<00:21, 35.0MB/s]model.safetensors:  47%|███████████████████▊                      | 671M/1.42G [00:19<00:23, 32.0MB/s]model.safetensors:  48%|████████████████████▏                     | 682M/1.42G [00:19<00:22, 33.6MB/s]model.safetensors:  49%|████████████████████▍                     | 692M/1.42G [00:19<00:19, 37.5MB/s]model.safetensors:  49%|████████████████████▊                     | 703M/1.42G [00:20<00:21, 34.2MB/s]model.safetensors:  50%|█████████████████████                     | 713M/1.42G [00:20<00:19, 36.4MB/s]model.safetensors:  51%|█████████████████████▎                    | 724M/1.42G [00:20<00:18, 37.0MB/s]model.safetensors:  52%|█████████████████████▋                    | 734M/1.42G [00:20<00:18, 37.5MB/s]model.safetensors:  52%|█████████████████████▉                    | 744M/1.42G [00:21<00:22, 29.9MB/s]model.safetensors:  53%|██████████████████████▎                   | 755M/1.42G [00:21<00:22, 30.0MB/s]model.safetensors:  54%|██████████████████████▌                   | 765M/1.42G [00:22<00:21, 30.3MB/s]model.safetensors:  55%|██████████████████████▉                   | 776M/1.42G [00:22<00:20, 31.7MB/s]model.safetensors:  55%|███████████████████████▏                  | 786M/1.42G [00:22<00:18, 34.0MB/s]model.safetensors:  56%|███████████████████████▌                  | 797M/1.42G [00:22<00:15, 39.0MB/s]model.safetensors:  57%|███████████████████████▊                  | 807M/1.42G [00:23<00:17, 36.0MB/s]model.safetensors:  58%|████████████████████████▏                 | 818M/1.42G [00:23<00:15, 38.3MB/s]model.safetensors:  58%|████████████████████████▍                 | 828M/1.42G [00:23<00:17, 34.3MB/s]model.safetensors:  59%|████████████████████████▊                 | 839M/1.42G [00:24<00:15, 37.4MB/s]model.safetensors:  60%|█████████████████████████                 | 849M/1.42G [00:24<00:15, 38.1MB/s]model.safetensors:  60%|█████████████████████████▍                | 860M/1.42G [00:24<00:14, 38.7MB/s]model.safetensors:  61%|█████████████████████████▋                | 870M/1.42G [00:24<00:13, 40.1MB/s]model.safetensors:  62%|██████████████████████████                | 881M/1.42G [00:25<00:14, 36.8MB/s]model.safetensors:  63%|██████████████████████████▎               | 891M/1.42G [00:25<00:13, 38.1MB/s]model.safetensors:  63%|██████████████████████████▋               | 902M/1.42G [00:25<00:15, 34.3MB/s]model.safetensors:  64%|██████████████████████████▉               | 912M/1.42G [00:26<00:14, 35.6MB/s]model.safetensors:  65%|███████████████████████████▎              | 923M/1.42G [00:26<00:13, 38.0MB/s]model.safetensors:  66%|███████████████████████████▌              | 933M/1.42G [00:26<00:12, 38.9MB/s]model.safetensors:  66%|███████████████████████████▉              | 944M/1.42G [00:26<00:11, 40.4MB/s]model.safetensors:  67%|████████████████████████████▏             | 954M/1.42G [00:26<00:11, 41.9MB/s]model.safetensors:  68%|████████████████████████████▍             | 965M/1.42G [00:27<00:10, 42.7MB/s]model.safetensors:  69%|████████████████████████████▊             | 975M/1.42G [00:27<00:11, 39.7MB/s]model.safetensors:  69%|█████████████████████████████             | 986M/1.42G [00:27<00:11, 39.2MB/s]model.safetensors:  70%|█████████████████████████████▍            | 996M/1.42G [00:27<00:10, 42.5MB/s]model.safetensors:  71%|█████████████████████████████            | 1.01G/1.42G [00:28<00:10, 39.3MB/s]model.safetensors:  72%|█████████████████████████████▎           | 1.02G/1.42G [00:28<00:10, 37.9MB/s]model.safetensors:  72%|█████████████████████████████▋           | 1.03G/1.42G [00:28<00:11, 34.8MB/s]model.safetensors:  73%|█████████████████████████████▉           | 1.04G/1.42G [00:29<00:10, 37.6MB/s]model.safetensors:  74%|██████████████████████████████▏          | 1.05G/1.42G [00:29<00:09, 38.2MB/s]model.safetensors:  74%|██████████████████████████████▌          | 1.06G/1.42G [00:29<00:09, 36.3MB/s]model.safetensors:  75%|██████████████████████████████▊          | 1.07G/1.42G [00:30<00:09, 36.9MB/s]model.safetensors:  76%|███████████████████████████████▏         | 1.08G/1.42G [00:30<00:09, 35.3MB/s]model.safetensors:  77%|███████████████████████████████▍         | 1.09G/1.42G [00:30<00:09, 36.1MB/s]model.safetensors:  77%|███████████████████████████████▊         | 1.10G/1.42G [00:30<00:08, 38.4MB/s]model.safetensors:  78%|████████████████████████████████         | 1.11G/1.42G [00:31<00:08, 35.9MB/s]model.safetensors:  79%|████████████████████████████████▎        | 1.12G/1.42G [00:31<00:08, 37.1MB/s]model.safetensors:  80%|████████████████████████████████▋        | 1.13G/1.42G [00:31<00:07, 37.1MB/s]model.safetensors:  80%|████████████████████████████████▉        | 1.14G/1.42G [00:32<00:07, 35.1MB/s]model.safetensors:  81%|█████████████████████████████████▎       | 1.15G/1.42G [00:32<00:07, 33.6MB/s]model.safetensors:  82%|█████████████████████████████████▌       | 1.16G/1.42G [00:32<00:07, 32.7MB/s]model.safetensors:  83%|█████████████████████████████████▊       | 1.17G/1.42G [00:33<00:07, 33.9MB/s]model.safetensors:  83%|██████████████████████████████████▏      | 1.18G/1.42G [00:33<00:07, 32.1MB/s]model.safetensors:  84%|██████████████████████████████████▍      | 1.20G/1.42G [00:33<00:06, 34.3MB/s]model.safetensors:  85%|██████████████████████████████████▊      | 1.21G/1.42G [00:33<00:06, 34.5MB/s]model.safetensors:  86%|███████████████████████████████████      | 1.22G/1.42G [00:34<00:05, 35.1MB/s]model.safetensors:  86%|███████████████████████████████████▍     | 1.23G/1.42G [00:34<00:05, 35.3MB/s]model.safetensors:  87%|███████████████████████████████████▋     | 1.24G/1.42G [00:34<00:05, 33.8MB/s]model.safetensors:  88%|███████████████████████████████████▉     | 1.25G/1.42G [00:35<00:05, 31.8MB/s]model.safetensors:  89%|████████████████████████████████████▎    | 1.26G/1.42G [00:35<00:05, 30.9MB/s]model.safetensors:  89%|████████████████████████████████████▌    | 1.27G/1.42G [00:35<00:04, 33.5MB/s]model.safetensors:  90%|████████████████████████████████████▉    | 1.28G/1.42G [00:36<00:04, 34.8MB/s]model.safetensors:  91%|█████████████████████████████████████▏   | 1.29G/1.42G [00:36<00:03, 35.9MB/s]model.safetensors:  91%|█████████████████████████████████████▍   | 1.30G/1.42G [00:36<00:03, 31.3MB/s]model.safetensors:  92%|█████████████████████████████████████▊   | 1.31G/1.42G [00:37<00:03, 30.2MB/s]model.safetensors:  93%|██████████████████████████████████████   | 1.32G/1.42G [00:37<00:03, 30.2MB/s]model.safetensors:  94%|██████████████████████████████████████▍  | 1.33G/1.42G [00:37<00:02, 31.6MB/s]model.safetensors:  94%|██████████████████████████████████████▋  | 1.34G/1.42G [00:38<00:02, 30.2MB/s]model.safetensors:  95%|███████████████████████████████████████  | 1.35G/1.42G [00:38<00:02, 31.9MB/s]model.safetensors:  96%|███████████████████████████████████████▎ | 1.36G/1.42G [00:38<00:01, 32.2MB/s]model.safetensors:  97%|███████████████████████████████████████▌ | 1.37G/1.42G [00:39<00:01, 31.8MB/s]model.safetensors:  97%|███████████████████████████████████████▉ | 1.38G/1.42G [00:39<00:01, 30.7MB/s]model.safetensors:  98%|████████████████████████████████████████▏| 1.39G/1.42G [00:39<00:00, 29.6MB/s]model.safetensors:  99%|████████████████████████████████████████▌| 1.41G/1.42G [00:40<00:00, 25.3MB/s]model.safetensors: 100%|████████████████████████████████████████▊| 1.42G/1.42G [00:40<00:00, 27.1MB/s]model.safetensors: 100%|█████████████████████████████████████████| 1.42G/1.42G [00:41<00:00, 28.8MB/s]model.safetensors: 100%|█████████████████████████████████████████| 1.42G/1.42G [00:41<00:00, 34.7MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
("Bert-Score: {'precision': [0.9340652227401733], 'recall': "
 "[0.9245126247406006], 'f1': [0.9292643666267395], 'hashcode': "
 "'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.45.2)'}")
[nltk_data] Downloading package wordnet to /home/leningfe/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/leningfe/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /home/leningfe/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
"Meteor-Score: {'meteor': 0.37180012567275916}"
#+end_example

** Similitud de Textos con Glove y Sim-score
#+begin_src python :session :results output :exports both
import torch.nn.functional as F
from torchtext.vocab import GloVe
glove = GloVe(name='6B', dim=100)

sentence1 = "The cat is on the mat"
sentence2 = "The dog is on the mat"


# Function to get sentence embedding by averaging word embeddings
def get_sentence_embedding(sentence, glove):
    sentence_words = sentence.lower().split()
    word_embeddings = [glove[word] for word in sentence_words if word in glove.stoi]
    if word_embeddings:
        return torch.mean(torch.stack(word_embeddings), dim=0)
    else:
        return torch.zeros(glove.dim)  # Return zero vector if no embeddings are found

# Get the sentence embeddings for both sentences
embedding_sentence1 = get_sentence_embedding(words_sentence1, glove)
embedding_sentence2 = get_sentence_embedding(words_sentence2, glove)

cosine_similarity = F.cosine_similarity(embedding_sentence1,
                                        embedding_sentence2, dim=0)

pprint(f"Cosine similarity between the sentences: {cosine_similarity.item():.4f}")
#+end_src

#+RESULTS:
Cosine similarity between the sentences: 0.9948
#+print_bibliography:


