#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Inteligencia Artificial Generativa
#+date: 2024-10-02
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment

* Modelamiento Generativo 2024-10-02 
** Questions and keywords
- modelo generativo :: produce datos nuevos similares al dataset de
  entrenamiento. son esencialmente probabilisticos.
- modelos discriminativos :: son modelos que clasifican (supervisado,
  no supervisado y por refuerzo)
- modelos generativos condicionales :: $P(\mathbf{x}|y)$
- GAN :: es un modelo generativo (herramienta) mas un discriminativo
  (lupa). El generativo quiere engañar al discriminativo. El modelo
  discriminativo no puede distinguir el dato real y el falso. Aparecen
  en 2014.
- AGI inteligencia artificial general :: requiere conocimiento,
  razonamiento, pensamiento creativo, vision, lenguaje. debe
  confundirse con la humana
- no entiendo como dice que los modelos generativo y discrimintivo "share notes" :: al
  parecer la nota que se comparte es el valor que el discriminador
  obtiene indicando si el fake generado es mejor
- cuanto es un zetabyte :: 
- ¿puede usar un modelo generativo en el tiempo? ::
- transformers ::
- aprendizaje por refuerzo con realimetntación humana :: aprovecha el
  feedback humano para mejorar
- RLHF :: requiere datos de entrenamiento de humanos
- transformers :: atención, sensitiva contexto
- gan ::  requiere competencia de modelos
** Notes
- Modelo discrimintavio estima $P(y|x)$
  - Predecir el clima
  - categorizar libros
  - categorizar imagenes
- Modelo discriminativo tiene una probabilidad de que la predicción
  corresponda a una clase.
- El modelo generativo estima $P(x)$ pero hay la opción
  condicional. E.g. hacer un vangoh pero nocturno
- Existen varias aplicaciones en redes, en graficos, juegos,
  marketing, educativas, médicas, industriales, chatbots
- Existen diferentes juegos de entrada y salida para la I. Generativa:
  - prompt a texto
  - promt a imagen y
  - de imagen a imagen
- Tiene generalización y particularidad
- Los GANs es una combinacion entre modelo generativo y discriminativo
- Inteligencia artificial generativa puede:
  - escribir codigo para una website
  - contestar a un cliente preguntas de servicio
  - generar una imagen
- Inteligencia artificial general: hacer trabajos humanos 100%. Esto es discutible.
- Factores que facilitan la IA generativa son:
  - aumento poder computacional
  - grandes cantidades de datos
  - competencias geopoliticas y entre desarrolladores
  - herramientas y modelos disponibles (comunidad)
- transformers
** Summary
Los modelos generativos son aquellos modelos de /machine learning/ que
entrenan un modelo capaz de producir nuevos datos similares a los de
un conjunto de datos. Un modelo generativo es esencialmente
probabilístico ya que se desea poder obtener diferentes variaciones
del resultado final antes que el mismo resultado cada vez. Estos
modelos no requieren que el dataset tenga etiquetas. Trata de modelar
la probabilidad de observar un objeto $\mathbf{x}$

Un ejemplo de modelo discriminativo son los clasificadores. Pues,
éstos tratan de predecir una etiqueta. Es decir trata de modelar la
probabilidad de una etiqueta $y$ dada una observación $\mathbf{x}$

- Modelo discriminativo :: $P(y|\mathbf{x})$
- Modelo Generativo :: $P(\mathbf{x})$
- Modelo Generativo Condicional :: es la probabilidad de ver una
  observación $\mathbf{x}$ con una etiqueta $y$ específica i.e. $P(\mathbf{x}|y)$

* 2024-10-07 Entrenamiento y Evaluación de Modelos
** Questions and keywords
- stable diffusion:: ¿que es?
- personally identifiable information :: ??
- las ideas sobre transferlearning sobre el tamaño del dataset y el
  orden en que se ejecutan o son nuevas o son equivocadas no las
  conozco y debo averiguar. Por ejemplo se afirmó que fine tuning se
  hace luego de transfer learning
- espacio latente :: está relacionado al concepto de embeddings
- métricas modelos generativos:
  - inception score IS (img) :: evalua imagenes eneradas segin la
    probabilidad de pertenecer a distintas categorías. Quería generar
    gatos son gatos o no.
  - frechet inception distance FID (img) :: mide distancia entre las
    distribuciones de las imágenes relates y las generadas.
  - bleu (texto) :: similitud del texto generado y el de referencia
  - Rouge (texto) :: similitud del texto generado y el de referencia
- injusticia entre comparar lo que la máquina hace con respecto a lo
  que hace el humano
- gold standard :: evaluación inteligente realizada por humanos u otras IA
- test de turing :: aplica como medida en IA generativa?
** Notes
- recopilación de datos:
  - grandes volumenes
  - datos diversos
  - datos ricos
  - requiere pre procesamiento
- preprocesamiento es una tarea adecuada
- privacidad y seguridad :: los datos han de cumplir con PII con su
  respectiva controles de seguridad.
*** Entrenamimento de modelos
- el hardware a utilizar
- el tiempo requerido se refleja en el tamaño del dataset, la
  complejidad del modelo y el número de rondas de entrenamient
- el costo
- técnicas avanzadas de entrenamiento:
  - Transfer learning :: transfiere conocimiento de una tarea a otra
  - fine tuning :: es un tipo de transfer learning para un dataset más
    pequeño. se usa luego de transfer learning
  - human in the loop :: ajusta las respuestas con lo que el humano da feedback
  - embeddings :: representaciones únicas de entidades de
    datos. representan la información de manera compacta
*** Evaluación de modelos:
- en modelos generativos cómo se evalúa su desempeño
- evaluación discriminativa :: puede usar precisión/accuracy. Pero no
  son aplicables para medir la creatividad del modelo
- en general se mide el progreso del modelo en el tiempo
- criterios de comparación entre diferentes versiones del modelo
- comparación del modelo con el rendimiento humano
- hay metricas específicas par atexto, imagen o audio
- la comparación con el rendimiento humano. comparación de
  habilidades. se puede suponer que es una comparación injusta. por
  ejmplo, comparar si el producto generado por la IA debe ser
  comparado con el trabajo humano.
- una relativa amenaza sobre las habilitades humanas
** TODO Métricas de Evaluación de Imagenes generadas por GAN (FID y LPIPS) [66%]
- [X] Investigación de FID
- [X] Investigación de LPIPS
- [ ] Generar documento
** Summary
* 2024-10-09 Exposición métricas y  Representación del Conocimiento
** Questions and keywords
- n-grams ::
- BLEU :: cómo es y cómo funciona
- ROUGE :: cómo varía cuando es alto o bajo. Cuando es 1 son idénticos si es menor, difiere.
- ¿se verificó el cómputo de ROUGE y METEOR? ::
- decoder :: 
- encoder ::
- hay algun limite para el tamaño del espacio latente? ::
- espacio latente ::
- que tan complicado es generar una fake imagen medica ::
- que tan raro es hacer una genreación en el tiempo ::
- cómo funciona pytorch a nivel de autoencodres, vaes ::
- se puede establecer una relacion entre la entropia de shannon y el espacio latente ::
- cómo se llama el paper de Jona :: un espacio latente independiente
  del tipo de dato. es sobre representación platónica. asumen que es
  parte de la realidad. Mientras es más completo el modelo converge el
  tema de lenguaje y de imagen para generar una imagen de manzana y
  viceversa. The Platonic Representation Hypothesis
- modelado paramétrico :: 
- likelihood o verosimilitud :: identificar parámetros que maximicen la probabilidad
- MLE :: máxima verosimilitud
- densidad tractable :: se define de antemano: normal bernoulli
- densidad aproximada :: depende de los datos
- que se debe hacer en la tarea? :: resumen analisis discusion?
** Notes
*** Exposicion
- METEOR parece ser superior a BLEU
- Valores altos de METEOR implica una alta similitud del texto generado con el original
- METEOR parece ser superior que ROUGE 
*** Represetacion del conocimiento
- Reducir el espacio de alta dimensionalidad a un espacio latente con menores dimensiones
- el espacio latente aprende representaciones simplificadas de datos
- los nuevos datos son variar coordenadas en el espacio latente
- este movimiento en el espacio latente podría por ejemplo afectar las
  expresiones faciales de un rostro si es una ia generativa de rostros
  o avegentar o rejuvenecer
- el espacio latente permite explorar **relaciones abstractas** entre los datos
- un modelado paramétrico es una familia de distribuciones de probabilidad
- la verosimilitud se calcula con el logaritmo para tener una suma de
  los logaritmos de las probabilidad
- MLE selecciona que valores de parámetros $\theta$ que maximizan la verosimilitud
- hay varios approaches para el modelado de la función de
  densidad. esto se conoce como taxonomía
** Summary
** TODO Tarea [100%]
- [X] leer el paper del aula virtual. Presentar conclusiones.
* 2024-10-14 Capitulo 2 Redes Neuronales
** Questions and keywords
- redes neuronales ::
- axones :: 
- dendritas ::
- funciones de activación ::
- relu :: no comprendo bien la fórmula de la relu
- revisar ajuste de pesos del perceptron ::
- gradient descent ::
- stochastic gradient descent ::
- minibatch ::
- red neuronal se puede aplicar en aprendizaje por refuerzo :: las
  salidas son acciones, pero que son las entradas
- algoritmo de back propagation ::
- ¿se menciona el problema del gradiente cuando las capas son muy grandes :: el
  gradiente desaparece por la profundidad
- deep neural netowrks :: tiene 3 o más capas ocultas
- 0 :: negro
- 255 :: blanco
- kernel :: ??
- lo que no mencionan es que los valores de la convolución también se aprenden 
** Notes
*** Exposición Jona audio
- FAD :: Freched Audio Distance. Similar al FID pero con audio.
  - utiliza todo el dataset 
- Signal to noise ratio :: cuando el valor es más alto la señal es más
  nitida. Es el cociente de la Potencia de la señal dividido para la Potencia del ruido. Se toma logaritmo
  - es fácil de calcular
  - es estandard
  - sus dificultades que no captura todas las caracteristicas perceptuales el audio
*** Capítulo 2 redes neuronales artificiales
- son una analogía de las redes nueronales biológicas
- el perceptron es usar una recta para separar las clases (boundary regions)
- $h(x_1,x_2)= g(w_0+w_1x_1+w_2x_2)$
- funcioes de activacion:
  - step function
  - sigmoid
  - relu
- aplicaciones son logica binaria como la compuerta lógica OR en función de la tabla de verdad
- algoritmo de descenso del gradiente. Sugiero revisar cómo opera el
  algoritmo para hacer el ajuste de pesos. Se parte aleatoriamente y se usa la gradiente
- el perceptron separa las clases en forma lineal
- las rdes neuronale multicapas son para problemas que no son
  linealmente dependientes. Para resolver en casos que la región a
  separar no es lineal. Ejemplo son círculos concéntricos
- el algoritmo de backpropagation se usa cuando existen varias capas.
- Overfitting sobre ajuste de los datos :: la red presenta una metrica
  de evaluacion con un muy buen score en los datos de entrenamiento
  pero pobre en los datos de testeo
- Dropout :: desactiva neuronas aleatoriamente durante el entrenamiento
- Frameworks de DeepLearning:
  - Pytorch
  - Tensorflow [[https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.64829&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false][tensorflow-playground]]
*** Computer vision
- imagen :: matriz de pixels con valores en tres capas RGB en un rango de 0 a 255
- convolución :: filtro que usa un kernel que extrae características
  de una región de la imágen. algo parecido a un detector de bordes
- la convolucion permite disminuir el tamaño de la matriz de la imagen
  y obtener valores segun el conocimiento local
- el detector de borde:
  |----+----+-----|
  | -1 | -1 |  -1 |
  |----+----+-----|
  | -1 |  8 |  -1 |
  |----+----+-----|
  | -1 | -1 | - 1 |
  |----+----+-----|
- pooling :: obtiene un valor del resultado de la convolucional e.g. el max-pooling
- flattening ::
- los pasos de convolucion y pooling se repitenn varias
  veces. mientras estoy raliezando convolution and pooling estoy en
  low level features, es decir reconociendo curvas y bordes y al final
  de la red ya tengo una operación de alto nivel que sería reconocer objetos.
- redes neuronales convolucionales sirven para reconcoer objetos en imagenes 
** Summary
* 2024-10-16 Redes ConvNet VGG
** Questions and keywords
- embeddings ::
- en verdad se puede usar una red convolucional en series de tiempo? ::
- segun la profesora que no va redes neuronales recurrentes en series de tiempo?? :: 
- espectograma ::
- bloques residuales :: 
** Notes
*** CNN
- importante la invarianza a la traslación
- las convnets son versatiles par usar modelos pre-entrenados
- la primeras capas detectan caracteristicas simples como bordes
- el modelo de redes neuronales convolucionales pueden adaptarse a
  otros problemas porque reconocen patrones
- invarianza a la posición
- procesan datos en secuencia???
- procesan datos en paralelo???
- modelos pre-entrenados:
  - vgg16:
    - Desarrollado en Oxford
    - Uso repetido de convolucionales con filtros de $3 \times 3$
    - 16 capas : 13 conv + 3 densas
    - capas de max pooling
    - el vector de caracteristica 4096
  - resnet50
  - inceptionv3
*** Series de tiempo
- determinar coomo funcionan las convnets en series de tiempo
*** Speech recognition
- ????
  
** Summary
** TODO Tareas
- [ ] consulta sobre embeddings aula virtual
  - [ ] word2vec
  - [ ] glove
  - [ ] bert
