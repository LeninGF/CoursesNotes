
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Inteligencia Artificial Generativa
#+date: 2024-10-02
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment

* Modelamiento Generativo 2024-10-02 
** Questions and keywords
- modelo generativo :: produce datos nuevos similares al dataset de
  entrenamiento. son esencialmente probabilisticos.
- modelos discriminativos :: son modelos que clasifican (supervisado,
  no supervisado y por refuerzo)
- modelos generativos condicionales :: $P(\mathbf{x}|y)$
- GAN :: es un modelo generativo (herramienta) mas un discriminativo
  (lupa). El generativo quiere engañar al discriminativo. El modelo
  discriminativo no puede distinguir el dato real y el falso. Aparecen
  en 2014.
- AGI inteligencia artificial general :: requiere conocimiento,
  razonamiento, pensamiento creativo, vision, lenguaje. debe
  confundirse con la humana
- no entiendo como dice que los modelos generativo y discrimintivo "share notes" :: al
  parecer la nota que se comparte es el valor que el discriminador
  obtiene indicando si el fake generado es mejor
- cuanto es un zetabyte :: 
- ¿puede usar un modelo generativo en el tiempo? ::
- transformers ::
- aprendizaje por refuerzo con realimetntación humana :: aprovecha el
  feedback humano para mejorar
- RLHF :: requiere datos de entrenamiento de humanos
- transformers :: atención, sensitiva contexto
- gan ::  requiere competencia de modelos
** Notes
- Modelo discrimintavio estima $P(y|x)$
  - Predecir el clima
  - categorizar libros
  - categorizar imagenes
- Modelo discriminativo tiene una probabilidad de que la predicción
  corresponda a una clase.
- El modelo generativo estima $P(x)$ pero hay la opción
  condicional. E.g. hacer un vangoh pero nocturno
- Existen varias aplicaciones en redes, en graficos, juegos,
  marketing, educativas, médicas, industriales, chatbots
- Existen diferentes juegos de entrada y salida para la I. Generativa:
  - prompt a texto
  - promt a imagen y
  - de imagen a imagen
- Tiene generalización y particularidad
- Los GANs es una combinacion entre modelo generativo y discriminativo
- Inteligencia artificial generativa puede:
  - escribir codigo para una website
  - contestar a un cliente preguntas de servicio
  - generar una imagen
- Inteligencia artificial general: hacer trabajos humanos 100%. Esto es discutible.
- Factores que facilitan la IA generativa son:
  - aumento poder computacional
  - grandes cantidades de datos
  - competencias geopoliticas y entre desarrolladores
  - herramientas y modelos disponibles (comunidad)
- transformers
** Summary
Los modelos generativos son aquellos modelos de /machine learning/ que
entrenan un modelo capaz de producir nuevos datos similares a los de
un conjunto de datos. Un modelo generativo es esencialmente
probabilístico ya que se desea poder obtener diferentes variaciones
del resultado final antes que el mismo resultado cada vez. Estos
modelos no requieren que el dataset tenga etiquetas. Trata de modelar
la probabilidad de observar un objeto $\mathbf{x}$

Un ejemplo de modelo discriminativo son los clasificadores. Pues,
éstos tratan de predecir una etiqueta. Es decir trata de modelar la
probabilidad de una etiqueta $y$ dada una observación $\mathbf{x}$

- Modelo discriminativo :: $P(y|\mathbf{x})$
- Modelo Generativo :: $P(\mathbf{x})$
- Modelo Generativo Condicional :: es la probabilidad de ver una
  observación $\mathbf{x}$ con una etiqueta $y$ específica i.e. $P(\mathbf{x}|y)$

* 2024-10-07 Entrenamiento y Evaluación de Modelos
** Questions and keywords
- stable diffusion :: generan imagenes o un dato revirtiendo la
  contaminación de éste por ruido.
- personally identifiable information PII :: information used to
  identify a person e.g. name, phone, face
- las ideas sobre transferlearning sobre el tamaño del dataset y el
  orden en que se ejecutan o son nuevas o son equivocadas no las
  conozco y debo averiguar. Por ejemplo se afirmó que fine tuning se
  hace luego de transfer learning. De acuerdo a Gemini Fine Tuning es
  un método específico dentro de transfer learning.
- espacio latente :: está relacionado al concepto de embeddings
- métricas modelos generativos:
  - inception score IS (img) :: evalua imagenes eneradas segin la
    probabilidad de pertenecer a distintas categorías. Quería generar
    gatos son gatos o no.
  - frechet inception distance FID (img) :: mide distancia entre las
    distribuciones de las imágenes relates y las generadas.
  - bleu (texto) :: similitud del texto generado y el de referencia
  - Rouge (texto) :: similitud del texto generado y el de referencia
- injusticia entre comparar lo que la máquina hace con respecto a lo
  que hace el humano
- gold standard :: evaluación inteligente realizada por humanos u otras IA
- test de turing :: aplica como medida en IA generativa?
** Notes
- recopilación de datos:
  - grandes volumenes
  - datos diversos
  - datos ricos
  - requiere pre procesamiento
- preprocesamiento es una tarea adecuada
- privacidad y seguridad :: los datos han de cumplir con PII con su
  respectiva controles de seguridad.
*** Entrenamimento de modelos
- el hardware a utilizar
- el tiempo requerido se refleja en el tamaño del dataset, la
  complejidad del modelo y el número de rondas de entrenamient
- el costo
- técnicas avanzadas de entrenamiento:
  - Transfer learning :: transfiere conocimiento de una tarea a otra
  - fine tuning :: es un tipo de transfer learning para un dataset más
    pequeño.
  - human in the loop :: ajusta las respuestas con lo que el humano da feedback
  - embeddings :: representaciones únicas de entidades de
    datos. representan la información de manera compacta
*** Evaluación de modelos:
- en modelos generativos cómo se evalúa su desempeño
- evaluación discriminativa :: puede usar precisión/accuracy. Pero no
  son aplicables para medir la creatividad del modelo
- en general se mide el progreso del modelo en el tiempo
- criterios de comparación entre diferentes versiones del modelo
- comparación del modelo con el rendimiento humano
- hay metricas específicas para texto, imagen o audio
- la comparación con el rendimiento humano. comparación de
  habilidades. se puede suponer que es una comparación injusta. por
  ejmplo, comparar si el producto generado por la IA debe ser
  comparado con el trabajo humano.
- una relativa amenaza sobre las habilitades humanas
** TODO Métricas de Evaluación de Imagenes generadas por GAN (FID y LPIPS) [100%]
- [X] Investigación de FID
- [X] Investigación de LPIPS
- [X] Generar documento
** Summary
Para realizar el entrenamiento de modelos de machine learning es
importante partir de obtener datos. En ocaciones esto puede requerir
autorizaciones ya que pueden estar involucrados datos personales.

Con los datos obtenidos pasamos a la etapa de procesamiento que
siempre es necesaria y a la de entrenamiento en donde se ha de
observar el hardware a utilizar y los costos involucrados por el
tiempo de computación. Se puede seleccionar entre técnicas como
transfer learnign o fine tuning. La primera, se aplica cuando el
dataset es pequeño o similar al original y consiste en reutilizar los
pesos ya entrenados. Usando al modelo básicamente como un extractor de
características para luego sustituir las etapas de salida con una red
simple para completar el problema. En cambio en Fine Tuning, que se
usa cuando tenemos suficientes datos, se trata de re-entrenar el
modelo por completo. Se puede decir que se usa los pesos
pre-entrenados como puntos de partida. Finalmente, se puede combinar
transfer learning y fine tuning al congelar unas capas y ajustar otras.

Finalmente está la evaluación de los modelos. En donde se ha de tener
en cuenta que en modelos discriminativos, se tiene medidas
discriminativas como accuracy, precision, recall. En cambio en modelos
generativos en donde se pretende a veces medir la creatividad del
modelo existen otras métricas como Inception Score, Frechet Inceptin
Distance, Bleu Rouge y finalmente comparar con un humano.

* 2024-10-09 Exposición métricas y  Representación del Conocimiento
** Questions and keywords
- n-grams :: secuencia consecutiva de texto. E.g. separar en bigramas
  la oración Lenin se cree astuto. [Lenin se], [se cree] [cree astuto]
- BLEU :: medir similitud de texto. Valor cercano a 1 implica textos similares
- ROUGE :: Se centra en la recuperacón de información. Cuando es 1 son idénticos si es menor, difiere.
- ¿se verificó el cómputo de ROUGE y METEOR? ::
- decoder :: 
- encoder ::
- hay algun limite para el tamaño del espacio latente? ::
- espacio latente ::
- que tan complicado es generar una fake imagen medica ::
- que tan raro es hacer una genreación en el tiempo ::
- cómo funciona pytorch a nivel de autoencodres, vaes ::
- se puede establecer una relacion entre la entropia de shannon y el espacio latente ::
- cómo se llama el paper de Jona :: un espacio latente independiente
  del tipo de dato. es sobre representación platónica. asumen que es
  parte de la realidad. Mientras es más completo el modelo converge el
  tema de lenguaje y de imagen para generar una imagen de manzana y
  viceversa. The Platonic Representation Hypothesis
- modelado paramétrico :: 
- likelihood o verosimilitud :: identificar parámetros que maximicen la probabilidad
- MLE :: máxima verosimilitud
- densidad tractable :: se define de antemano: normal bernoulli
- densidad aproximada :: depende de los datos
- que se debe hacer en la tarea? :: resumen analisis discusion?
** Notes
*** Exposicion
- METEOR parece ser superior a BLEU
- Valores altos de METEOR implica una alta similitud del texto generado con el original
- METEOR parece ser superior que ROUGE 
*** Represetacion del conocimiento
- Reducir el espacio de alta dimensionalidad a un espacio latente con menores dimensiones
- el espacio latente aprende representaciones simplificadas de datos
- los nuevos datos son variar coordenadas en el espacio latente
- este movimiento en el espacio latente podría por ejemplo afectar las
  expresiones faciales de un rostro si es una ia generativa de rostros
  o avegentar o rejuvenecer
- el espacio latente permite explorar **relaciones abstractas** entre los datos
- un modelado paramétrico es una familia de distribuciones de probabilidad
- la verosimilitud se calcula con el logaritmo para tener una suma de
  los logaritmos de las probabilidad
- MLE selecciona que valores de parámetros $\theta$ que maximizan la verosimilitud
- hay varios approaches para el modelado de la función de
  densidad. esto se conoce como taxonomía
** Summary
*** Métricas IAG
Inception Score y Freched Inception Distance:
- se usan en imagenes
- valor alto de IS indica que son de alta calidad y de clases diferentes
- FID compara la distribucion de las características de las imágenes reales y las generadas
- Un valor bajo de FID indica que las imágenes generadas son similares a las reales.
- BLEU y ROUGE son para evaluación de texto generado.
- BLEU usa n-grams (secuencia de palabras)
- BLEU más cercano a 1 significa mayor similitud entre el generado y el de referencia
- ROUGE se enfoa en la recuperación de información relevante.
*** Representación del conocimiento
El espacio latente permite hacer una representación útil y de menor
dimensionalidad de los datos de un fenómeno. Es decir, una
representacion simplificada de los datos. Al traducir los datos por
medio del modelo a un espacio latente se pueden hacer operaciones
matemáticas sobre los mismos que pueden devenir en nuevas
representaciones. Es decir, se establecen relaciones abstractas.  El
MLE o estimación de máxima verosimilitud, es un método estadístico que
permite determinar los parámetros más probables de un modelo. Es decir
encontrar la mejor configuración de parámetros que ajustan el modelo a
sus predicciones.
** TODO Tarea [100%]
- [X] leer el paper del aula virtual. Presentar conclusiones.
* 2024-10-14 Capitulo 2 Redes Neuronales
** Questions and keywords
- redes neuronales ::
- axones :: 
- dendritas ::
- funciones de activación ::
- relu :: $f(z) = \max(0,z)$
- revisar ajuste de pesos del perceptron ::
- gradient descent ::
- stochastic gradient descent ::
- minibatch ::
- red neuronal se puede aplicar en aprendizaje por refuerzo :: las
  salidas son acciones, pero que son las entradas
- algoritmo de back propagation ::
- ¿se menciona el problema del gradiente cuando las capas son muy grandes :: el
  gradiente desaparece por la profundidad
- deep neural netowrks :: tiene 3 o más capas ocultas
- 0 :: negro
- 255 :: blanco
- kernel :: ??
- lo que no mencionan es que los valores de la convolución también se aprenden 
** Notes
*** Exposición Jona audio
- FAD :: Freched Audio Distance. Similar al FID pero con audio.
  - utiliza todo el dataset 
- Signal to noise ratio :: cuando el valor es más alto la señal es más
  nitida. Es el cociente de la Potencia de la señal dividido para la Potencia del ruido. Se toma logaritmo
  - es fácil de calcular
  - es estandard
  - sus dificultades que no captura todas las caracteristicas perceptuales el audio
*** Capítulo 2 redes neuronales artificiales
- son una analogía de las redes nueronales biológicas
- el perceptron es usar una recta para separar las clases (boundary regions)
- $h(x_1,x_2)= g(w_0+w_1x_1+w_2x_2)$
- funcioes de activacion:
  - step function
  - sigmoid
  - relu
- aplicaciones son logica binaria como la compuerta lógica OR en función de la tabla de verdad
- algoritmo de descenso del gradiente. Sugiero revisar cómo opera el
  algoritmo para hacer el ajuste de pesos. Se parte aleatoriamente y se usa la gradiente
- el perceptron separa las clases en forma lineal
- las rdes neuronale multicapas son para problemas que no son
  linealmente dependientes. Para resolver en casos que la región a
  separar no es lineal. Ejemplo son círculos concéntricos
- el algoritmo de backpropagation se usa cuando existen varias capas.
- Overfitting sobre ajuste de los datos :: la red presenta una metrica
  de evaluacion con un muy buen score en los datos de entrenamiento
  pero pobre en los datos de testeo
- Dropout :: desactiva neuronas aleatoriamente durante el entrenamiento
- Frameworks de DeepLearning:
  - Pytorch
  - Tensorflow [[https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.64829&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false][tensorflow-playground]]
*** Computer vision
- imagen :: matriz de pixels con valores en tres capas RGB en un rango de 0 a 255
- convolución :: filtro que usa un kernel que extrae características
  de una región de la imágen. algo parecido a un detector de bordes
- la convolucion permite disminuir el tamaño de la matriz de la imagen
  y obtener valores segun el conocimiento local
- el detector de borde:
  |----+----+-----|
  | -1 | -1 |  -1 |
  |----+----+-----|
  | -1 |  8 |  -1 |
  |----+----+-----|
  | -1 | -1 | - 1 |
  |----+----+-----|
- pooling :: obtiene un valor del resultado de la convolucional e.g. el max-pooling
- flattening :: 
- los pasos de convolucion y pooling se repitenn varias
  veces. mientras estoy raliezando convolution and pooling estoy en
  low level features, es decir reconociendo curvas y bordes y al final
  de la red ya tengo una operación de alto nivel que sería reconocer objetos.
- redes neuronales convolucionales sirven para reconcoer objetos en imagenes 
** Summary
Las redes neuronales artificiales son una simplificación matemática de
las neuronas biológicas. En una red neuronal se usa una función de
activación que le da un caracter no lineal para activar o desactivar a
la neurona. El aprendizaje consiste en exponer la red a pares de
ejemplo, respuesta y ajustar los valores de los pesos que conectan las
diferentes unidades para obtener el mejor erendimiento.  Existen
diferentes funciones de activación que buscan tanto mitigar los
problemas del /vanishing gradient/ como dotar de no linealidad a la
red. El entrenamiento de una red puede verse afectado por el
/overfitting/ o sobre ajuste que es una condición en la que el modelo
predice muy bien sobre los datos de entrenamiento. Pero falla cuando
usamos el dataset de prueba.

En cuanto a visión por computador, inicialmente se usaron filtros
definidos para extraer características de las imágenes. Mas tarde con
las redes convolucionales, este tema cambiaría ya que permiten obtener
una autoextracción de características que son aprendidas por la
red. Existen diferntes

|-----------+-----------|
| Red       | Num Capas |
|-----------+-----------|
| vgg16     |        16 |
| YOLO      |        26 |
| GoogleNet |        22 |
|-----------+-----------|

* 2024-10-16 Redes ConvNet VGG
** Questions and keywords
- embeddings ::
- en verdad se puede usar una red convolucional en series de tiempo? ::
- segun la profesora que no va redes neuronales recurrentes en series de tiempo?? :: 
- espectograma ::
- bloques residuales :: 
** Notes
*** CNN
- importante la invarianza a la traslación
- las convnets son versatiles par usar modelos pre-entrenados
- la primeras capas detectan caracteristicas simples como bordes
- el modelo de redes neuronales convolucionales pueden adaptarse a
  otros problemas porque reconocen patrones
- invarianza a la posición
- procesan datos en secuencia???
- procesan datos en paralelo???
- modelos pre-entrenados:
  - vgg16:
    - Desarrollado en Oxford
    - Uso repetido de convolucionales con filtros de $3 \times 3$
    - 16 capas : 13 conv + 3 densas
    - capas de max pooling
    - el vector de caracteristica 4096
  - resnet50
  - inceptionv3
*** Series de tiempo
- determinar coomo funcionan las convnets en series de tiempo
*** Speech recognition
- Sugiere representar el sonido como un espectograma y tratarlo como
  una imagen para usar las convolucionales.
  
** Summary
Las convolucionales son intersantes porque extraen características
locales y son invariables a la translación. También son
jerárquicas. Existen modelos como vgg16, inception, resnet.  Un
embedding es una representación numérica de un dato. Por ejemplo, se
puede convertir una palabra de una cadena de texto a un vector
numérico. Permiten reducir la dimensionalidad. Se debe tomar en cuenta
que en cambio si representamos cada palabra de un texto en una
codificación one-hot, se consumirá mucha más memoria.

- word2vec:
  - skip-gram: predecir el contexto dada una palabra
  - CBOW: el continuos bag of words es predecir una palabra dada las de contexto
- glove: usa matriz de co ocurrencia
** TODO Tareas
- [-] consulta sobre embeddings aula virtual
  - [ ] word2vec
  - [ ] glove
  - [X] bert
* 2024-10-21 Redes Resneet
** Questions and keywords
- bloque residual :: skip connections, pasar los valores a una parte
  más adelante de la red para mitigar la perdida del gradiente
- revisar el ejemplo de calculo en los slides
- ¿permite el bloque residual aumentar el número de capas de una red?
- en los slides se menciona que fine tuning es reentrenar por completo
  pero partiendo de los pesos pre-entrenados
- ¿resnet 150 dice que tiene 177 capas?
- congelamiento de capas :: sólo entrena las últimas capas (extractor de características)
** Notes
- La red Resent aprende la diferencia entre la función sin activación y la entrada $f(Wx+b)-x$
- Ejemplo x=2, W=0.5 y b=1. Considere Relu $f(z)=max(0,z)$
- evita que la gradiente se desvanezca al utilizar el residuo
- la neurona residual puede mantener el valor original sin cambios (salida cercana a la entrada)
- asegura que fluya el gradiente
- Ejemplos de redes resnet es ResNet50, ResNet152
- Se puede usar para hacer fine-tuning
- se puede usar en freeze layers
- Resnet son utilizadas en aplicacinoes de visión por computador (e.g. detección de objetos)
- Fine - Tuning:
  - Tiene más probabilidad de Overfitting si los datos son pocos
  - es adecuado si se tiene un conjunto grande de datos
- Layer Freeze:
  - para conjuntos de datos de pocos samples
- Ejercicio 1 clasificación de imágenes con freeze layers
** Summary
Son redes neuronales profundas que mitigan el problema del vanishing
gradient gracias a las skip connections o conexiones de salto que
pemiten conectar directamente una capa a otra. El bloque residual
obliga, entonces, a la red a aprender la diferencia entre la entrada y
salida al pasar la entrada directamentea a la salida del bloque sin
afectarla de conversiones, operaciones y demas. Es decir aprende la
diferencia entre $h(x)-x$. Obliga a la red a aprender pequeñas
correcciones en vez de complejas transformaciones. con esto simplifica
el aprendizaje reduciendo complejidad.

Si se compara transfer learning  fine tuning se tiene que:
- transfer learning o congelación de capas es más rápido.
- transfer learning sólo entrena las últimas capas
- transfer learning ideal para datasets pequeños
- fine tuning ajusta todas las capas
- fine tuning es más demoroso. ajusta todos los pesos
- fine tuning adecuado cuando se tienen suficientes datos
- fine tuning es más suceptible a overfiting sin pocos datos. Transfer
  learning mitiga el overfitting.
  
* 2024-10-23 GoogleNet e Inception
** Questions and keywords
- inception bloque ::
- hierarchical sofmax :: se uso en word2vec pero podriamos usar para
  un pruning?
- word2vec utiliza CBOW(continuos bag of words) ::
- one-hot vector ::
- skip-gram :: predice las palabras de contexto dadas una palabra
- word2vec se entrena para cada corpus o ya está entrenado? ::
- cómo usar word2vec en español ::
- co-ocurrence matrix ::
- context window ::
- glove se usa en calculo de similitud :: 
** Notes
*** GoogleNet Inception
- los bloques inception rducen el costo computacional
*** Ejercicio
- consiste en modificar la estructura del Inception
- se usa el cifar 10
*** Exposiciones - word2vec
- word2vec permite la matematizacion del lenguaje
- usa una red neuronal de una capa
- crea dos matrices principales
- permite hacer analogías de palabras:
  Rey - Hombre + Mujer = Reina
- el coseno de similitud permite saber que las palabras son
  similares. esto permite verificar que los embeddings funciona
- contexto limitado
- capa de entrada: palabras actual
- capa oculta son las neuronas de los embeddings
- tiene dos arquitecturas CBOW y skip-gram
- el objetivo de CBOW es maximizar la probabilidad de predecir la
  palabra objetivo dada las palabras de contexto
- skip-gram:
  - inpt: palabra
  - capa oculta: 
  - salida: es el embedding resultante
*** GLoVe
- Stanford
- trabaja relaciones semanticas reina es a rey y sintácticas big a bigger
- su nucleo es una matriz de co-ocurrencia
- la matriz de co-ocurrencia cuenta las palabras en una ventana de contexto pares
- lo malo es que requiere una gran cantidad de datos  para entrenar
- costoso  para matrices grandes
** Summary
GoogleNet o Inception utiliza la aplicación de varios kernels en
paralelo para extraer características de la imagen. Es decir, extrae
características de diferentes escalas simultáneamente. 
** 2024-10-28 YOLO
** Questions and keywords
- YOLO :: 
- object detection :: identificar y localizar objetos en una imagen/escena
- mAP :: mean average precision
- idea :: combinar yolo y redes de grafos para poder explicar escenas
  es mejor que transformers que combinen texto e imagen
** Notes
- Es un modelo pre-entrenado
- detecta uno o múltiples objetos usando bounding boxes
- Ventajas: preciso, abierto, rápido
- 91 fps para YOLO high
*** Yolo v1
- 24 capas, 4 max pooling y 2 capas
- input 448x448
- usa Relu a las capas intermedias
- capa de salida usa activación lineal para obtener coordenadas
- usa batch normalization
- usa:
  - bloques residuales: divide la imagen en un acuadricula y predice
    la probabilidad de un objeto en las celdas
  - regresion de cajas delimitadores: poner las coordenadas de los objetos
- se queda con la caja que tenga mayor precision del objeto
*** Batch Normalization
- ttoma por bloques los datos y para que tengan una media cercana a 0 y varianza cercana a 1
- es normalizar los valores
- evita el vanishing gradient y el exploding gradient
- reduce dependencia d einicio de pesos
- reduce sobre-ajuste
- limita la variación del gradiente
*** Métrica IoU
- Es el area de intersección sobre area de unión
- Mientras más cercano a 1 excelente superposición
- cercano a 0, no hay objeto
** Summary
YOLO es un modelo de computer vision que se emplea en tareas de real
time de detección de objetos. Es intersante que combina clasificación
y regresión para proveer de la clase del objeto así como el bounding
box mediante las coordenadas $x$, $y$ y el $w$, $h$
** TODO [%]
- app para celular, usar version 5
- checar la aplicacion de detección de melanoma con YOLO
- Análisis papers version 3 y 4 (2 papers). Son papers de Arxiv
- Prueba el proximo miercoles

* 2024-10-30 RNN
** Questions and keywords
- RNN :: recurrent neural network
- consultar la clasificación de muchos a uno y muchos a muchos en RNN
- encoder :: representar la información en un embedding
- decoder :: reconstruye la información de salida
- unigram :: de uno en uno
- bigram :: de dos en dos recorriendo
- trigram :: de tres en tres
- token :: unidad minima de analisis
- preguntar a jonathan de donde saco la funcion print examples 
** Notes
- retener información de los datos anteriores
- son redes que trabajen en una secuencia de datos
- retiene información de palabras anteriores
- este diseño ayuda a comprender el contexto
- RNN tiene una estructura d emuchos a uno
*** Modelos de secuencia a secuencia
- son utiles en procesameinto de lenguaje natural
- en computer vision para comprender escenas e.g. en vídeo
- Muchos a uno :: una sola salida por ejemplo un clasificador de sentimentos
- muchos a muchos:
  - Generador de Texto
  - Traductor de Texto: formado por un *encoder* y un *decoder*
  - Modelos de Lenguaje: generan texto. Predeice la siguiente palabra. 
** Ejercicio
Comparar el número de parámetros de una ANN wrt RNN:
1. vocabulario de 10000
2. usan una capa densa de 256 unidades
3. Usar summary para ver la arquitectura y el número de parámetros
4. embedding de 64 para RNN
[[https://colab.research.google.com/drive/1Rid1ImfCW1UqvslNzztA0lUae-VK2qT8?hl=es#scrollTo=xPYsGMEb5sdU][colab-compare-rnn-y-ann]]
*** Modelos de lenguaje
- Buscan predecir la probabilidad de secuencias de palabras se usan: unigram, bigram y trigram
- En general es calcular la probabilidad de la palabra actual en función de las anterioresl.
- skip-gram: calcula probabilidad de palabras cercanas. tantos anteriores y posteriores
- las redes neuronales usan /Softmax/
*** Capa de embedding
- recibe secuencia de tokens que representan el texto a procesar
- se convierte las palabras en indices o vectores
- la capa converte a cada token en un vector de números que captura relaciones semánticas
- palabras con significados similares tendran representaciones vectoriales cercanas
*** Capa RNN
- son celdas que procesan la secuencia
- permiten al modelo recordar información anterior
- la salida es una probabilidad o una rpedicción de clases
*** Diccionarios de vocabulario
1. obtener palabras únicas
2. se crea un diccionario donde cada palabra es la clave y el indice es el valor
3. se puede obtener el diccionario inverso
*** Ejercicio 1
construir un diccinario por palabras y por indices a partir de las sheldon_quotes
#+begin_src python :results output :exports both :session
sheldon_quotes = ["You're afraid of insects and women, Ladybugs must render you catatonic.",
                  'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.']
all_words = ''.join(sheldon_quotes).split(' ')
print(all_words)
#+end_src

#+RESULTS:
: ["You're", 'afraid', 'of', 'insects', 'and', 'women,', 'Ladybugs', 'must', 'render', 'you', 'catatonic.Scissors', 'cuts', 'paper,', 'paper', 'covers', 'rock,', 'rock', 'crushes', 'lizard,', 'lizard', 'poisons', 'Spock,', 'Spock', 'smashes', 'scissors,', 'scissors', 'decapitates', 'lizard,', 'lizard', 'eats', 'paper,', 'paper', 'disproves', 'Spock,', 'Spock', 'vaporizes', 'rock,', 'and', 'as', 'it', 'always', 'has,', 'rock', 'crushes', 'scissors.']

#+begin_src python :results output :exports both :session
unique_words = list(set(all_words))
print(len(unique_words))
#+end_src

#+RESULTS:
: 35

#+begin_src python :results output :exports both :session
index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}
print(index_to_word.items())
#+end_src

#+RESULTS:
: dict_items([(0, 'Ladybugs'), (1, 'Spock'), (2, 'Spock,'), (3, "You're"), (4, 'afraid'), (5, 'always'), (6, 'and'), (7, 'as'), (8, 'catatonic.Scissors'), (9, 'covers'), (10, 'crushes'), (11, 'cuts'), (12, 'decapitates'), (13, 'disproves'), (14, 'eats'), (15, 'has,'), (16, 'insects'), (17, 'it'), (18, 'lizard'), (19, 'lizard,'), (20, 'must'), (21, 'of'), (22, 'paper'), (23, 'paper,'), (24, 'poisons'), (25, 'render'), (26, 'rock'), (27, 'rock,'), (28, 'scissors'), (29, 'scissors,'), (30, 'scissors.'), (31, 'smashes'), (32, 'vaporizes'), (33, 'women,'), (34, 'you')])

#+begin_src python :results output :exports both :session
word_to_index = {wd:i for i, wd in enumerate(sorted(unique_words))}
print(word_to_index.items())
#+end_src

#+RESULTS:
: dict_items([('Ladybugs', 0), ('Spock', 1), ('Spock,', 2), ("You're", 3), ('afraid', 4), ('always', 5), ('and', 6), ('as', 7), ('catatonic.Scissors', 8), ('covers', 9), ('crushes', 10), ('cuts', 11), ('decapitates', 12), ('disproves', 13), ('eats', 14), ('has,', 15), ('insects', 16), ('it', 17), ('lizard', 18), ('lizard,', 19), ('must', 20), ('of', 21), ('paper', 22), ('paper,', 23), ('poisons', 24), ('render', 25), ('rock', 26), ('rock,', 27), ('scissors', 28), ('scissors,', 29), ('scissors.', 30), ('smashes', 31), ('vaporizes', 32), ('women,', 33), ('you', 34)])
*** Ejercicio 2
Dividir los textos en caracteres para poder hacer generación de texto
mediante dos secuencias.

Se requeire una funcion ~print_examples~ para que se pueda ver como se transforman los datos
#+begin_src python :results output :exports both :session
step = 2
chars_window = 10
next_chars = []
sentences = []
all_char = all_words
for i in range(0, len(all_char) - chars_window,step):
    sentences.append(all_char[i:i+chars_window])
    next_chars.append(all_char[i+chars_window])
print(next_chars)
#+end_src

#+RESULTS:
: ['catatonic.Scissors', 'paper,', 'covers', 'rock', 'lizard,', 'poisons', 'Spock', 'scissors,', 'decapitates', 'lizard', 'paper,', 'disproves', 'Spock', 'rock,', 'as', 'always', 'rock', 'scissors.']
*** Keras
- el padding permite colocar el vector de un mismo tamaño. Es decir si
  cada uno de los documentos del corpus no tienen la misma cantdiad de
  palabras padding equilibra esto.
[[https://colab.research.google.com/drive/1g7p2hjIwdAswOuKua0OB_37mLSm09XXs?hl=es#scrollTo=-kY3hT8WOWLC][colab-rnn-keras-problema3]]
** Summary

* 2024-11-06 RNN y LST
** Questions and keywords
- forward propagation ::
- vanishing gradeint :: gradientes con valores cercanos a 0
- simple rnn ::
- que pasa si se diseña una RNN que no comparta los pesos $W_a$? ::
- LSTM ::
- recurrent dropout :: aquel que se hace en las LSTM
- no me queda claro como comparar el one-hot y el embedding :: 
** Notes
- RNN son suceptibles al vanishing gradient
- Las GRU cells alivian el vanishing gradient
- las GRU usan un calculo y segun el valor de la compuerta GU toma el
  valor del estado anterior o el valor calculado
- LSTM reducen efectos de exploding y vanishing gradient
- LSTM guardan información de contexto en las celdas
- 0 olvida 1 actualiza
- la capa de **embedding** reduce la dimensión con respecto a
  **one-hot** en la vectorización del modelo de lenguaje
- sin embargo, embedding requiere un mayor entrenamiento
- Control del Overfitting con Dropout  en las LSTM 
*** Ejercicios
- vocabulary_size 8000
- wordvec_dim 100
- max_tex_len 200
- cargar datos del imdb
- usar pad_sequences seteado a max_tex_len para el padding
- para representar enone hot usar to_categorical(xtrain, vocabulary_size)
- hacer el one-hot tambien para el xtest
** Summary
* 2024-11-11 Multiclassification
** Questions and keywords
- binary crossentropy ::
- word2vec ::
- glove ::
- skip-gram ::
- CBOW :: 
- pad_sequences ::
- ngrams ::  divide una palabra en mas unidades por ejemplo para
  conjugaciones o palabras raras
- como se usa sparse categorical cross entropy ::
- revisar la interpretación del recall :: recuerdo que recall puede
  ser importante en casos como predecir cancer en vez de precision
  creo que por la idea de encontrar clases malignas
- qué significa o cómo funciona la matrix de confusion ::
- no comprendo como se puede usar el trade off de precision y recall :: al
  parecer se acepta si y solo si se supera un valor umbral. no queda claro
** Notes
- Uso de Softmax
- Uso de One-hot
- Uso de to_categorical y pd.Series
- FastText usa ngrams
- gensim libreria contiene word2vec y fasttext
- en keras un modelo multiclass se cambia loss con categorical
  crossentropy y la capa final que tiene tantas neuronas como clases y va a usar softmax
- observar la *paradoja del accuracy* en donde el valor puede ser algo
  sólo porque hay mayor cantidad de elementos de una clase
- uso de ~sklearn metrics~ para confusion matrix
- observar que el support en el reporte de clasificación tienen que
  ver con todos los ejemplos verdaderos de una clase
** Summary
* 2024-11-18 LSTM multiclass
** Questions and keywords
- NMT ::
- Aki :: proyecto con datos pre-entrenado para traducción de texto
- sent :: token para indicar inicio/fin de oración
- temperatura :: que tan aleatoria son las predicciones. Es un
  paralelo a la física. Mientras el valor es más alto es más
  creativo. 1 es neutro.
- chas_window :: la ventana de caracter controla la cantidad de
  caracteres nuevos? Parece que no el programa introduce un
  contador. Por el código, parece que chars_windows tiene que ver con
  el input_shape del tensor de entrada al modelo.
- ¿se puede usar accuracy en generación de texto? :: segun profesor se
  requeire itervención humana para evaluar la coherencia del texto gnerado
- por que el volcabulario en 5 en el ejemplo? ::
- como se declara el input shape en el programa de keras :: (chars_window, vocab)
- la capa lstm que no retorna secuencias ¿es un solo valor o una especie de flatten? :: 
** Notes
- Se usa modelos pre entrenados para generar texto
- Para poder comprender las oraciones es necesario hacer un pre procesamiento
*** Generación de texto
- la generación de texto con palabras requiere un dataset grande
- es más económico la generación de texto a nivel de caracteres pero
  tiene más errores
- Se usa funciones como ~index2char~ para convertir el índice numérico
  a su correspondiente caracter
- ~def scale sofmax~ :: normaliza la softmax
- El valor de temperatura, en código sencillo, vuelve caótico el resultado. Simula creatividad.
- Los modelos de generación de texto son similares a clasificación
- Si los resultados no son coeherentes se requeire más épocas de entrenamiento.
- **el ejemplo del taller toma bastante tiempo**
**** Preparacion de datos
1. transformar texto en secuencia de indices
2. X y Y estan en one hot
3. char window la mism alongitud de caracteres con padding en X
*** Ejercicio
- Frases de Sheldon
- configuracion inicial
  - cjars_window = 20
  - step = 3
  - m epochs 5
  - batch de 128
- obtener vocabulario
- transformar secuencias en caracterres y represetntaciones numericas
- definir el modelo
- compilar
- resumin
- entrenar
- generar texto
*** Neuro Machine Translation
- pasar de un lenguaje a otro automáticamente
- mas grande el texto requeire más contexto i.e. más unidades RNN
- Objetivo obtener un traductor de portugues a español
** Summary
* 2024-11-25 Neural Machine Translation
** Questions and keywords
- encoder ::
- decoder ::
- NMT ::
- No cacho para que es RepeatVector en el Encoder ::
- No cacho para que va el TimeDistributed en el Decoder :: añada una
  capa densa a cda una de las celdas
- cómo se interpreta el BLEU score ¿cuándo es bueno? ¿cuándo es malo? :: 
** Notes
- Formado por encoder y decoder
- El encoder toma la oración a traducir
- El decoder genera la oración traducida
*** Time Distributed
- Aplica una capa a cada elemento de una capa
*** BLEU
- Métrica para evaluar la calidad de las traducciones automáticas.
- una buena traducción tendria un valor de 0.4
- no captura la semantica de las oraciones
- se le incorpora una penalziación cuando la traducción es muy corta
** Summary
* 2024-11-29 LLM Conceptos
** Questions and keywords
- LLM ::
- huggingface ::
- dónde puedo obtener las descripciones de las salidas que tengo de la
  aplicación del pipeline? Supongo que el API debe indicar esto pero
  ayer que intenté me confundí
- que es sacremoses para tokenizacion ::
- cómo colocar la gpu en huggingface pipeline :: 
** Notes
- LLM está entre NLP y Deep Learning
- Proceso de cliclo de desarrollo tiene etapas:
  1. preprocesamiento de datos
  2. diseño modelo
  3. pre-entrenamiento y fine tuning
  4. evaluación
- Se utiliza la plataforma de Huggingface

*** Programa LLM
#+begin_src python :session :results output :exports both
from transformers import pipeline

text_classifier = pipeline(task="text-classification",
model="nlptown/bert-base-multilingual-uncased-sentiment")
text = "The service at Hotel Atena was amazing. Food was great and service warm"
sentiment = text_classifier(text)
print(sentiment)
#+end_src

#+RESULTS:
: Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.
: [{'label': '5 stars', 'score': 0.7138378024101257}]
*** Tareas LLM
1. Generación de Código
2. Reconocimiento de Intención
3. Reconocimiento de nombres
4. Clasificación de texto
5. Generación de texto
6. Question Answering
7. Traducción de un lenguaje a otro
** Summary
* 2024-12-02 Transformers
** Questions and keywords
- multihead attention ::
- feed fordward ::
- Pytorch usado por un mayor control? ::
- positional encoding :: usa funcion seno y coseno para determinar la
  posicion de lsa palabras ¿como funciona no es facil de entender? Los
  positional encodings registran o generan un embedding de las
  palabras según su posición. Esto al mezclar con embeddings permite
  generar un contexto tanto por la palabra y su posición
** Notes
- esta formado por dos stacks: encoder y decoder
- cada stack tiene mecanismos de atención y redes feed fordward
- no usa ni redes convolucionales ni recurrencia
- aplicaciones en traducción, resumenes y QA
- d_model :: dimensionalidad de los embeddings
- n_heads :: número de attention heads
- Arquitecturas
  - encoder a decoder T5, Bart
  - encoder only: bert, text classification, extractive, QA, generative
  - decoder only: gpt, text classification, extractive, QA, generative
*** Transformer sólo Decoder
- Genera texto
- Predice la siguiente palabra
*** Transformer Encoder-Decoder
- Para traducción y resumenes
- Entender el lenguaje y generar secuencias

*** Inicialización de un modelo de Transformer
#+begin_src python :session :results output :exports both
import torch
import torch.nn as nn
from torch.nn import Transformer
d_model = 512
n_heads = 8
num_encoder_layers = 6
num_decoder_layers = 6

model = Transformer(
    d_model=d_model,
    nhead=n_heads,
    num_encoder_layers=num_encoder_layers,
    num_decoder_layers=num_decoder_layers)
print(model)
#+end_src

#+RESULTS:
#+begin_example
/home/leningfe/miniforge3/envs/tfmlenv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
#+end_example
*** Mecanismos de Atención
- usa un *positional encodding*
- luego del positional encoding usa el embeding  que da como resultado el PE

#+begin_src python :session :results output :exports both
import math
class PositionalEncoder(nn.Module):
    def __init__(self, d_model, max_seq_length=512):
        super(PositionalEncoder, self).__init__()
        self.d_model = d_model
        self.max_seq_length = max_seq_length
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0)/d_model))
        pe[:,0::2] = torch.sin(position * div_term)
        pe[:,1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x):
        x = x+self.pe[:, :x.size(1)]
        return x                                             
                                                                           
#+end_src

#+RESULTS:

Creamos una instancion del posotional encoder y un tensor de embedings
de shape batch size x seq length por d_model
#+begin_src python :session :results output :exports both
batch_size = 32
max_seq_length = 100
d_model = 512
# print(d_model)
positional_encoder = PositionalEncoder(d_model, max_seq_length)

input_embeddings = torch.rand((batch_size, max_seq_length, d_model))
# print(input_embeddings)
output = positional_encoder(input_embeddings)
print(output.shape)
#+end_src

#+RESULTS:
: torch.Size([32, 100, 512])

#+begin_src python :session :results output :exports both
print(input_embeddings[0,0])
#+end_src

#+RESULTS:
#+begin_example
tensor([0.6081, 0.9769, 0.7945, 0.0643, 0.7223, 0.3077, 0.0153, 0.9961, 0.0778,
        0.2010, 0.3631, 0.9391, 0.4442, 0.0162, 0.5765, 0.4988, 0.2838, 0.6827,
        0.4558, 0.8558, 0.0719, 0.0426, 0.5466, 0.0261, 0.5751, 0.7336, 0.9868,
        0.7089, 0.6503, 0.3550, 0.8687, 0.3820, 0.5695, 0.8389, 0.0949, 0.4780,
        0.8735, 0.5548, 0.1398, 0.3678, 0.5104, 0.0246, 0.5948, 0.0869, 0.9552,
        0.9033, 0.9903, 0.6954, 0.8157, 0.5610, 0.0714, 0.8654, 0.0635, 0.7582,
        0.7158, 0.3857, 0.6658, 0.9398, 0.2020, 0.7560, 0.4201, 0.7357, 0.5870,
        0.5627, 0.1358, 0.9484, 0.2152, 0.1514, 0.0197, 0.9358, 0.8393, 0.9454,
        0.1244, 0.8846, 0.5036, 0.1352, 0.4524, 0.6291, 0.3131, 0.0735, 0.9112,
        0.3519, 0.1451, 0.2477, 0.3223, 0.5541, 0.2179, 0.5776, 0.3878, 0.6364,
        0.5451, 0.3754, 0.7048, 0.2516, 0.7220, 0.9948, 0.4145, 0.8489, 0.6077,
        0.3472, 0.3786, 0.0943, 0.0958, 0.1572, 0.7297, 0.3348, 0.3025, 0.5984,
        0.6922, 0.6454, 0.1049, 0.0921, 0.3050, 0.9036, 0.0769, 0.2932, 0.7392,
        0.0076, 0.9711, 0.2280, 0.8903, 0.3155, 0.4397, 0.2386, 0.7416, 0.2960,
        0.0485, 0.8445, 0.8494, 0.1073, 0.2935, 0.4069, 0.1032, 0.1477, 0.8783,
        0.8235, 0.4062, 0.6295, 0.5730, 0.6382, 0.5150, 0.0062, 0.9359, 0.9071,
        0.4807, 0.6514, 0.8544, 0.2755, 0.5661, 0.1407, 0.9734, 0.0778, 0.1769,
        0.3814, 0.9025, 0.9714, 0.6925, 0.1683, 0.6420, 0.8961, 0.6083, 0.8439,
        0.9429, 0.7977, 0.0801, 0.2538, 0.8550, 0.3171, 0.3534, 0.0039, 0.8873,
        0.4950, 0.3662, 0.5000, 0.2822, 0.4913, 0.4483, 0.6770, 0.2733, 0.2541,
        0.5970, 0.3519, 0.8552, 0.2011, 0.8889, 0.7723, 0.2493, 0.7438, 0.5231,
        0.3643, 0.0806, 0.7213, 0.1131, 0.4272, 0.8792, 0.2208, 0.2587, 0.7218,
        0.5927, 0.7434, 0.7329, 0.2761, 0.3349, 0.9534, 0.3022, 0.8019, 0.3126,
        0.0980, 0.0401, 0.9610, 0.5154, 0.9512, 0.3773, 0.6500, 0.7287, 0.4854,
        0.3232, 0.1685, 0.0809, 0.0320, 0.8069, 0.0171, 0.7063, 0.1448, 0.0268,
        0.8262, 0.1104, 0.7539, 0.8456, 0.8028, 0.6953, 0.5535, 0.0948, 0.9035,
        0.2928, 0.9571, 0.6772, 0.9627, 0.0325, 0.2681, 0.0192, 0.4569, 0.3524,
        0.4347, 0.5511, 0.7037, 0.6144, 0.1797, 0.4347, 0.7703, 0.1245, 0.3642,
        0.0195, 0.3807, 0.0531, 0.2573, 0.5339, 0.9965, 0.8214, 0.8436, 0.6968,
        0.1921, 0.1381, 0.7885, 0.1407, 0.6808, 0.8760, 0.0570, 0.6680, 0.0081,
        0.8767, 0.3993, 0.7001, 0.8803, 0.7764, 0.7945, 0.7102, 0.0525, 0.0381,
        0.8506, 0.9434, 0.4662, 0.2848, 0.1414, 0.1973, 0.0494, 0.0581, 0.1495,
        0.1465, 0.6537, 0.8925, 0.1242, 0.4013, 0.5540, 0.2915, 0.3083, 0.8323,
        0.5184, 0.5766, 0.1265, 0.2369, 0.0881, 0.0885, 0.5246, 0.6720, 0.6669,
        0.3492, 0.9437, 0.0266, 0.5169, 0.9614, 0.8183, 0.6549, 0.5436, 0.2179,
        0.0237, 0.3525, 0.0500, 0.0055, 0.3020, 0.9606, 0.8994, 0.1108, 0.4086,
        0.5177, 0.1637, 0.4564, 0.7367, 0.8234, 0.0778, 0.0732, 0.8256, 0.5933,
        0.2430, 0.6551, 0.4111, 0.7803, 0.9751, 0.0561, 0.1929, 0.8629, 0.7066,
        0.6408, 0.6288, 0.6640, 0.3480, 0.0654, 0.8879, 0.5475, 0.3027, 0.7648,
        0.3765, 0.5275, 0.2261, 0.1900, 0.4801, 0.5520, 0.1905, 0.4682, 0.3524,
        0.3766, 0.3640, 0.8123, 0.4022, 0.3799, 0.4229, 0.0032, 0.5058, 0.5945,
        0.3866, 0.1609, 0.7692, 0.5306, 0.3958, 0.3435, 0.9721, 0.2909, 0.4782,
        0.9641, 0.4744, 0.2830, 0.8419, 0.7016, 0.2596, 0.9251, 0.7823, 0.6292,
        0.7712, 0.1080, 0.2347, 0.9921, 0.0041, 0.1246, 0.4204, 0.5042, 0.9745,
        0.9802, 0.7607, 0.6566, 0.9966, 0.7924, 0.3334, 0.1043, 0.6350, 0.2559,
        0.3937, 0.5870, 0.7955, 0.0022, 0.4881, 0.5368, 0.8281, 0.2680, 0.1119,
        0.2998, 0.6414, 0.1477, 0.3362, 0.0992, 0.3399, 0.6109, 0.3680, 0.7974,
        0.8445, 0.1581, 0.5200, 0.5946, 0.2656, 0.0497, 0.0269, 0.3357, 0.6781,
        0.4509, 0.4417, 0.3459, 0.3856, 0.0384, 0.5275, 0.7893, 0.1675, 0.8023,
        0.2539, 0.7642, 0.6339, 0.3356, 0.4743, 0.4282, 0.0262, 0.8122, 0.4796,
        0.1123, 0.5530, 0.4775, 0.7892, 0.8328, 0.8213, 0.0368, 0.9375, 0.7253,
        0.9070, 0.3262, 0.8203, 0.7576, 0.6830, 0.9094, 0.6425, 0.1691, 0.3341,
        0.3675, 0.0810, 0.4691, 0.3270, 0.3672, 0.9186, 0.5373, 0.9727, 0.7833,
        0.1797, 0.4427, 0.3421, 0.2050, 0.5832, 0.1734, 0.7989, 0.4777, 0.3942,
        0.0780, 0.2902, 0.4780, 0.4708, 0.6286, 0.1207, 0.2710, 0.8466, 0.5553,
        0.1321, 0.9471, 0.4087, 0.2373, 0.8352, 0.2180, 0.6675, 0.4345, 0.7757,
        0.5565, 0.6442, 0.6346, 0.1318, 0.8238, 0.5734, 0.6299, 0.9936])
#+end_example

#+begin_src python :session :results output :exports both
print(output[0,0])
#+end_src

#+RESULTS:
#+begin_example
tensor([0.6081, 1.9769, 0.7945, 1.0643, 0.7223, 1.3077, 0.0153, 1.9961, 0.0778,
        1.2010, 0.3631, 1.9391, 0.4442, 1.0162, 0.5765, 1.4988, 0.2838, 1.6827,
        0.4558, 1.8558, 0.0719, 1.0426, 0.5466, 1.0261, 0.5751, 1.7336, 0.9868,
        1.7089, 0.6503, 1.3550, 0.8687, 1.3820, 0.5695, 1.8389, 0.0949, 1.4780,
        0.8735, 1.5548, 0.1398, 1.3678, 0.5104, 1.0246, 0.5948, 1.0869, 0.9552,
        1.9033, 0.9903, 1.6954, 0.8157, 1.5610, 0.0714, 1.8654, 0.0635, 1.7582,
        0.7158, 1.3857, 0.6658, 1.9398, 0.2020, 1.7560, 0.4201, 1.7357, 0.5870,
        1.5627, 0.1358, 1.9484, 0.2152, 1.1514, 0.0197, 1.9358, 0.8393, 1.9454,
        0.1244, 1.8846, 0.5036, 1.1352, 0.4524, 1.6291, 0.3131, 1.0735, 0.9112,
        1.3519, 0.1451, 1.2477, 0.3223, 1.5541, 0.2179, 1.5776, 0.3878, 1.6364,
        0.5451, 1.3754, 0.7048, 1.2516, 0.7220, 1.9948, 0.4145, 1.8489, 0.6077,
        1.3472, 0.3786, 1.0943, 0.0958, 1.1572, 0.7297, 1.3348, 0.3025, 1.5984,
        0.6922, 1.6454, 0.1049, 1.0921, 0.3050, 1.9036, 0.0769, 1.2932, 0.7392,
        1.0076, 0.9711, 1.2280, 0.8903, 1.3155, 0.4397, 1.2386, 0.7416, 1.2960,
        0.0485, 1.8445, 0.8494, 1.1073, 0.2935, 1.4069, 0.1032, 1.1477, 0.8783,
        1.8235, 0.4062, 1.6295, 0.5730, 1.6382, 0.5150, 1.0062, 0.9359, 1.9071,
        0.4807, 1.6514, 0.8544, 1.2755, 0.5661, 1.1407, 0.9734, 1.0778, 0.1769,
        1.3814, 0.9025, 1.9714, 0.6925, 1.1683, 0.6420, 1.8961, 0.6083, 1.8439,
        0.9429, 1.7977, 0.0801, 1.2538, 0.8550, 1.3171, 0.3534, 1.0039, 0.8873,
        1.4950, 0.3662, 1.5000, 0.2822, 1.4913, 0.4483, 1.6770, 0.2733, 1.2541,
        0.5970, 1.3519, 0.8552, 1.2011, 0.8889, 1.7723, 0.2493, 1.7438, 0.5231,
        1.3643, 0.0806, 1.7213, 0.1131, 1.4272, 0.8792, 1.2208, 0.2587, 1.7218,
        0.5927, 1.7434, 0.7329, 1.2761, 0.3349, 1.9534, 0.3022, 1.8019, 0.3126,
        1.0980, 0.0401, 1.9610, 0.5154, 1.9512, 0.3773, 1.6500, 0.7287, 1.4854,
        0.3232, 1.1685, 0.0809, 1.0320, 0.8069, 1.0171, 0.7063, 1.1448, 0.0268,
        1.8262, 0.1104, 1.7539, 0.8456, 1.8028, 0.6953, 1.5535, 0.0948, 1.9035,
        0.2928, 1.9571, 0.6772, 1.9627, 0.0325, 1.2681, 0.0192, 1.4569, 0.3524,
        1.4347, 0.5511, 1.7037, 0.6144, 1.1797, 0.4347, 1.7703, 0.1245, 1.3642,
        0.0195, 1.3807, 0.0531, 1.2573, 0.5339, 1.9965, 0.8214, 1.8436, 0.6968,
        1.1921, 0.1381, 1.7885, 0.1407, 1.6808, 0.8760, 1.0570, 0.6680, 1.0081,
        0.8767, 1.3993, 0.7001, 1.8803, 0.7764, 1.7945, 0.7102, 1.0525, 0.0381,
        1.8506, 0.9434, 1.4662, 0.2848, 1.1414, 0.1973, 1.0494, 0.0581, 1.1495,
        0.1465, 1.6537, 0.8925, 1.1242, 0.4013, 1.5540, 0.2915, 1.3083, 0.8323,
        1.5184, 0.5766, 1.1265, 0.2369, 1.0881, 0.0885, 1.5246, 0.6720, 1.6669,
        0.3492, 1.9437, 0.0266, 1.5169, 0.9614, 1.8183, 0.6549, 1.5436, 0.2179,
        1.0237, 0.3525, 1.0500, 0.0055, 1.3020, 0.9606, 1.8994, 0.1108, 1.4086,
        0.5177, 1.1637, 0.4564, 1.7367, 0.8234, 1.0778, 0.0732, 1.8256, 0.5933,
        1.2430, 0.6551, 1.4111, 0.7803, 1.9751, 0.0561, 1.1929, 0.8629, 1.7066,
        0.6408, 1.6288, 0.6640, 1.3480, 0.0654, 1.8879, 0.5475, 1.3027, 0.7648,
        1.3765, 0.5275, 1.2261, 0.1900, 1.4801, 0.5520, 1.1905, 0.4682, 1.3524,
        0.3766, 1.3640, 0.8123, 1.4022, 0.3799, 1.4229, 0.0032, 1.5058, 0.5945,
        1.3866, 0.1609, 1.7692, 0.5306, 1.3958, 0.3435, 1.9721, 0.2909, 1.4782,
        0.9641, 1.4744, 0.2830, 1.8419, 0.7016, 1.2596, 0.9251, 1.7823, 0.6292,
        1.7712, 0.1080, 1.2347, 0.9921, 1.0041, 0.1246, 1.4204, 0.5042, 1.9745,
        0.9802, 1.7607, 0.6566, 1.9966, 0.7924, 1.3334, 0.1043, 1.6350, 0.2559,
        1.3937, 0.5870, 1.7955, 0.0022, 1.4881, 0.5368, 1.8281, 0.2680, 1.1119,
        0.2998, 1.6414, 0.1477, 1.3362, 0.0992, 1.3399, 0.6109, 1.3680, 0.7974,
        1.8445, 0.1581, 1.5200, 0.5946, 1.2656, 0.0497, 1.0269, 0.3357, 1.6781,
        0.4509, 1.4417, 0.3459, 1.3856, 0.0384, 1.5275, 0.7893, 1.1675, 0.8023,
        1.2539, 0.7642, 1.6339, 0.3356, 1.4743, 0.4282, 1.0262, 0.8122, 1.4796,
        0.1123, 1.5530, 0.4775, 1.7892, 0.8328, 1.8213, 0.0368, 1.9375, 0.7253,
        1.9070, 0.3262, 1.8203, 0.7576, 1.6830, 0.9094, 1.6425, 0.1691, 1.3341,
        0.3675, 1.0810, 0.4691, 1.3270, 0.3672, 1.9186, 0.5373, 1.9727, 0.7833,
        1.1797, 0.4427, 1.3421, 0.2050, 1.5832, 0.1734, 1.7989, 0.4777, 1.3942,
        0.0780, 1.2902, 0.4780, 1.4708, 0.6286, 1.1207, 0.2710, 1.8466, 0.5553,
        1.1321, 0.9471, 1.4087, 0.2373, 1.8352, 0.2180, 1.6675, 0.4345, 1.7757,
        0.5565, 1.6442, 0.6346, 1.1318, 0.8238, 1.5734, 0.6299, 1.9936])
#+end_example
*** Self Attention
- Es uun producto escalar o transformaci[on lineal que se hace sobre las matrices de *query-key*
- A cada token se le ejecuta tres proyecciones lineales que son Q, K , V
- producto punto de Q, K
- softmax sobre el producto punto de Q, K nos da probabilidades
- estos scores se multiplican sobre V y generan una actualización a los valores de los embeddings
- la ventaja del multihead es que se ejecuta de manera paralela
*** Ejercicio Construir un transformer solo encoder
los hiperparametros a usar son
#+begin_src python :session :results output :exports both
num_classes = 3
vocab_size = 10000
batch_size = 8
d_model = 512
num_heads = 8
num_layers = 6
d_ff = 2048
sequence_length = 256
dropout = 0.1
#+end_src

#+begin_src python :session :results output  :exports both
class Persona():
    def __init__(self, nombre, edad):
        self.nombre = nombre
        self.edad = edad
    def saludar(self):
        print(f"Hola {self.nombre}")
    def set_name(self, name):
        self.name = name
    def set_age(self, age):
        self.age = age
    def get_age(self):
        print(f"My age is {self.age}")

person1 = Persona("Lenin", 99)
person1.saludar()
#+end_src

#+RESULTS:
: Hola Lenin

** Summary
* 2024-12-04 Transformer sólo Decoder
** Questions and keywords
- PositionWiseFeedForward ::
- Decoder layer :: atención cruzada
- self attention ::
- multiheadattention :: cada cabeza tiene un criterio diferentes. Calcula KQV
- mascara causal o máscara de atenttion :: 
** Notes
- Revisar la ayuda memoria de Transformers en [[https://posgrado-virtual.epn.edu.ec/pluginfile.php/7931/mod_resource/content/1/Tarjetas%20de%20memoria%20sobre%20Transformers.pdf][aula virtual]]
- La capa de decoder usa Atención Auto Regresiva
- Un modelo autoregresivo genera *tokens* secuencialmente
- Usa los tokens anteriores como contexto para su predicción
- los criterios de finalización puede ser una longitud máxima de secuencia o una palabra de stop EoS
- el sólo decoder usa máscaras para mejorar la predicción de la siguiente palabra
- para poder estimar la probabilidad de las palabras el decoder
  transformer usa capa lineal más softmax.
- 
*** Ejercicio Implementar un Transformer Encoder-Decoder
Usar secuencias randoms y los valores hiperparametros para probar un
transformer solo decoder para que pruebe con una secuencia random

#+begin_src python
num_classes = 3
vocab_size = 10000
batch_size = 8
d_model = 512
num_heads = 8
num_layers = 6
d_ff = 2048
sequence_length = 256
dropout = 0.1
#+end_src

Sobre los hiperparametros

- ~src_vocab_size~, ~tgt_vocab_size~: Vocabulary sizes for source and target sequences, both set to 5000.
- ~d_model~: Dimensionality of the model's embeddings, set to 512.
- ~num_heads~: Number of attention heads in the multi-head attention mechanism, set to 8.
- ~num_layers~: Number of layers for both the encoder and the decoder, set to 6.
- ~d_ff~: Dimensionality of the inner layer in the feed-forward network, set to 2048.
- ~max_seq_length~: Maximum sequence length for positional encoding, set to 100.
- ~dropout~: Dropout rate for regularization, set to 0.1.
** Summary
* 2024-12-09 Encoder Decoder
** Questions and keywords
- cross attention ::
- output embedding :: mapean a tokens de salida esperados
** Notes
- del encoder salen hidden states??? al decoder
- los output embeddings es unir dos partes ???
- Hay un ejercicio de implementar el trnasformer encoder decoder para
  probar en una secuencia random con los hiperparametros de la diapositiva.
*** Book Tunstall
- La idea de los mecanismos de atención es no producir un único estado oculto (i.e. single hidden state)
- La idea de los mecanismos de atención es generar un estado oculto en
  cada paso que el decoder puede acceder
- Para evitar que el decoder tenga ver todos los estados al mismo
  tiempo y generar una entrada muy grande se recurre a un mecanismo de
  priorización de estados.
- La atención es dar pesos a las salidas del encoder para dirigir la
  atención a los estados del encoder más relevantes.
- El transformador aprende alineamientos no triviales entre  palabras
- El mecanismo de atención opera sobre cada uno de los estados del
  decoder y el encoder y sus salidas van a una Feed Forward Neural Network
- 
** Summary
* 2024-12-16 LLMs Pre-Entrenadas Clasificación Resumen y QA
** Questions and keywords
- LLM ::
- HuggingFace ::
- pipeline :: ¿cómo se define o aque va el concetpo? vea primeras diapositivas
- AutoModel :: 
- AutoTokenizer :: preparacion del texto
- last hidden state :: representación oculta final del encoder
- es raro que no use el outputs.last_hidden_state ::
- logits :: puntuaciones sin normalizar que dan la probabilidad
** Notes
- Pipeline
  - interfaz sencilla de alto nivel
  - mayor abstracción
  - menor control y pesonalizacion
- ~from_pretrained~ cara los pesos
- Se utilizara las siguientes variables:
- AutoModel y AutoTokenizer permiten adaptar a tareas específicas
*** AutoModel
- tiene una capa de salida lineal
- se declara una clase para tomar los pooled output
AutoModel no proporciona una salida de clasificación o *classification
head*. Por esta razón es mejor añadirla creando una clase. Ahora esto
permite disponer de una maor personalización y control sobre la tarea,
dejando sólo la tarea de tokenización en el otro pipeline.
#+begin_src python :session :results output :exports both
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
text = "I am an example sequence for text classification"
#+end_src

Se declara la siguiente clase para ajustar el modelo a una tarea de
clasificacion
#+begin_src python
class SimpleClassifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(Simpleclassifier, self).__init__()
        self.fc = nn.Linear(input_size, num_classes)
    def forward(self, x):
        return self.fc(x)
#+end_src

Ahora para aplicar la predicción es necesario tokenizar el texto de
entrada y obtener el hidden state final

#+begin_src python :session :results output :exports both
 inputs = tokenizer(
      text, return_tensors="pt", padding=True,
     truncation=True, max_length=64
      )
 outputs = model(**inputs)
 pooled_output = outputs.pooler_output
 print("Hidden states size: ", outputs.last_hidden_state.shape)
 print("Pooled output size: ", pooled_output.shape)
 classifier_head = SimpleClassifier(
 pooled_output.size(-1), num_classes=2
 )
 logits = classifier_head(pooled_output)
 probs = torch.softmax(logits, dim=1)
 print("Predicted Class Probabilities: ", probs)
#+end_src


*** AutoModel for sequence classification
- facilita las tareas de clasificación
- ya tiene el classification head
- está preconfigurado
- sus salidas son los logits
- para obtener clases se usa ~argmax~ para saber en donde se tiene la mayor probabilidad
*** AutoModelForCausalLM
- Genera texto causal en función del texto anterior
- Ejemplo es el GPT-2
- Al hacer el ~decode~ se pasa por medio de los embeddings para
  traducir los vectores en palabras.
- Preconfigurado
- Auto-regresivo: generar nuevas palabras en base a las anteriores
- Usa un prompt?
*** Datasets HuggingFace
1. cargar el dataset ~load_dataset~
2. el DataLoader de Pytorch procesa los datos por lotes i.e. batches
3. Iterar los elementos a traves del DataLoader

Clasificación :: Para clasificación se tiene el dataset IMDB
generacion de texto :: Stanford NLP
*** Entrenamiento del LLM para generacion de texto
- Supervisado
- el texto original proporciona tanto la entrada como las labels
- una LLM en un proceso autoregresivo genera la siguiente palabra en
  base a la anterior.
*** Ejercicio Pipeline vs Clases Automáticas
- la 2 es la falsa
#+begin_src python :session :results output :exports both
 from transformers import AutoModelForCausalLM, AutoTokenizer
 from datasets import load_dataset

 model_name = "gpt2"
 tokenizer = AutoTokenizer.from_pretrained(model_name)
 model = AutoModelForCausalLM.from_pretrained(model_name)

 dataset = load_dataset("stanfordnlp/shp", "default")
 train_data = dataset["train"]

 prompt = train_data[0]["history"][:60]
 inputs = tokenizer.encode(prompt, return_tensors="pt")
 
 output = model.generate(inputs, max_length=29)
 generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

#+end_src

#+begin_src python :session :results output :exports both
print(prompt)
print(generated_text)
#+end_src


*** Ejercicios de clasificacion
#+begin_src python :session :results output :exports both
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "textattack/distilbert-base-uncased-SST-2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

text = ["awsome movie", "what an awful movie", "I regret it to have watched it"]

inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
outputs = model(**inputs)
logits = outputs.logits
print(logits)
predicted_classes = torch.argmax(logits, dim=1).tolist()
for idx, predicted_class in enumerate(predicted_classes):
    print(f"predicted class for {text[idx]}: {predicted_class}")
#+end_src

#+RESULTS:
: tensor([[-2.2862,  1.7893],
:         [-0.3728,  0.2801],
:         [ 0.6427, -0.4970]], grad_fn=<AddmmBackward0>)
: predicted class for awsome movie: 1
: predicted class for what an awful movie: 1
: predicted class for I regret it to have watched it: 0

*** Resumenes Texto
- extractivos :: reutiliza las palabras y usa un encoder
- abstractivos :: genera secuecnias diferentes y no necesariamente
  corresponde y usa un encoder y decoder

Observar los demas ejemplos en GitHub

*** Proceso de Traducción
- mapeo del idioma fuente al idioma objetivo
- se usa modelos de tipo encoder-decoder
- Ejercicio de Traduccion para redactar libro de frases en español.

** Summary

* 2025-01-06 Metricas de Evaluación
** Questions and keywords

** Notes
- Localidad de los modelos bajados de huggingface en
  - linux: ~/.cache/huggingface/hub
  - windows: c:\users\<username>\.cache\huggingface\hub
- Clasificación utiliza metricas tradicionales que comparan los ground truth y los valores predichos
- Para IAGenerativa se usa otras métricas
- Para otros casos considerar el uso posible de KPIs u otros para
  evaluar el rendimiento del modelo en un negocio
*** Preplexity
- mide la habilidad de un modelo para predecir la proima palabra en una secuencia de texto.
- una perplexity menor a 10 es un buen valor de ata confianza ... excelente
- perplexity de 10 a 50 es buena o aceptable
- perplexity superior a 100 no es un valor aceptable
- calcula la probabilidad condicional en logaritmo en base 2 de la
  palabra actual dado las anteriores. Es la eexponencial de la entropia cruzada.
*** BLEU
- es parecido a n-grams
- se puede comparar con mas de una referencia y devuelve mas de un valor de precision
- no evalua la gramatica ni el contexto
- se aplica para traduccion automatica, modelos de resumen y generacion de texto en genral
- En el ejercicio se puede notar que disminuir la cantidad de texto
  generado aumenta la probabilidad de tener un score mas alto
** Ejercicio Metricas básicas de Clasificación con Evaluate
#+begin_src python :session :results output :exports both

from evaluate import load

accuracy = load("accuracy")
precision = load("precision")
recall = load("recall")
f1 = load("f1")

real_labels = [0,1,0,1,1]
predicted_labels = [0,0,0,1,1]
acc_val = accuracy.compute(references=real_labels, predictions=predicted_labels)
precision_val = precision.compute(references=real_labels, predictions=predicted_labels)
recall_val = recall.compute(references=real_labels, predictions=predicted_labels)
f1_val = f1.compute(references=real_labels, predictions=predicted_labels)

print(f"acc: {acc_val}")
print(f"precision: {precision_val}")
print(f"recall: {recall_val}")
print(f"f1: {f1_val}")
#+end_src

#+RESULTS:
: acc: {'accuracy': 0.8}
: precision: {'precision': 1.0}
: recall: {'recall': 0.6666666666666666}
: f1: {'f1': 0.8}

** Ejercicio Métricas de Clasificación con PipeLine
Checkar porque parece que podemos resolver con PipeLine y con
AutoModel estan los dos pero creo que debe ser uno u otro
#+begin_src python :session :results output :exports both
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from evaluate import load

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
model = model.to(device)
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device=0)

new_data = ["this movie was terrible", "best movie ever"]

predictions = classifier(new_data)
predicted_labels = [1 if pred["label"]=="POSITIVE" else 0 for pred in predictions]
print(predicted_labels)
# tokenizar dato de entrada
new_input = tokenizer(new_data, return_tensors="pt", padding=True, truncation=True, max_length=64)
new_input = new_input.to(device)
with torch.no_grad():
    outputs = model(**new_input)

predicted = torch.argmax(outputs.logits, dim=1).tolist()
print(predicted)
# etiquetas ground truth

real = [0,1]

accuracy = load("accuracy")
precision = load("precision")
recall = load("recall")
f1 = load("f1")

acc_val = accuracy.compute(references=real, predictions=predicted)
precision_val = precision.compute(references=real, predictions=predicted)
recall_val = recall.compute(references=real, predictions=predicted)
f1_val = f1.compute(references=real, predictions=predicted)

print(f"acc: {acc_val}")
print(f"precision: {precision_val}")
print(f"recall: {recall_val}")
print(f"f1: {f1_val}")
#+end_src

#+RESULTS:
: [0, 1]
: [0, 1]
: acc: {'accuracy': 1.0}
: precision: {'precision': 1.0}
: recall: {'recall': 1.0}
: f1: {'f1': 1.0}
** Perplexity

#+begin_src python :session :results output :exports both
import torch
from evaluate import load
from transformers import AutoModelForCausalLM, AutoTokenizer

# revisando si la GPU esta disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_name = "gpt2"
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

tokenizer = AutoTokenizer.from_pretrained(model_name)

# configurando el padding token a eos_token
tokenizer.pad_token = tokenizer.eos_token
# Preparar el texto semilla
prompt = "Latest research findings in Antartica show"
input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
attention_mask = torch.ones(input_ids.shape, device=device)
# Generacion de texto
output = model.generate(input_ids,
                        max_length=45,
                        num_return_sequences=1)
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

# Probando el Perplexity Score
# se requiere tokenizar el texto generado
inputs = tokenizer(generated_text,
                   return_tensors="pt",
                   padding=True,
                   truncation=True).to(device)
inputs['attention_mask'] = torch.ones(inputs['input_ids'].shape, device=device)
# cargando el perplexity score
perplexity = load("perplexity", module_type="metric")

# results = perplexity.compute(predictions=generated_text, model_id="gpt2")
results = perplexity.compute(model=model,
                             input_ids=inputs['input_ids'],
                             attention_mask=inputs['attention_mask'],
                             pad_token_id=tokenizer.pad_token_id)
print(results)
print(results["mean_perplexity"])


#+end_src

#+RESULTS:
: The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
: Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
: Latest research findings in Antartica show that the presence of a single molecule in the brain is associated with a number of neurodegenerative diseases, including Alzheimer's disease, Parkinson's disease, and Huntington's disease.

** Summary

* 2025-01-08 Metricas ROUGE y METEOR
** Questions and keywords
- ROUGE ::
- stemmming :: palabras como good y well
- lemamatizacion :: raizes de palabras
** Notes
*** Rouge
- sirve para medir la similitudentre resumenesgenerados por modelos y por referencias humanas
- puntaje cercanos a 1 indican mayor similitud
- Requiere instalación
  #+begin_src shell
   pip install rouge-score
  #+end_src

#+begin_src python :session :results output :exports both
import evaluate

rouge = evaluate.load('rouge')

predictions = ["The cat sat on the mat."]
references = ["The cat is sitting on the mat."]

results = rouge.compute(predictions=predictions,
                        references=references)
print(results)
#+end_src

#+RESULTS:
: Downloading builder script:   0% 0.00/6.27k [00:00<?, ?B/s]Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 9.88MB/s]
: {'rouge1': 0.7692307692307692, 'rouge2': 0.5454545454545454, 'rougeL': 0.7692307692307692, 'rougeLsum': 0.7692307692307692}

Da un error al ejecutar
#+begin_src python :session :results output :exports both
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
scores = scorer.score(references, predictions)
print(scores)

#+end_src

#+RESULTS:

*** METEOR
- presenta metricas como f1-score, precision y recall
- se compara con BLEU que BLEU sólo usa palabras consecutivas
- observar que BLEU compara en funcion de 1 grama bi grama tri grama y
  tetragrama para indicar el desempeño
#+begin_src python :session :results output :exports both
from evaluate import load

bleu_metric = load('bleu')
meteor_metric = load('meteor')

predictions = ["He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures."]
references = ["He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures."]

results_bleu = bleu_metric.compute(predictions=predictions, references=references)
results_meteor = meteor_metric.compute(predictions=predictions, references=references)
print(f"BLEU: {results_bleu}")
print(f"Meteor: {results_meteor}")


#+end_src

#+RESULTS:
#+begin_example
Downloading builder script:   0%|                                        | 0.00/5.94k [00:00<?, ?B/s]Downloading builder script: 100%|███████████████████████████████| 5.94k/5.94k [00:00<00:00, 3.22MB/s]
Downloading extra modules:   0%|                                         | 0.00/1.55k [00:00<?, ?B/s]Downloading extra modules: 4.07kB [00:00, 3.85MB/s]                                                  
Downloading extra modules:   0%|                                         | 0.00/3.34k [00:00<?, ?B/s]Downloading extra modules: 100%|████████████████████████████████| 3.34k/3.34k [00:00<00:00, 10.4MB/s]
Downloading builder script:   0%|                                        | 0.00/7.02k [00:00<?, ?B/s]Downloading builder script: 100%|███████████████████████████████| 7.02k/7.02k [00:00<00:00, 18.0MB/s]
[nltk_data] Downloading package wordnet to /home/leningfe/nltk_data...
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/leningfe/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt_tab.zip.
[nltk_data] Downloading package omw-1.4 to /home/leningfe/nltk_data...
BLEU: {'bleu': 0.25928256340208583, 'precisions': [0.7, 0.3684210526315789, 0.2222222222222222, 0.11764705882352941], 'brevity_penalty': 0.9048374180359595, 'length_ratio': 0.9090909090909091, 'translation_length': 20, 'reference_length': 22}
Meteor: {'meteor': 0.6531090723751274}
#+end_example
*** Bert-Score
- el mismo principio pero usa word2vec y glovec
- se obtiene una mejor y mas correcta evaluacion con tecnologias nuevas
#+begin_src shell
pip install bert-score
#+end_src
#+begin_src python :session :results output :exports both
from evaluate import load
bertscore = load("bertscore")
predictions = ["The burrow stretched forward like a narrow corridor for a while, then plunged abruptly downward, so quickly that Alice had no chance to stop herself before she was tumbling into an extremely deep shaft."]
references = ["The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well."]

results = bertscore.compute(predictions=predictions,
                            references=references,
                            model_type="roberta-large")
print(f"Bert-Score: {results}")

# para meteor
meteor_score = load("meteor")
results_meteor = meteor_score.compute(predictions=predictions,
                                      references=references)
print(f"Meteor-Score: {results_meteor}")


#+end_src

#+RESULTS:
#+begin_example
Downloading builder script:   0%|                                         | 0.00/7.95k [00:00<?, ?B/s]Downloading builder script: 100%|████████████████████████████████| 7.95k/7.95k [00:00<00:00, 9.61MB/s]
WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.
tokenizer_config.json:   0%|                                               | 0.00/25.0 [00:00<?, ?B/s]tokenizer_config.json: 100%|████████████████████████████████████████| 25.0/25.0 [00:25<00:00, 1.04s/B]tokenizer_config.json: 100%|████████████████████████████████████████| 25.0/25.0 [00:25<00:00, 1.04s/B]
config.json:   0%|                                                          | 0.00/482 [00:00<?, ?B/s]config.json: 100%|████████████████████████████████████████████████████| 482/482 [00:00<00:00, 244kB/s]
vocab.json:   0%|                                                          | 0.00/899k [00:00<?, ?B/s]vocab.json: 100%|██████████████████████████████████████████████████| 899k/899k [00:00<00:00, 2.50MB/s]vocab.json: 100%|██████████████████████████████████████████████████| 899k/899k [00:00<00:00, 2.31MB/s]
merges.txt:   0%|                                                          | 0.00/456k [00:00<?, ?B/s]merges.txt: 100%|██████████████████████████████████████████████████| 456k/456k [00:00<00:00, 4.47MB/s]
tokenizer.json:   0%|                                                     | 0.00/1.36M [00:00<?, ?B/s]tokenizer.json: 100%|████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 10.2MB/s]tokenizer.json: 100%|████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 7.38MB/s]
model.safetensors:   0%|                                                  | 0.00/1.42G [00:00<?, ?B/s]model.safetensors:   1%|▎                                        | 10.5M/1.42G [00:01<02:19, 10.1MB/s]model.safetensors:   1%|▌                                        | 21.0M/1.42G [00:01<02:08, 10.9MB/s]model.safetensors:   2%|▉                                        | 31.5M/1.42G [00:02<01:41, 13.6MB/s]model.safetensors:   3%|█▏                                       | 41.9M/1.42G [00:03<01:34, 14.6MB/s]model.safetensors:   4%|█▌                                       | 52.4M/1.42G [00:03<01:21, 16.7MB/s]model.safetensors:   4%|█▊                                       | 62.9M/1.42G [00:04<01:14, 18.2MB/s]model.safetensors:   5%|██                                       | 73.4M/1.42G [00:30<19:15, 1.17MB/s]model.safetensors:  17%|███████                                   | 241M/1.42G [00:40<16:51, 1.17MB/s]model.safetensors:  18%|███████▍                                  | 252M/1.42G [00:40<02:51, 6.83MB/s]model.safetensors:  29%|████████████                              | 409M/1.42G [00:50<01:43, 9.78MB/s]model.safetensors:  40%|████████████████▋                         | 566M/1.42G [01:00<01:27, 9.78MB/s]model.safetensors:  51%|█████████████████████▎                    | 724M/1.42G [01:10<00:55, 12.6MB/s]model.safetensors:  74%|██████████████████████████████▌          | 1.06G/1.42G [01:30<00:28, 12.6MB/s]model.safetensors:  81%|█████████████████████████████████▎       | 1.15G/1.42G [01:10<00:09, 27.0MB/s]model.safetensors:  83%|██████████████████████████████████▏      | 1.18G/1.42G [01:13<00:09, 26.0MB/s]model.safetensors:  85%|██████████████████████████████████▊      | 1.21G/1.42G [01:14<00:08, 25.5MB/s]model.safetensors:  86%|███████████████████████████████████      | 1.22G/1.42G [01:40<00:08, 25.5MB/s]model.safetensors:  86%|███████████████████████████████████▍     | 1.23G/1.42G [01:15<00:08, 24.3MB/s]model.safetensors:  87%|███████████████████████████████████▋     | 1.24G/1.42G [01:16<00:07, 24.2MB/s]model.safetensors:  88%|███████████████████████████████████▉     | 1.25G/1.42G [01:16<00:07, 24.2MB/s]model.safetensors:  89%|████████████████████████████████████▎    | 1.26G/1.42G [01:17<00:07, 22.9MB/s]model.safetensors:  89%|████████████████████████████████████▌    | 1.27G/1.42G [01:17<00:06, 22.2MB/s]model.safetensors:  90%|████████████████████████████████████▉    | 1.28G/1.42G [01:18<00:06, 21.2MB/s]model.safetensors:  91%|█████████████████████████████████████▏   | 1.29G/1.42G [01:19<00:06, 19.7MB/s]model.safetensors:  91%|█████████████████████████████████████▍   | 1.30G/1.42G [01:20<00:07, 15.8MB/s]model.safetensors:  92%|█████████████████████████████████████▊   | 1.31G/1.42G [01:21<00:08, 13.3MB/s]model.safetensors:  93%|██████████████████████████████████████   | 1.32G/1.42G [01:22<00:07, 13.8MB/s]model.safetensors:  94%|██████████████████████████████████████▍  | 1.33G/1.42G [01:22<00:06, 14.9MB/s]model.safetensors:  94%|██████████████████████████████████████▋  | 1.34G/1.42G [01:23<00:04, 16.3MB/s]model.safetensors:  95%|███████████████████████████████████████  | 1.35G/1.42G [01:24<00:04, 16.4MB/s]model.safetensors:  96%|███████████████████████████████████████▎ | 1.36G/1.42G [01:24<00:03, 17.3MB/s]model.safetensors:  96%|███████████████████████████████████████▎ | 1.36G/1.42G [01:50<00:03, 17.3MB/s]model.safetensors:  97%|███████████████████████████████████████▌ | 1.37G/1.42G [01:25<00:02, 17.1MB/s]model.safetensors:  97%|███████████████████████████████████████▉ | 1.38G/1.42G [01:25<00:01, 19.1MB/s]model.safetensors:  98%|████████████████████████████████████████▏| 1.39G/1.42G [01:26<00:01, 17.3MB/s]model.safetensors:  99%|████████████████████████████████████████▌| 1.41G/1.42G [01:26<00:00, 17.9MB/s]model.safetensors: 100%|████████████████████████████████████████▊| 1.42G/1.42G [01:27<00:00, 19.3MB/s]model.safetensors: 100%|█████████████████████████████████████████| 1.42G/1.42G [01:27<00:00, 19.3MB/s]model.safetensors: 100%|█████████████████████████████████████████| 1.42G/1.42G [01:27<00:00, 16.2MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Bert-Score: {'precision': [0.9340652823448181], 'recall': [0.9245127439498901], 'f1': [0.929264485836029], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.46.3)'}
[nltk_data] Downloading package wordnet to /home/leningfe/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
[nltk_data] Downloading package punkt_tab to
[nltk_data]     /home/leningfe/nltk_data...
[nltk_data]   Package punkt_tab is already up-to-date!
[nltk_data] Downloading package omw-1.4 to /home/leningfe/nltk_data...
[nltk_data]   Package omw-1.4 is already up-to-date!
Meteor-Score: {'meteor': 0.37180012567275916}
#+end_example
*** Similitud de Textos
- se usan dos enfoques principales basados en diccinarios lexicos. no captura el contexto
- wordnet y meteor como que quedaron relegados
*** Evaluacion de QA
- metrica sensible
- requiere de referencias
- se usa metricas como Exact Match (EM)
#+begin_src python :session :results output :exports both
from evaluate import load
exact_match = load("exact_match")

predictions = ["The cat sat on the mat.",
               "Theaters are great.",
               "Like comparing oranges and apples."]
references = ["The cat sat on the mat?",
              "Theaters are great.",
              "Like comparing apples and oranges."]

results = exact_match.compute(predictions=predictions,
                              references=references)

print(f"Exact Match: {results}")

#+end_src

#+RESULTS:
: Downloading builder script:   0%|                                         | 0.00/5.67k [00:00<?, ?B/s]Downloading builder script: 100%|█████████████████████████████████| 5.67k/5.67k [00:00<00:00, 979kB/s]
: Exact Match: {'exact_match': 0.3333333333333333}
*** Similitud coseno
instalar:
#+begin_src shell
pip install tqdm
pip install gensim
#+end_src

- Hacer con GloVe, Sim-score y Word2vec como Tarea
- texto prueba

** Summary
** 2025-01-20 AutoEnconders VAE
** Questions and keywords
- Sobre el proyecto
  - 2025-01-27 Proyecto con Pruebas
  - 2025-02-05 Paper final
- Autoencoder ::
- espacio latente ::
- embedding vector ::
- encoder :: toma una imagen de eentrada y la mapea al espacio latente
- donde hay el bias  en la convolucional
- conv2dtr :: duplican la informacion
** Notes
- uso de keras
- autoencoders para generacion de imagenes con limitaciones
- variational autoencoder al usar un espacio probabilistico permite
  superar las limitaciones de autoencoders que pueden producir
  resultados extraños cuando los vectores estan entre dos categorías
- encoder y decoder se entrenan juntos formando el autoencoder
- enoder comprime datos
- encoder extrae caracteristicas
*** Autoencoder
- se entrena para codificación y decodficicacion usando el espacio latente
- Fashion Mnist
- En el autoencoder se crea un vector de embedding z que es el espacio latente
- en ese vector se codifica la información original
- la arquitectura del encoder permite obtener caracteristicas mas relevantes
- se reduce el numero de dimensoines especiales
- se generan objetos nuevos diferentes a las imagenes originales
- Ejercicio de AutoEncoder
  - aplicar la formula para calculaf el numero de parametros
- sirve para limpiar ruido
- reconstruye datos desde espacio latente
- 
*** Decoder
- se usa la convolución transpuesta para en vez de reducir la
  inforamción tener informaciones bidimensionales
- el inputlayer es la capa latente
