{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeninGF/CoursesNotes/blob/main/InteligenciaArtificalGenerativa/Problems/transformers/EjercicioTransformerEncoderDecoder-IAG-2024B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f32028e",
      "metadata": {
        "id": "5f32028e",
        "papermill": {
          "duration": 0.001948,
          "end_time": "2024-12-04T20:02:41.186222",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.184274",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Transformer Encoder Decoder\n",
        "\n",
        "\n",
        "\n",
        "Coder: Lenin G. Falconí\n",
        "\n",
        "\n",
        "\n",
        "Asignatura: Tópicos Especiales (Inteligencia Artificial)\n",
        "\n",
        "\n",
        "\n",
        "Fecha: 2024-12-11"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar un transformer Encoder Decoder se requiere de cross attention para conectar las capas de encoder a las de decoder:\n",
        "\n",
        "1. Embedding Layer\n",
        "2. Positional Encoding\n",
        "3. Pila de capas de Encoder\n",
        "3. Cross Attention\n",
        "3. Pila de capas de Decoder\n",
        "4. La salida que sería un sequence to sequence"
      ],
      "metadata": {
        "id": "XbBuXn88Xo5H"
      },
      "id": "XbBuXn88Xo5H"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "X59gz0VMX41c"
      },
      "id": "X59gz0VMX41c",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiHead attention\n",
        " the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model.\n",
        "`scaled_dot_product_attention`: the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n",
        "\n",
        "`attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
        "\n",
        "`split_heads`: This method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n",
        "\n",
        "`combine_heads`: combines the results back into a single tensor of shape (batch_size, seq_length, d_model)\n",
        "\n",
        "`forward`: The forward method is where the actual computation happens:"
      ],
      "metadata": {
        "id": "11qGlN6Iau4-"
      },
      "id": "11qGlN6Iau4-"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  num_heads: The number of attention heads to split the input into.\n",
        "  d_model is divisible by num_heads\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    # Initialize dimensions\n",
        "    self.d_model = d_model # Model's dimension\n",
        "    self.num_heads = num_heads # Number of attention heads\n",
        "    self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "    # Linear layers for transforming inputs\n",
        "    self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "    self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "    self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "    self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "    # Calculate attention scores\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "    if mask is not None:\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Softmax is applied to obtain attention probabilities\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # Multiply by values to obtain the final output\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    # Reshape the input to have num_heads for multi-head attention\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    # Combine the multiple heads back to original shape\n",
        "    batch_size, _, seq_length, d_k = x.size()\n",
        "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    # Apply linear transformations and split heads\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    # Perform scaled dot-product attention\n",
        "    attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "    # Combine heads and apply output transformation\n",
        "    output = self.W_o(self.combine_heads(attn_output))\n",
        "    return output"
      ],
      "metadata": {
        "id": "g9esaIUXaw6h"
      },
      "id": "g9esaIUXaw6h",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position Wise Feed Forward\n",
        "defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs."
      ],
      "metadata": {
        "id": "vro4H5JZchHl"
      },
      "id": "vro4H5JZchHl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(PositionWiseFeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "q13-TuLLcgFx"
      },
      "id": "q13-TuLLcgFx",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "The PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence.\n",
        "\n",
        "`max_seq_length`: The maximum length of the sequence for which positional encodings are pre-computed.\n",
        "`pe`: A tensor filled with zeros, which will be populated with positional encodings.\n",
        "`position`: A tensor containing the position indices for each position in the sequence.\n",
        "`div_term`: A term used to scale the position indices in a specific way.\n",
        "\n",
        "The sine function is applied to the even indices and the cosine function to the odd indices of pe."
      ],
      "metadata": {
        "id": "lEWej5ILdGkl"
      },
      "id": "lEWej5ILdGkl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "C2YTutDrdJ8H"
      },
      "id": "C2YTutDrdJ8H",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Encoder Layer\n",
        "\n",
        "The EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
      ],
      "metadata": {
        "id": "LjIEZlNUreDl"
      },
      "id": "LjIEZlNUreDl"
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "dV2U9gU5rhuH"
      },
      "id": "dV2U9gU5rhuH",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Transformer"
      ],
      "metadata": {
        "id": "jVdd5xHCsjpo"
      },
      "id": "jVdd5xHCsjpo"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, dropout, num_classes, max_sequence_length):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Linear(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, mask)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0bIxXCrBuXxM"
      },
      "id": "0bIxXCrBuXxM",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Layer\n",
        "\n",
        "The DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model."
      ],
      "metadata": {
        "id": "oiHV4UT3HCae"
      },
      "id": "oiHV4UT3HCae"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "KwO2OUMDHB8Y"
      },
      "id": "KwO2OUMDHB8Y",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Transformer"
      ],
      "metadata": {
        "id": "Ew5tkq-XHERE"
      },
      "id": "Ew5tkq-XHERE"
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "aerp7qYHHN7D"
      },
      "id": "aerp7qYHHN7D",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba con Datos Aleatorios\n",
        "\n",
        "Se declara un dataset que genera datos sintéticos para evaluar el rendimiento del modelo en clasificacción"
      ],
      "metadata": {
        "id": "eHbPYyAluaLu"
      },
      "id": "eHbPYyAluaLu"
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "vocab_size = 1000\n",
        "sequence_length = 64\n",
        "dropout = 0.1\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "GEnAZ1Hux98u"
      },
      "id": "GEnAZ1Hux98u",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = Transformer(vocab_size, vocab_size, d_model, num_heads, num_layers, d_ff, sequence_length, dropout) # Pass sequence_length and dropout while creating the instance\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "F_zn614uySSO"
      },
      "id": "F_zn614uySSO",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n"
      ],
      "metadata": {
        "id": "XTaNU4_G0W0T",
        "outputId": "a985089e-f41a-43ae-e826-d31a3511df8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XTaNU4_G0W0T",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generando un dataset de secuencia de ejemplo"
      ],
      "metadata": {
        "id": "L_YopufjVAjn"
      },
      "id": "L_YopufjVAjn"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a simple sequence dataset\n",
        "def create_dataset(sequence_length, vocab_size, batch_size):\n",
        "    x = np.arange(vocab_size)\n",
        "    y = np.roll(x, -1)  # Shifted sequence\n",
        "    x = np.tile(x, (batch_size, 1))\n",
        "    y = np.tile(y, (batch_size, 1))\n",
        "    return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Dummy dataset\n",
        "src, tgt = create_dataset(sequence_length, vocab_size, batch_size)\n",
        "src = src[:, :sequence_length]\n",
        "tgt = tgt[:, :sequence_length]\n",
        "\n",
        "# Masks for padding (if necessary)\n",
        "src_mask = torch.nn.Transformer.generate_square_subsequent_mask(sequence_length).to(device)  # Call from torch.nn.Transformer\n",
        "tgt_mask = torch.nn.Transformer.generate_square_subsequent_mask(sequence_length).to(device)  # Call from torch.nn.Transformer\n"
      ],
      "metadata": {
        "id": "9IUIdlqfH4dk"
      },
      "id": "9IUIdlqfH4dk",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src.shape, tgt.shape, src_mask.shape, tgt_mask.shape"
      ],
      "metadata": {
        "id": "346IUfKnWxzK",
        "outputId": "3f4682be-d18b-471d-b613-eebecaef86f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "346IUfKnWxzK",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 64]),\n",
              " torch.Size([16, 64]),\n",
              " torch.Size([64, 64]),\n",
              " torch.Size([64, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src"
      ],
      "metadata": {
        "id": "UJjgjtqXW0y4",
        "outputId": "1b1743ad-6db1-499b-a9e4-8297816b93a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UJjgjtqXW0y4",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  ..., 61, 62, 63],\n",
              "        [ 0,  1,  2,  ..., 61, 62, 63],\n",
              "        [ 0,  1,  2,  ..., 61, 62, 63],\n",
              "        ...,\n",
              "        [ 0,  1,  2,  ..., 61, 62, 63],\n",
              "        [ 0,  1,  2,  ..., 61, 62, 63],\n",
              "        [ 0,  1,  2,  ..., 61, 62, 63]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt"
      ],
      "metadata": {
        "id": "RxQhmfZAW4CP",
        "outputId": "6b53f04e-2d0c-42d6-9a2c-a8e4870322bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RxQhmfZAW4CP",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        ...,\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop\n",
        "\n",
        "Haciendo un lazo de entrenamiento"
      ],
      "metadata": {
        "id": "8hgJvBaxVQEV"
      },
      "id": "8hgJvBaxVQEV"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, src, tgt, src_mask, tgt_mask, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "        # Replacing view with reshape to handle non-contiguous tensors\n",
        "        loss = criterion(output.reshape(-1, vocab_size), tgt.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 1 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Train the model\n",
        "train_model(model, src, tgt, src_mask, tgt_mask, criterion, optimizer, epochs=20)"
      ],
      "metadata": {
        "id": "7H5IT4wAVNUs",
        "outputId": "cf1d20e4-2936-4ad1-c79e-829ae8a82d99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7H5IT4wAVNUs",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.4571903944015503\n",
            "Epoch 2/20, Loss: 1.1055076122283936\n",
            "Epoch 3/20, Loss: 0.8289587497711182\n",
            "Epoch 4/20, Loss: 0.6056163311004639\n",
            "Epoch 5/20, Loss: 0.4524783492088318\n",
            "Epoch 6/20, Loss: 0.33423587679862976\n",
            "Epoch 7/20, Loss: 0.2503746449947357\n",
            "Epoch 8/20, Loss: 0.19388259947299957\n",
            "Epoch 9/20, Loss: 0.15040621161460876\n",
            "Epoch 10/20, Loss: 0.12003596127033234\n",
            "Epoch 11/20, Loss: 0.0983632430434227\n",
            "Epoch 12/20, Loss: 0.08239667117595673\n",
            "Epoch 13/20, Loss: 0.07036348432302475\n",
            "Epoch 14/20, Loss: 0.06080608442425728\n",
            "Epoch 15/20, Loss: 0.053940299898386\n",
            "Epoch 16/20, Loss: 0.04725867509841919\n",
            "Epoch 17/20, Loss: 0.04256439208984375\n",
            "Epoch 18/20, Loss: 0.038610804826021194\n",
            "Epoch 19/20, Loss: 0.035773251205682755\n",
            "Epoch 20/20, Loss: 0.03295738995075226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Loop\n",
        "Haciendo un lazo para evaluar el desempeño"
      ],
      "metadata": {
        "id": "wnu_Z5TLVVyN"
      },
      "id": "wnu_Z5TLVVyN"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, src, tgt, src_mask, tgt_mask, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        output = model(src, tgt)\n",
        "        loss = criterion(output.reshape(-1, vocab_size), tgt.reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / src.size(0)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "# Evaluate the model\n",
        "avg_loss, perplexity = evaluate_model(model, src, tgt, src_mask, tgt_mask, criterion)\n",
        "print(f\"Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "meZydM71VbVT",
        "outputId": "f3e43ba7-df2d-407b-cee5-fb2b246ca23d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "meZydM71VbVT",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.0010, Perplexity: 1.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a prediction for the src sequence and print the first 100 results given by the model\n",
        "\n",
        "# Assuming the model and necessary variables (src, tgt, etc.) are already defined as in the provided code.\n",
        "\n",
        "# Generate predictions\n",
        "def predict_sequence(model, src, max_len):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        # Initialize the input sequence with the first token of src sequence.\n",
        "        input_seq = src[:, 0].unsqueeze(1)  # Start with the first token\n",
        "        for _ in range(max_len):\n",
        "          output = model(src, input_seq)\n",
        "          _, predicted_token = torch.max(output[:, -1, :], dim=-1)\n",
        "          predictions.append(predicted_token)\n",
        "          input_seq = torch.cat([input_seq, predicted_token.unsqueeze(1)], dim=1)\n",
        "        return torch.stack(predictions, dim=1)\n",
        "\n",
        "predicted_sequence = predict_sequence(model, src, sequence_length)\n",
        "\n",
        "# Print the first 100 results for the first sequence in the batch.\n",
        "print(predicted_sequence[0, :100])"
      ],
      "metadata": {
        "id": "0OiwBnUgV3fc",
        "outputId": "f6ed56d0-5d0d-4b25-bbed-30739895a76a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0OiwBnUgV3fc",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt[:100]"
      ],
      "metadata": {
        "id": "4Q6hlAHpbd4R",
        "outputId": "240503c9-da88-4b93-f8be-375fad477622",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4Q6hlAHpbd4R",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        ...,\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64],\n",
              "        [ 1,  2,  3,  ..., 62, 63, 64]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "FR0a_HM33pxR",
        "outputId": "fe9735e2-2c48-43ef-ed9c-1d4b61c39a9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FR0a_HM33pxR",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "DGIsp_WKYg0G",
        "outputId": "f3cf520f-d99c-4cc2-9ba6-aed878661adc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DGIsp_WKYg0G",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "Transformer                                   --\n",
              "├─Embedding: 1-1                              512,000\n",
              "├─Embedding: 1-2                              512,000\n",
              "├─PositionalEncoding: 1-3                     --\n",
              "├─ModuleList: 1-4                             --\n",
              "│    └─EncoderLayer: 2-1                      --\n",
              "│    │    └─MultiHeadAttention: 3-1           1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-2      2,099,712\n",
              "│    │    └─LayerNorm: 3-3                    1,024\n",
              "│    │    └─LayerNorm: 3-4                    1,024\n",
              "│    │    └─Dropout: 3-5                      --\n",
              "│    └─EncoderLayer: 2-2                      --\n",
              "│    │    └─MultiHeadAttention: 3-6           1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-7      2,099,712\n",
              "│    │    └─LayerNorm: 3-8                    1,024\n",
              "│    │    └─LayerNorm: 3-9                    1,024\n",
              "│    │    └─Dropout: 3-10                     --\n",
              "│    └─EncoderLayer: 2-3                      --\n",
              "│    │    └─MultiHeadAttention: 3-11          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-12     2,099,712\n",
              "│    │    └─LayerNorm: 3-13                   1,024\n",
              "│    │    └─LayerNorm: 3-14                   1,024\n",
              "│    │    └─Dropout: 3-15                     --\n",
              "│    └─EncoderLayer: 2-4                      --\n",
              "│    │    └─MultiHeadAttention: 3-16          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-17     2,099,712\n",
              "│    │    └─LayerNorm: 3-18                   1,024\n",
              "│    │    └─LayerNorm: 3-19                   1,024\n",
              "│    │    └─Dropout: 3-20                     --\n",
              "│    └─EncoderLayer: 2-5                      --\n",
              "│    │    └─MultiHeadAttention: 3-21          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-22     2,099,712\n",
              "│    │    └─LayerNorm: 3-23                   1,024\n",
              "│    │    └─LayerNorm: 3-24                   1,024\n",
              "│    │    └─Dropout: 3-25                     --\n",
              "│    └─EncoderLayer: 2-6                      --\n",
              "│    │    └─MultiHeadAttention: 3-26          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-27     2,099,712\n",
              "│    │    └─LayerNorm: 3-28                   1,024\n",
              "│    │    └─LayerNorm: 3-29                   1,024\n",
              "│    │    └─Dropout: 3-30                     --\n",
              "├─ModuleList: 1-5                             --\n",
              "│    └─DecoderLayer: 2-7                      --\n",
              "│    │    └─MultiHeadAttention: 3-31          1,050,624\n",
              "│    │    └─MultiHeadAttention: 3-32          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-33     2,099,712\n",
              "│    │    └─LayerNorm: 3-34                   1,024\n",
              "│    │    └─LayerNorm: 3-35                   1,024\n",
              "│    │    └─LayerNorm: 3-36                   1,024\n",
              "│    │    └─Dropout: 3-37                     --\n",
              "│    └─DecoderLayer: 2-8                      --\n",
              "│    │    └─MultiHeadAttention: 3-38          1,050,624\n",
              "│    │    └─MultiHeadAttention: 3-39          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-40     2,099,712\n",
              "│    │    └─LayerNorm: 3-41                   1,024\n",
              "│    │    └─LayerNorm: 3-42                   1,024\n",
              "│    │    └─LayerNorm: 3-43                   1,024\n",
              "│    │    └─Dropout: 3-44                     --\n",
              "│    └─DecoderLayer: 2-9                      --\n",
              "│    │    └─MultiHeadAttention: 3-45          1,050,624\n",
              "│    │    └─MultiHeadAttention: 3-46          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-47     2,099,712\n",
              "│    │    └─LayerNorm: 3-48                   1,024\n",
              "│    │    └─LayerNorm: 3-49                   1,024\n",
              "│    │    └─LayerNorm: 3-50                   1,024\n",
              "│    │    └─Dropout: 3-51                     --\n",
              "│    └─DecoderLayer: 2-10                     --\n",
              "│    │    └─MultiHeadAttention: 3-52          1,050,624\n",
              "│    │    └─MultiHeadAttention: 3-53          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-54     2,099,712\n",
              "│    │    └─LayerNorm: 3-55                   1,024\n",
              "│    │    └─LayerNorm: 3-56                   1,024\n",
              "│    │    └─LayerNorm: 3-57                   1,024\n",
              "│    │    └─Dropout: 3-58                     --\n",
              "│    └─DecoderLayer: 2-11                     --\n",
              "│    │    └─MultiHeadAttention: 3-59          1,050,624\n",
              "│    │    └─MultiHeadAttention: 3-60          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-61     2,099,712\n",
              "│    │    └─LayerNorm: 3-62                   1,024\n",
              "│    │    └─LayerNorm: 3-63                   1,024\n",
              "│    │    └─LayerNorm: 3-64                   1,024\n",
              "│    │    └─Dropout: 3-65                     --\n",
              "│    └─DecoderLayer: 2-12                     --\n",
              "│    │    └─MultiHeadAttention: 3-66          1,050,624\n",
              "│    │    └─MultiHeadAttention: 3-67          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-68     2,099,712\n",
              "│    │    └─LayerNorm: 3-69                   1,024\n",
              "│    │    └─LayerNorm: 3-70                   1,024\n",
              "│    │    └─LayerNorm: 3-71                   1,024\n",
              "│    │    └─Dropout: 3-72                     --\n",
              "├─Linear: 1-6                                 513,000\n",
              "├─Dropout: 1-7                                --\n",
              "======================================================================\n",
              "Total params: 45,675,496\n",
              "Trainable params: 45,675,496\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9972e5f1",
      "metadata": {
        "papermill": {
          "duration": 0.001771,
          "end_time": "2024-12-04T20:02:41.190006",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.188235",
          "status": "completed"
        },
        "tags": [],
        "id": "9972e5f1"
      },
      "source": [
        "## Referencias\n",
        "- https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
        "- https://campus.datacamp.com/es/courses/introduction-to-llms-in-python/building-a-transformer-architecture?ex=15\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 119408755,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6.896552,
      "end_time": "2024-12-04T20:02:45.742633",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-12-04T20:02:38.846081",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}