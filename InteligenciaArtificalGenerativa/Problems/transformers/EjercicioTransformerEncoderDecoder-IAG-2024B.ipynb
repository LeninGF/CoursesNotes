{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeninGF/CoursesNotes/blob/main/InteligenciaArtificalGenerativa/Problems/transformers/EjercicioTransformersDecoder-IAG-2024B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f32028e",
      "metadata": {
        "id": "5f32028e",
        "papermill": {
          "duration": 0.001948,
          "end_time": "2024-12-04T20:02:41.186222",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.184274",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Transformers Decoder\n",
        "\n",
        "\n",
        "\n",
        "Coder: Lenin G. Falconí\n",
        "\n",
        "\n",
        "\n",
        "Asignatura: Tópicos Especiales (Inteligencia Artificial)\n",
        "\n",
        "\n",
        "\n",
        "Fecha: 2024-12-11"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar un transformer Encoder se requiere de:\n",
        "\n",
        "1. Embedding Layer\n",
        "2. Positional Encoding\n",
        "3. Pila de capas de Decoder\n",
        "4. La salida que sería un classification head"
      ],
      "metadata": {
        "id": "XbBuXn88Xo5H"
      },
      "id": "XbBuXn88Xo5H"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "X59gz0VMX41c"
      },
      "id": "X59gz0VMX41c",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiHead attention\n",
        " the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model.\n",
        "`scaled_dot_product_attention`: the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n",
        "\n",
        "`attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
        "\n",
        "`split_heads`: This method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n",
        "\n",
        "`combine_heads`: combines the results back into a single tensor of shape (batch_size, seq_length, d_model)\n",
        "\n",
        "`forward`: The forward method is where the actual computation happens:"
      ],
      "metadata": {
        "id": "11qGlN6Iau4-"
      },
      "id": "11qGlN6Iau4-"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  num_heads: The number of attention heads to split the input into.\n",
        "  d_model is divisible by num_heads\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    # Initialize dimensions\n",
        "    self.d_model = d_model # Model's dimension\n",
        "    self.num_heads = num_heads # Number of attention heads\n",
        "    self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "    # Linear layers for transforming inputs\n",
        "    self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "    self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "    self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "    self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "    # Calculate attention scores\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "    if mask is not None:\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Softmax is applied to obtain attention probabilities\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # Multiply by values to obtain the final output\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    # Reshape the input to have num_heads for multi-head attention\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    # Combine the multiple heads back to original shape\n",
        "    batch_size, _, seq_length, d_k = x.size()\n",
        "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    # Apply linear transformations and split heads\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    # Perform scaled dot-product attention\n",
        "    attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "    # Combine heads and apply output transformation\n",
        "    output = self.W_o(self.combine_heads(attn_output))\n",
        "    return output"
      ],
      "metadata": {
        "id": "g9esaIUXaw6h"
      },
      "id": "g9esaIUXaw6h",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position Wise Feed Forward\n",
        "defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs."
      ],
      "metadata": {
        "id": "vro4H5JZchHl"
      },
      "id": "vro4H5JZchHl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(PositionWiseFeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "q13-TuLLcgFx"
      },
      "id": "q13-TuLLcgFx",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "The PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence.\n",
        "\n",
        "`max_seq_length`: The maximum length of the sequence for which positional encodings are pre-computed.\n",
        "`pe`: A tensor filled with zeros, which will be populated with positional encodings.\n",
        "`position`: A tensor containing the position indices for each position in the sequence.\n",
        "`div_term`: A term used to scale the position indices in a specific way.\n",
        "\n",
        "The sine function is applied to the even indices and the cosine function to the odd indices of pe."
      ],
      "metadata": {
        "id": "lEWej5ILdGkl"
      },
      "id": "lEWej5ILdGkl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "C2YTutDrdJ8H"
      },
      "id": "C2YTutDrdJ8H",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Encoder Layer\n",
        "\n",
        "The EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
      ],
      "metadata": {
        "id": "LjIEZlNUreDl"
      },
      "id": "LjIEZlNUreDl"
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "dV2U9gU5rhuH"
      },
      "id": "dV2U9gU5rhuH",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Transformer"
      ],
      "metadata": {
        "id": "jVdd5xHCsjpo"
      },
      "id": "jVdd5xHCsjpo"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, dropout, num_classes, max_sequence_length):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Linear(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, mask)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0bIxXCrBuXxM"
      },
      "id": "0bIxXCrBuXxM",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Layer\n",
        "\n",
        "The DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model."
      ],
      "metadata": {
        "id": "oiHV4UT3HCae"
      },
      "id": "oiHV4UT3HCae"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, tgt_mask):\n",
        "        x2 = self.norm1(x)\n",
        "        x2, _ = self.self_attn(x2, x2, x2, attn_mask=tgt_mask)\n",
        "        x = x + self.dropout1(x2)\n",
        "        x2 = self.norm2(x)\n",
        "        x2 = self.linear2(self.dropout(torch.relu(self.linear1(x2))))\n",
        "        x = x + self.dropout2(x2)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "KwO2OUMDHB8Y"
      },
      "id": "KwO2OUMDHB8Y",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Transformer"
      ],
      "metadata": {
        "id": "Ew5tkq-XHERE"
      },
      "id": "Ew5tkq-XHERE"
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, num_layers, d_ff, vocab_size, sequence_length, dropout, num_classes):\n",
        "        super(DecoderOnlyTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, sequence_length, d_model))\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model * sequence_length, num_classes)\n",
        "\n",
        "    def forward(self, x, tgt_mask=None):\n",
        "        x = self.embedding(x) + self.pos_encoding\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, tgt_mask)\n",
        "        x = self.norm(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "aerp7qYHHN7D"
      },
      "id": "aerp7qYHHN7D",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba con Datos Aleatorios\n",
        "\n",
        "Se declara un dataset que genera datos sintéticos para evaluar el rendimiento del modelo en clasificacción"
      ],
      "metadata": {
        "id": "eHbPYyAluaLu"
      },
      "id": "eHbPYyAluaLu"
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "vocab_size = 1000\n",
        "sequence_length = 256\n",
        "dropout = 0.1\n",
        "num_classes = 3"
      ],
      "metadata": {
        "id": "GEnAZ1Hux98u"
      },
      "id": "GEnAZ1Hux98u",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = DecoderOnlyTransformer(d_model, num_heads, num_layers, d_ff, vocab_size, sequence_length, dropout, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "F_zn614uySSO"
      },
      "id": "F_zn614uySSO",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n"
      ],
      "metadata": {
        "id": "XTaNU4_G0W0T",
        "outputId": "61d6b40c-8263-4d28-a71c-22fd49650530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XTaNU4_G0W0T",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "num_samples = 1000\n",
        "\n",
        "# Generate random integers between 0 and vocab_size-1 for X\n",
        "X = np.random.randint(0, vocab_size, size=(num_samples, sequence_length)).astype(np.int64)\n",
        "y = np.random.randint(0, num_classes, num_samples).astype(np.int64)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X)\n",
        "y_tensor = torch.tensor(y)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "9IUIdlqfH4dk"
      },
      "id": "9IUIdlqfH4dk",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test set\n",
        "test_size = int(0.2 * len(dataset))  # 20% of the data for testing\n",
        "train_size = len(dataset) - test_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ],
      "metadata": {
        "id": "H575aUoCLWlV"
      },
      "id": "H575aUoCLWlV",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "6nuyTIAdMNCA"
      },
      "id": "6nuyTIAdMNCA",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "# X_tensor = X_tensor.to(device)\n",
        "# y_tensor = y_tensor.to(device)\n"
      ],
      "metadata": {
        "id": "hBrcG2aw0cNH"
      },
      "id": "hBrcG2aw0cNH",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch_X, batch_y in train_dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # calculando la accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += batch_y.size(0)\n",
        "        correct_predictions += (predicted == batch_y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy:{accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "5lDK0LpWup7a",
        "outputId": "9b27e077-d709-41ef-d8ac-f1854c0cd17d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5lDK0LpWup7a",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.1114, Accuracy:0.3387\n",
            "Epoch 2/10, Loss: 0.7380, Accuracy:0.4662\n",
            "Epoch 3/10, Loss: 0.3876, Accuracy:0.6400\n",
            "Epoch 4/10, Loss: 0.4433, Accuracy:0.6550\n",
            "Epoch 5/10, Loss: 0.5081, Accuracy:0.6937\n",
            "Epoch 6/10, Loss: 0.3445, Accuracy:0.7612\n",
            "Epoch 7/10, Loss: 0.1835, Accuracy:0.8512\n",
            "Epoch 8/10, Loss: 0.0983, Accuracy:0.9100\n",
            "Epoch 9/10, Loss: 0.0624, Accuracy:0.9450\n",
            "Epoch 10/10, Loss: 0.0493, Accuracy:0.9537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch_X, batch_y in test_dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += batch_y.size(0)\n",
        "        correct_predictions += (predicted == batch_y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "AAPPCC4busw9",
        "outputId": "4fddee6e-4b21-4c3e-cf46-bf2b718a7645",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AAPPCC4busw9",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 2.5734, Test Accuracy: 0.3900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: using the test dataloader write a code to get the predictions by the model\n",
        "\n",
        "import torch\n",
        "\n",
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch_X, _ in test_dataloader:  # We don't need the true labels for prediction\n",
        "        batch_X = batch_X.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy()) # Move predictions to CPU and convert to numpy\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "kRNOeocpM8QH",
        "outputId": "9da91fcd-a35c-4d32-9c25-a8f0b9b5b246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kRNOeocpM8QH",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "FR0a_HM33pxR",
        "outputId": "6609df1b-cb64-4718-88ff-bf51b2b09710",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FR0a_HM33pxR",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "DGIsp_WKYg0G",
        "outputId": "0f7ea1c3-e07c-4ac2-b62c-e2016d2c0ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DGIsp_WKYg0G",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================\n",
              "Layer (type:depth-idx)                                       Param #\n",
              "=====================================================================================\n",
              "DecoderOnlyTransformer                                       131,072\n",
              "├─Embedding: 1-1                                             512,000\n",
              "├─ModuleList: 1-2                                            --\n",
              "│    └─DecoderLayer: 2-1                                     --\n",
              "│    │    └─MultiheadAttention: 3-1                          1,050,624\n",
              "│    │    └─Linear: 3-2                                      1,050,624\n",
              "│    │    └─Dropout: 3-3                                     --\n",
              "│    │    └─Linear: 3-4                                      1,049,088\n",
              "│    │    └─LayerNorm: 3-5                                   1,024\n",
              "│    │    └─LayerNorm: 3-6                                   1,024\n",
              "│    │    └─Dropout: 3-7                                     --\n",
              "│    │    └─Dropout: 3-8                                     --\n",
              "│    └─DecoderLayer: 2-2                                     --\n",
              "│    │    └─MultiheadAttention: 3-9                          1,050,624\n",
              "│    │    └─Linear: 3-10                                     1,050,624\n",
              "│    │    └─Dropout: 3-11                                    --\n",
              "│    │    └─Linear: 3-12                                     1,049,088\n",
              "│    │    └─LayerNorm: 3-13                                  1,024\n",
              "│    │    └─LayerNorm: 3-14                                  1,024\n",
              "│    │    └─Dropout: 3-15                                    --\n",
              "│    │    └─Dropout: 3-16                                    --\n",
              "│    └─DecoderLayer: 2-3                                     --\n",
              "│    │    └─MultiheadAttention: 3-17                         1,050,624\n",
              "│    │    └─Linear: 3-18                                     1,050,624\n",
              "│    │    └─Dropout: 3-19                                    --\n",
              "│    │    └─Linear: 3-20                                     1,049,088\n",
              "│    │    └─LayerNorm: 3-21                                  1,024\n",
              "│    │    └─LayerNorm: 3-22                                  1,024\n",
              "│    │    └─Dropout: 3-23                                    --\n",
              "│    │    └─Dropout: 3-24                                    --\n",
              "│    └─DecoderLayer: 2-4                                     --\n",
              "│    │    └─MultiheadAttention: 3-25                         1,050,624\n",
              "│    │    └─Linear: 3-26                                     1,050,624\n",
              "│    │    └─Dropout: 3-27                                    --\n",
              "│    │    └─Linear: 3-28                                     1,049,088\n",
              "│    │    └─LayerNorm: 3-29                                  1,024\n",
              "│    │    └─LayerNorm: 3-30                                  1,024\n",
              "│    │    └─Dropout: 3-31                                    --\n",
              "│    │    └─Dropout: 3-32                                    --\n",
              "│    └─DecoderLayer: 2-5                                     --\n",
              "│    │    └─MultiheadAttention: 3-33                         1,050,624\n",
              "│    │    └─Linear: 3-34                                     1,050,624\n",
              "│    │    └─Dropout: 3-35                                    --\n",
              "│    │    └─Linear: 3-36                                     1,049,088\n",
              "│    │    └─LayerNorm: 3-37                                  1,024\n",
              "│    │    └─LayerNorm: 3-38                                  1,024\n",
              "│    │    └─Dropout: 3-39                                    --\n",
              "│    │    └─Dropout: 3-40                                    --\n",
              "│    └─DecoderLayer: 2-6                                     --\n",
              "│    │    └─MultiheadAttention: 3-41                         1,050,624\n",
              "│    │    └─Linear: 3-42                                     1,050,624\n",
              "│    │    └─Dropout: 3-43                                    --\n",
              "│    │    └─Linear: 3-44                                     1,049,088\n",
              "│    │    └─LayerNorm: 3-45                                  1,024\n",
              "│    │    └─LayerNorm: 3-46                                  1,024\n",
              "│    │    └─Dropout: 3-47                                    --\n",
              "│    │    └─Dropout: 3-48                                    --\n",
              "├─LayerNorm: 1-3                                             1,024\n",
              "├─Linear: 1-4                                                393,219\n",
              "=====================================================================================\n",
              "Total params: 19,951,619\n",
              "Trainable params: 19,951,619\n",
              "Non-trainable params: 0\n",
              "====================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9972e5f1",
      "metadata": {
        "papermill": {
          "duration": 0.001771,
          "end_time": "2024-12-04T20:02:41.190006",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.188235",
          "status": "completed"
        },
        "tags": [],
        "id": "9972e5f1"
      },
      "source": [
        "## Referencias\n",
        "- https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
        "- https://campus.datacamp.com/es/courses/introduction-to-llms-in-python/building-a-transformer-architecture?ex=15\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 119408755,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6.896552,
      "end_time": "2024-12-04T20:02:45.742633",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-12-04T20:02:38.846081",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}