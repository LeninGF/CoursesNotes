{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeninGF/CoursesNotes/blob/main/InteligenciaArtificalGenerativa/Problems/transformers/EjercicioTransformersEncoder-IAG-2024B_LeninFalconi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f32028e",
      "metadata": {
        "id": "5f32028e",
        "papermill": {
          "duration": 0.001948,
          "end_time": "2024-12-04T20:02:41.186222",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.184274",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Transformers Encoder\n",
        "\n",
        "\n",
        "\n",
        "Coder: Lenin G. Falconí\n",
        "\n",
        "\n",
        "\n",
        "Asignatura: Tópicos Especiales (Inteligencia Artificial)\n",
        "\n",
        "\n",
        "\n",
        "Fecha: 2024-12-02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder\n",
        "\n",
        "Para realizar un transformer Encoder se requiere de:\n",
        "\n",
        "1. Embedding Layer\n",
        "2. Positional Encoding\n",
        "3. Pila de capas de Encoder\n",
        "4. La salida que sería un classification head"
      ],
      "metadata": {
        "id": "XbBuXn88Xo5H"
      },
      "id": "XbBuXn88Xo5H"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "X59gz0VMX41c"
      },
      "id": "X59gz0VMX41c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiHead attention\n",
        " the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model.\n",
        "`scaled_dot_product_attention`: the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n",
        "\n",
        "`attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
        "\n",
        "`split_heads`: This method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n",
        "\n",
        "`combine_heads`: combines the results back into a single tensor of shape (batch_size, seq_length, d_model)\n",
        "\n",
        "`forward`: The forward method is where the actual computation happens:"
      ],
      "metadata": {
        "id": "11qGlN6Iau4-"
      },
      "id": "11qGlN6Iau4-"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  num_heads: The number of attention heads to split the input into.\n",
        "  d_model is divisible by num_heads\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    # Initialize dimensions\n",
        "    self.d_model = d_model # Model's dimension\n",
        "    self.num_heads = num_heads # Number of attention heads\n",
        "    self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "    # Linear layers for transforming inputs\n",
        "    self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "    self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "    self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "    self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "    # Calculate attention scores\n",
        "    attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "    if mask is not None:\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Softmax is applied to obtain attention probabilities\n",
        "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # Multiply by values to obtain the final output\n",
        "    output = torch.matmul(attn_probs, V)\n",
        "    return output\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    # Reshape the input to have num_heads for multi-head attention\n",
        "    batch_size, seq_length, d_model = x.size()\n",
        "    return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    # Combine the multiple heads back to original shape\n",
        "    batch_size, _, seq_length, d_k = x.size()\n",
        "    return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    # Apply linear transformations and split heads\n",
        "    Q = self.split_heads(self.W_q(Q))\n",
        "    K = self.split_heads(self.W_k(K))\n",
        "    V = self.split_heads(self.W_v(V))\n",
        "\n",
        "    # Perform scaled dot-product attention\n",
        "    attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "    # Combine heads and apply output transformation\n",
        "    output = self.W_o(self.combine_heads(attn_output))\n",
        "    return output"
      ],
      "metadata": {
        "id": "g9esaIUXaw6h"
      },
      "id": "g9esaIUXaw6h",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position Wise Feed Forward\n",
        "defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs."
      ],
      "metadata": {
        "id": "vro4H5JZchHl"
      },
      "id": "vro4H5JZchHl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(PositionWiseFeedForward, self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, d_ff)\n",
        "    self.fc2 = nn.Linear(d_ff, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "q13-TuLLcgFx"
      },
      "id": "q13-TuLLcgFx",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "The PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence.\n",
        "\n",
        "`max_seq_length`: The maximum length of the sequence for which positional encodings are pre-computed.\n",
        "`pe`: A tensor filled with zeros, which will be populated with positional encodings.\n",
        "`position`: A tensor containing the position indices for each position in the sequence.\n",
        "`div_term`: A term used to scale the position indices in a specific way.\n",
        "\n",
        "The sine function is applied to the even indices and the cosine function to the odd indices of pe."
      ],
      "metadata": {
        "id": "lEWej5ILdGkl"
      },
      "id": "lEWej5ILdGkl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "C2YTutDrdJ8H"
      },
      "id": "C2YTutDrdJ8H",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Encoder Layer\n",
        "\n",
        "The EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
      ],
      "metadata": {
        "id": "LjIEZlNUreDl"
      },
      "id": "LjIEZlNUreDl"
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "dV2U9gU5rhuH"
      },
      "id": "dV2U9gU5rhuH",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Transformer"
      ],
      "metadata": {
        "id": "jVdd5xHCsjpo"
      },
      "id": "jVdd5xHCsjpo"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, dropout, num_classes, max_sequence_length):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Linear(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, mask)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0bIxXCrBuXxM"
      },
      "id": "0bIxXCrBuXxM",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prueba con Datos Aleatorios\n",
        "\n",
        "Se declara un dataset que genera datos sintéticos para evaluar el rendimiento del modelo en clasificacción"
      ],
      "metadata": {
        "id": "eHbPYyAluaLu"
      },
      "id": "eHbPYyAluaLu"
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "vocab_size = 1000\n",
        "sequence_length = 256\n",
        "dropout = 0.1\n",
        "num_classes = 3"
      ],
      "metadata": {
        "id": "GEnAZ1Hux98u"
      },
      "id": "GEnAZ1Hux98u",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "num_samples = 1000\n",
        "\n",
        "X = np.random.rand(num_samples, sequence_length, vocab_size).astype(np.float32)\n",
        "y = np.random.randint(0, num_classes, num_samples).astype(np.int64)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X)\n",
        "y_tensor = torch.tensor(y)\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "F_zn614uySSO"
      },
      "id": "F_zn614uySSO",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerEncoder(vocab_size,\n",
        "                           d_model,\n",
        "                           num_heads,\n",
        "                           num_layers,\n",
        "                           d_ff,\n",
        "                           dropout,\n",
        "                           num_classes,\n",
        "                           sequence_length)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "qP6hzs-5uj4z"
      },
      "id": "qP6hzs-5uj4z",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n"
      ],
      "metadata": {
        "id": "XTaNU4_G0W0T",
        "outputId": "60a6613e-98a7-4082-9e0c-7c575fbeafdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XTaNU4_G0W0T",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "X_tensor = X_tensor.to(device)\n",
        "y_tensor = y_tensor.to(device)\n"
      ],
      "metadata": {
        "id": "hBrcG2aw0cNH"
      },
      "id": "hBrcG2aw0cNH",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # calculando la accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += batch_y.size(0)\n",
        "        correct_predictions += (predicted == batch_y).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy:{accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "5lDK0LpWup7a",
        "outputId": "f8629ddd-0219-4cb8-c875-6a099383488a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5lDK0LpWup7a",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.5117, Accuracy:0.3250\n",
            "Epoch 2/10, Loss: 1.1159, Accuracy:0.3450\n",
            "Epoch 3/10, Loss: 1.1351, Accuracy:0.3190\n",
            "Epoch 4/10, Loss: 1.1200, Accuracy:0.3380\n",
            "Epoch 5/10, Loss: 1.1208, Accuracy:0.3270\n",
            "Epoch 6/10, Loss: 1.1245, Accuracy:0.3330\n",
            "Epoch 7/10, Loss: 1.1160, Accuracy:0.3290\n",
            "Epoch 8/10, Loss: 1.1233, Accuracy:0.3340\n",
            "Epoch 9/10, Loss: 1.1240, Accuracy:0.3460\n",
            "Epoch 10/10, Loss: 1.1127, Accuracy:0.3230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_tensor)\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    accuracy = (predictions == y_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "AAPPCC4busw9",
        "outputId": "60e14bdc-824b-403f-b987-76bdf7bfa2b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AAPPCC4busw9",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Assuming X_tensor is your dataset in tensor form\n",
        "with torch.no_grad():\n",
        "    # Move the data to the GPU if available\n",
        "    X_tensor = X_tensor.to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    outputs = model(X_tensor)\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "    # If you want to move predictions back to CPU and convert to numpy array\n",
        "    predictions = predictions.cpu().numpy()\n",
        "\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "zPqwjt__1RTK",
        "outputId": "25c55d9a-dbf6-4142-9d64-b106d65fa906",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zPqwjt__1RTK",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "FR0a_HM33pxR",
        "outputId": "eef29347-39df-41cb-c217-2fb9b4122e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FR0a_HM33pxR",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "DGIsp_WKYg0G",
        "outputId": "21ab1327-a7dd-4bf5-f4ce-c96dbcf2d3f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DGIsp_WKYg0G",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "TransformerEncoder                            --\n",
              "├─Linear: 1-1                                 512,512\n",
              "├─PositionalEncoding: 1-2                     --\n",
              "├─ModuleList: 1-3                             --\n",
              "│    └─EncoderLayer: 2-1                      --\n",
              "│    │    └─MultiHeadAttention: 3-1           1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-2      2,099,712\n",
              "│    │    └─LayerNorm: 3-3                    1,024\n",
              "│    │    └─LayerNorm: 3-4                    1,024\n",
              "│    │    └─Dropout: 3-5                      --\n",
              "│    └─EncoderLayer: 2-2                      --\n",
              "│    │    └─MultiHeadAttention: 3-6           1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-7      2,099,712\n",
              "│    │    └─LayerNorm: 3-8                    1,024\n",
              "│    │    └─LayerNorm: 3-9                    1,024\n",
              "│    │    └─Dropout: 3-10                     --\n",
              "│    └─EncoderLayer: 2-3                      --\n",
              "│    │    └─MultiHeadAttention: 3-11          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-12     2,099,712\n",
              "│    │    └─LayerNorm: 3-13                   1,024\n",
              "│    │    └─LayerNorm: 3-14                   1,024\n",
              "│    │    └─Dropout: 3-15                     --\n",
              "│    └─EncoderLayer: 2-4                      --\n",
              "│    │    └─MultiHeadAttention: 3-16          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-17     2,099,712\n",
              "│    │    └─LayerNorm: 3-18                   1,024\n",
              "│    │    └─LayerNorm: 3-19                   1,024\n",
              "│    │    └─Dropout: 3-20                     --\n",
              "│    └─EncoderLayer: 2-5                      --\n",
              "│    │    └─MultiHeadAttention: 3-21          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-22     2,099,712\n",
              "│    │    └─LayerNorm: 3-23                   1,024\n",
              "│    │    └─LayerNorm: 3-24                   1,024\n",
              "│    │    └─Dropout: 3-25                     --\n",
              "│    └─EncoderLayer: 2-6                      --\n",
              "│    │    └─MultiHeadAttention: 3-26          1,050,624\n",
              "│    │    └─PositionWiseFeedForward: 3-27     2,099,712\n",
              "│    │    └─LayerNorm: 3-28                   1,024\n",
              "│    │    └─LayerNorm: 3-29                   1,024\n",
              "│    │    └─Dropout: 3-30                     --\n",
              "├─Linear: 1-4                                 1,539\n",
              "======================================================================\n",
              "Total params: 19,428,355\n",
              "Trainable params: 19,428,355\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9972e5f1",
      "metadata": {
        "papermill": {
          "duration": 0.001771,
          "end_time": "2024-12-04T20:02:41.190006",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.188235",
          "status": "completed"
        },
        "tags": [],
        "id": "9972e5f1"
      },
      "source": [
        "## Referencias\n",
        "- https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
        "- https://campus.datacamp.com/es/courses/introduction-to-llms-in-python/building-a-transformer-architecture?ex=15\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 119408755,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6.896552,
      "end_time": "2024-12-04T20:02:45.742633",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-12-04T20:02:38.846081",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}