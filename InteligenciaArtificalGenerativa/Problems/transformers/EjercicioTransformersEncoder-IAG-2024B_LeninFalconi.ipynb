{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeninGF/CoursesNotes/blob/main/InteligenciaArtificalGenerativa/Problems/transformers/EjercicioTransformersEncoder-IAG-2024B_LeninFalconi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f32028e",
      "metadata": {
        "id": "5f32028e",
        "papermill": {
          "duration": 0.001948,
          "end_time": "2024-12-04T20:02:41.186222",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.184274",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Transformers Encoder\n",
        "\n",
        "\n",
        "\n",
        "Coder: Lenin G. Falconí\n",
        "\n",
        "\n",
        "\n",
        "Asignatura: Tópicos Especiales (Inteligencia Artificial)\n",
        "\n",
        "\n",
        "\n",
        "Fecha: 2024-12-02"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder\n",
        "\n",
        "Para realizar un transformer Encoder se requiere de:\n",
        "\n",
        "1. Embedding Layer\n",
        "2. Positional Encoding\n",
        "3. Pila de capas de Encoder\n",
        "4. La salida que sería un classification head"
      ],
      "metadata": {
        "id": "XbBuXn88Xo5H"
      },
      "id": "XbBuXn88Xo5H"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "X59gz0VMX41c"
      },
      "id": "X59gz0VMX41c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MultiHead attention\n",
        " the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model.\n",
        "`scaled_dot_product_attention`: the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n",
        "\n",
        "`attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)`\n",
        "\n",
        "`split_heads`: This method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n",
        "\n",
        "`combine_heads`: combines the results back into a single tensor of shape (batch_size, seq_length, d_model)\n",
        "\n",
        "`forward`: The forward method is where the actual computation happens:"
      ],
      "metadata": {
        "id": "11qGlN6Iau4-"
      },
      "id": "11qGlN6Iau4-"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  num_heads: The number of attention heads to split the input into.\n",
        "  d_model is divisible by num_heads\n",
        "\n",
        "  \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "g9esaIUXaw6h"
      },
      "id": "g9esaIUXaw6h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position Wise Feed Forward\n",
        "defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs."
      ],
      "metadata": {
        "id": "vro4H5JZchHl"
      },
      "id": "vro4H5JZchHl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "  \"\"\"\n",
        "  d_model: Dimensionality of the input.\n",
        "  d_ff: Dimensionality of the inner layer in the feed-forward network.\n",
        "  \"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "q13-TuLLcgFx"
      },
      "id": "q13-TuLLcgFx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "The PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence.\n",
        "\n",
        "`max_seq_length`: The maximum length of the sequence for which positional encodings are pre-computed.\n",
        "`pe`: A tensor filled with zeros, which will be populated with positional encodings.\n",
        "`position`: A tensor containing the position indices for each position in the sequence.\n",
        "`div_term`: A term used to scale the position indices in a specific way.\n",
        "\n",
        "The sine function is applied to the even indices and the cosine function to the odd indices of pe."
      ],
      "metadata": {
        "id": "lEWej5ILdGkl"
      },
      "id": "lEWej5ILdGkl"
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "C2YTutDrdJ8H"
      },
      "id": "C2YTutDrdJ8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Encoder Layer\n",
        "\n",
        "The EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
      ],
      "metadata": {
        "id": "LjIEZlNUreDl"
      },
      "id": "LjIEZlNUreDl"
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "dV2U9gU5rhuH"
      },
      "id": "dV2U9gU5rhuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Transformer"
      ],
      "metadata": {
        "id": "jVdd5xHCsjpo"
      },
      "id": "jVdd5xHCsjpo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer con Pytorch desde torch.nn"
      ],
      "metadata": {
        "id": "LEL_8Ff6WuDs"
      },
      "id": "LEL_8Ff6WuDs"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_encoder_layers = 1\n",
        "num_decoder_layers = 6\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "model = nn.Transformer(d_model=d_model,\n",
        "                       nhead=nhead,\n",
        "                       num_encoder_layers=num_encoder_layers,\n",
        "                       num_decoder_layers=num_decoder_layers)\n"
      ],
      "metadata": {
        "id": "a5BWBzxUWy-9",
        "outputId": "f03b4b9e-c712-463b-af7f-cc756294b308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "a5BWBzxUWy-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.5.1+cu121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "1pOYWl88XTfo",
        "outputId": "80c6d8b8-c523-43b9-c0fb-c3b317c5b812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1pOYWl88XTfo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "n3jvrwwNYlzg",
        "outputId": "0f0d4731-1331-4bf3-979a-14a2bdcd6162",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "n3jvrwwNYlzg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "DGIsp_WKYg0G",
        "outputId": "cdfc54d4-4af8-4217-db85-87ea4973a01f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DGIsp_WKYg0G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                                            Param #\n",
              "==========================================================================================\n",
              "Transformer                                                       --\n",
              "├─TransformerEncoder: 1-1                                         --\n",
              "│    └─ModuleList: 2-1                                            --\n",
              "│    │    └─TransformerEncoderLayer: 3-1                          3,152,384\n",
              "│    └─LayerNorm: 2-2                                             1,024\n",
              "├─TransformerDecoder: 1-2                                         --\n",
              "│    └─ModuleList: 2-3                                            --\n",
              "│    │    └─TransformerDecoderLayer: 3-2                          4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-3                          4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-4                          4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-5                          4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-6                          4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-7                          4,204,032\n",
              "│    └─LayerNorm: 2-4                                             1,024\n",
              "==========================================================================================\n",
              "Total params: 28,378,624\n",
              "Trainable params: 28,378,624\n",
              "Non-trainable params: 0\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)  # Move model to the device"
      ],
      "metadata": {
        "id": "htHwLYynbCNX"
      },
      "id": "htHwLYynbCNX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume you have a sequence of numerical IDs:\n",
        "input_sequence = torch.tensor([1, 5, 2, 8, 3])\n",
        "\n",
        "# Create a simple (random) embedding layer:\n",
        "embedding_layer = nn.Embedding(num_embeddings=10, embedding_dim=d_model) # 10 is the vocab size\n",
        "embedded_input = embedding_layer(input_sequence)\n",
        "\n",
        "# Reshape for Transformer input\n",
        "embedded_input = embedded_input.unsqueeze(1) # Add batch dimension\n",
        "\n",
        "#  Create a target sequence (can be the same as input for autoregressive tasks)\n",
        "target_sequence = input_sequence\n",
        "embedded_target = embedding_layer(target_sequence).unsqueeze(1) # Embed and add batch dimension\n",
        "\n",
        "# Pass embedded input and target to the model\n",
        "output = model(embedded_input, embedded_target) # Provide both src and tgt\n",
        "output"
      ],
      "metadata": {
        "id": "ABg_QIIOY0wj",
        "outputId": "abe30f80-c551-4b3b-eee9-09a2cbe1bb55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ABg_QIIOY0wj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2294, -0.6206, -0.6648,  ...,  0.6508,  0.2325, -0.6288]],\n",
              "\n",
              "        [[-0.6362, -0.9210,  0.2839,  ..., -0.0827,  0.6265,  0.8557]],\n",
              "\n",
              "        [[-0.0152,  0.1492,  0.4017,  ...,  0.6161,  1.6122,  0.1904]],\n",
              "\n",
              "        [[ 0.4158,  0.0035,  0.1774,  ...,  0.0126, -0.2418, -0.1176]],\n",
              "\n",
              "        [[ 0.3905, -1.0222, -0.5462,  ...,  0.6306,  0.3137,  0.2474]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_data=(embedded_input, embedded_target))"
      ],
      "metadata": {
        "id": "QaZtltvpaMvc",
        "outputId": "8e6227bb-ec84-40d9-90ac-b53484917f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QaZtltvpaMvc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "Transformer                                   [5, 1, 512]               --\n",
              "├─TransformerEncoder: 1-1                     [5, 1, 512]               --\n",
              "│    └─ModuleList: 2-1                        --                        --\n",
              "│    │    └─TransformerEncoderLayer: 3-1      [5, 1, 512]               3,152,384\n",
              "│    └─LayerNorm: 2-2                         [5, 1, 512]               1,024\n",
              "├─TransformerDecoder: 1-2                     [5, 1, 512]               --\n",
              "│    └─ModuleList: 2-3                        --                        --\n",
              "│    │    └─TransformerDecoderLayer: 3-2      [5, 1, 512]               4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-3      [5, 1, 512]               4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-4      [5, 1, 512]               4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-5      [5, 1, 512]               4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-6      [5, 1, 512]               4,204,032\n",
              "│    │    └─TransformerDecoderLayer: 3-7      [5, 1, 512]               4,204,032\n",
              "│    └─LayerNorm: 2-4                         [5, 1, 512]               1,024\n",
              "===============================================================================================\n",
              "Total params: 28,378,624\n",
              "Trainable params: 28,378,624\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 73.60\n",
              "===============================================================================================\n",
              "Input size (MB): 0.02\n",
              "Forward/backward pass size (MB): 1.17\n",
              "Params size (MB): 58.88\n",
              "Estimated Total Size (MB): 60.07\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Transformer?"
      ],
      "metadata": {
        "id": "RU-idFOTZpTb"
      },
      "id": "RU-idFOTZpTb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9972e5f1",
      "metadata": {
        "papermill": {
          "duration": 0.001771,
          "end_time": "2024-12-04T20:02:41.190006",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.188235",
          "status": "completed"
        },
        "tags": [],
        "id": "9972e5f1"
      },
      "source": [
        "## Transformer Encoder\n",
        "- https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch\n",
        "- https://campus.datacamp.com/es/courses/introduction-to-llms-in-python/building-a-transformer-architecture?ex=15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a903f88d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-04T20:02:41.194886Z",
          "iopub.status.busy": "2024-12-04T20:02:41.194604Z",
          "iopub.status.idle": "2024-12-04T20:02:44.080400Z",
          "shell.execute_reply": "2024-12-04T20:02:44.079272Z"
        },
        "papermill": {
          "duration": 2.89072,
          "end_time": "2024-12-04T20:02:44.082652",
          "exception": false,
          "start_time": "2024-12-04T20:02:41.191932",
          "status": "completed"
        },
        "tags": [],
        "id": "a903f88d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ef16ce",
      "metadata": {
        "papermill": {
          "duration": 0.001832,
          "end_time": "2024-12-04T20:02:44.087862",
          "exception": false,
          "start_time": "2024-12-04T20:02:44.086030",
          "status": "completed"
        },
        "tags": [],
        "id": "f3ef16ce"
      },
      "source": [
        "## Transformer Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d04d934",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-04T20:02:44.092834Z",
          "iopub.status.busy": "2024-12-04T20:02:44.092524Z",
          "iopub.status.idle": "2024-12-04T20:02:44.099151Z",
          "shell.execute_reply": "2024-12-04T20:02:44.098473Z"
        },
        "papermill": {
          "duration": 0.01097,
          "end_time": "2024-12-04T20:02:44.100716",
          "exception": false,
          "start_time": "2024-12-04T20:02:44.089746",
          "status": "completed"
        },
        "tags": [],
        "id": "0d04d934"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785f4e31",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-04T20:02:44.105885Z",
          "iopub.status.busy": "2024-12-04T20:02:44.105667Z",
          "iopub.status.idle": "2024-12-04T20:02:44.112270Z",
          "shell.execute_reply": "2024-12-04T20:02:44.111521Z"
        },
        "papermill": {
          "duration": 0.010884,
          "end_time": "2024-12-04T20:02:44.113751",
          "exception": false,
          "start_time": "2024-12-04T20:02:44.102867",
          "status": "completed"
        },
        "tags": [],
        "id": "785f4e31"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt)[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(F.relu(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a3e07d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-04T20:02:44.118840Z",
          "iopub.status.busy": "2024-12-04T20:02:44.118601Z",
          "iopub.status.idle": "2024-12-04T20:02:44.123756Z",
          "shell.execute_reply": "2024-12-04T20:02:44.122967Z"
        },
        "papermill": {
          "duration": 0.009681,
          "end_time": "2024-12-04T20:02:44.125505",
          "exception": false,
          "start_time": "2024-12-04T20:02:44.115824",
          "status": "completed"
        },
        "tags": [],
        "id": "06a3e07d"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_layers, dim_feedforward=2048, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.pe = PositionalEncoding(d_model)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        tgt = self.pe(tgt)\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(tgt, memory)\n",
        "        tgt = self.norm(tgt)\n",
        "        return tgt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db1aa3d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-04T20:02:44.130278Z",
          "iopub.status.busy": "2024-12-04T20:02:44.129997Z",
          "iopub.status.idle": "2024-12-04T20:02:44.818542Z",
          "shell.execute_reply": "2024-12-04T20:02:44.817093Z"
        },
        "papermill": {
          "duration": 0.692984,
          "end_time": "2024-12-04T20:02:44.820409",
          "exception": false,
          "start_time": "2024-12-04T20:02:44.127425",
          "status": "completed"
        },
        "tags": [],
        "id": "0db1aa3d",
        "outputId": "4b4a73eb-2d5f-414a-e025-93e9b03d3ee0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([20, 32, 512])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_layers = 6\n",
        "decoder = Decoder(d_model, nhead, num_layers)\n",
        "memory = torch.rand(10, 32, d_model)\n",
        "tgt = torch.rand(20, 32, d_model)\n",
        "output = decoder(tgt, memory)\n",
        "print(output.shape)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 119408755,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 6.896552,
      "end_time": "2024-12-04T20:02:45.742633",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-12-04T20:02:38.846081",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}