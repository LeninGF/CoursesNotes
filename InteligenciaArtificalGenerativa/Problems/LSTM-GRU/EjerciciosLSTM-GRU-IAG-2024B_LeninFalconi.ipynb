{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios sobre Redes Neuronales Recurrentes\n",
        "\n",
        "Coder: Lenin G. Falconí\n",
        "\n",
        "Asignatura: Tópicos Especiales (Inteligencia Artificial)\n",
        "\n",
        "Fecha: 2024-11-05"
      ],
      "metadata": {
        "id": "j_meEkMh7c8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 1 Preparación de Datos de Texto para Modelos Supervisados\n",
        "En este ejercicio, se trabajará con datos de texto analizando citas de Sheldon Cooper en la serie de televisión The Big Bang Theory. Esto permitirá analizar oraciones para obtener ideas sobre cómo es trabajar con datos de texto del mundo real.\n",
        "\n",
        "Se usarán comprensiones de diccionarios para crear diccionarios que asignen palabras a índices y viceversa. El uso de diccionarios, en lugar de, por ejemplo, un pandas.DataFrame, se debe a que son más intuitivos y no añaden una complejidad extra innecesaria. Los datos están disponibles en sheldon_quotes:"
      ],
      "metadata": {
        "id": "HjYR2tST8VtQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89a4aDJ27cRc",
        "outputId": "c1a574d9-9cf8-4abb-e26b-951d68767c0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
              " 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "sheldon_quotes = [\"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
        "                  'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.']\n",
        "sheldon_quotes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instrucciones\n",
        "Unir las oraciones en una variable y luego extraer todas las palabras, almacenándolas en `all_words`."
      ],
      "metadata": {
        "id": "VfwGrTH38yLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = ''.join(sheldon_quotes).split(' ')\n",
        "print(all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-YTWJdu9AKM",
        "outputId": "2a4e25f6-6db5-4160-d95b-59a2b1a0d670"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"You're\", 'afraid', 'of', 'insects', 'and', 'women,', 'Ladybugs', 'must', 'render', 'you', 'catatonic.Scissors', 'cuts', 'paper,', 'paper', 'covers', 'rock,', 'rock', 'crushes', 'lizard,', 'lizard', 'poisons', 'Spock,', 'Spock', 'smashes', 'scissors,', 'scissors', 'decapitates', 'lizard,', 'lizard', 'eats', 'paper,', 'paper', 'disproves', 'Spock,', 'Spock', 'vaporizes', 'rock,', 'and', 'as', 'it', 'always', 'has,', 'rock', 'crushes', 'scissors.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtener palabras únicas"
      ],
      "metadata": {
        "id": "d9M5MfzM9Mb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = list(set(all_words))\n",
        "print(len(unique_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7-zlqES9N0r",
        "outputId": "d044a275-9726-496f-b7b1-96e37f4cf63d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG45MvCc9PpO",
        "outputId": "19936cf4-c0ab-41e3-8da8-32b972ae6903"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"You're\",\n",
              " 'paper',\n",
              " 'scissors',\n",
              " 'has,',\n",
              " 'disproves',\n",
              " 'rock,',\n",
              " 'Ladybugs',\n",
              " 'you',\n",
              " 'as',\n",
              " 'women,',\n",
              " 'cuts',\n",
              " 'Spock',\n",
              " 'poisons',\n",
              " 'it',\n",
              " 'Spock,',\n",
              " 'covers',\n",
              " 'afraid',\n",
              " 'render',\n",
              " 'decapitates',\n",
              " 'and',\n",
              " 'rock',\n",
              " 'eats',\n",
              " 'vaporizes',\n",
              " 'lizard,',\n",
              " 'scissors,',\n",
              " 'crushes',\n",
              " 'lizard',\n",
              " 'smashes',\n",
              " 'paper,',\n",
              " 'insects',\n",
              " 'scissors.',\n",
              " 'of',\n",
              " 'always',\n",
              " 'catatonic.Scissors',\n",
              " 'must']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear un diccionario con índices como claves y palabras como valores usando\n",
        "comprensiones de diccionarios."
      ],
      "metadata": {
        "id": "ykiQSbVv9ZQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {i+1:wd for i, wd in enumerate(sorted(unique_words))}\n",
        "print(index_to_word.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e_aHAV79g9C",
        "outputId": "c71d244c-ae98-46b5-efc0-338b84e1d2de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([(1, 'Ladybugs'), (2, 'Spock'), (3, 'Spock,'), (4, \"You're\"), (5, 'afraid'), (6, 'always'), (7, 'and'), (8, 'as'), (9, 'catatonic.Scissors'), (10, 'covers'), (11, 'crushes'), (12, 'cuts'), (13, 'decapitates'), (14, 'disproves'), (15, 'eats'), (16, 'has,'), (17, 'insects'), (18, 'it'), (19, 'lizard'), (20, 'lizard,'), (21, 'must'), (22, 'of'), (23, 'paper'), (24, 'paper,'), (25, 'poisons'), (26, 'render'), (27, 'rock'), (28, 'rock,'), (29, 'scissors'), (30, 'scissors,'), (31, 'scissors.'), (32, 'smashes'), (33, 'vaporizes'), (34, 'women,'), (35, 'you')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear un diccionario con palabras como claves e índices como valores usando\n",
        "comprensiones de diccionarios."
      ],
      "metadata": {
        "id": "Ds7tR0Eo9lYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {wd:i+1 for i, wd in enumerate(sorted(unique_words))}\n",
        "print(word_to_index.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5ElsDQB9oVT",
        "outputId": "75b086d1-bd69-49c8-a943-0fe074a8fbe8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('Ladybugs', 1), ('Spock', 2), ('Spock,', 3), (\"You're\", 4), ('afraid', 5), ('always', 6), ('and', 7), ('as', 8), ('catatonic.Scissors', 9), ('covers', 10), ('crushes', 11), ('cuts', 12), ('decapitates', 13), ('disproves', 14), ('eats', 15), ('has,', 16), ('insects', 17), ('it', 18), ('lizard', 19), ('lizard,', 20), ('must', 21), ('of', 22), ('paper', 23), ('paper,', 24), ('poisons', 25), ('render', 26), ('rock', 27), ('rock,', 28), ('scissors', 29), ('scissors,', 30), ('scissors.', 31), ('smashes', 32), ('vaporizes', 33), ('women,', 34), ('you', 35)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 2: Mapeo de Palabras a Índices y Manejo de Palabras Desconocidas\n",
        "Previamente, se aprendió a crear diccionarios de índices a palabras y viceversa. En este ejercicio, se dividirá el texto por caracteres y se continuará preparando los datos para aprendizaje supervisado.\n",
        "\n",
        "Dividir los textos en caracteres puede parecer extraño, pero se hace con frecuencia para la generación de texto. Además, el proceso para preparar los datos es el mismo; el único cambio es cómo dividir los textos.\n",
        "\n",
        "Se creará el conjunto de datos de entrenamiento que contiene una lista de textos de longitud fija y sus etiquetas, que son los caracteres siguientes correspondientes.\n",
        "\n",
        "Se continuará utilizando el conjunto de datos que contiene citas de Sheldon (The Big Bang Theory), disponible en la variable sheldon_quotes.\n",
        "\n",
        "La función `print_examples()` imprimirá los pares para que se pueda ver cómo se\n",
        "transformaron los datos. Usar help() para ver detalles."
      ],
      "metadata": {
        "id": "dH1kNeYH9yeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step = 2\n",
        "chars_window = 10\n",
        "next_chars = []\n",
        "sentences = []\n",
        "combined_text = ' '.join(sheldon_quotes)\n",
        "len(combined_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jyWcJMo-MjR",
        "outputId": "bbe2fbfc-4378-410d-a799-b8bcb0b3a917"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "316"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "YJiL22O-C62Y",
        "outputId": "da5f9a40-cf09-451b-db0c-bba22481c0e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You're afraid of insects and women, Ladybugs must render you catatonic. Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(combined_text) - chars_window, step):\n",
        "    sentences.append(combined_text[i: i + chars_window])\n",
        "    next_chars.append(combined_text[i + chars_window])"
      ],
      "metadata": {
        "id": "zVh_Kg8H_Ri4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "source": [
        "def print_example(train_sentences, char_labels, num_examples=10):\n",
        "  \"\"\"Prints a specified number of training examples.\n",
        "\n",
        "  Args:\n",
        "    train_sentences: A list of training sentences.\n",
        "    char_labels: A list of corresponding labeled characters.\n",
        "    num_examples: The number of examples to print (default is 5).\n",
        "  \"\"\"\n",
        "  for i in range(min(num_examples, len(train_sentences))):\n",
        "    print(f\"Sentence: '{train_sentences[i]}', Next Char: '{char_labels[i]}'\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ILNwtLpXD87X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_example(sentences, next_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWZE9yeOAapc",
        "outputId": "ddd889e1-f6c2-40e0-c904-9ccdb52797aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: 'You're afr', Next Char: 'a'\n",
            "Sentence: 'u're afrai', Next Char: 'd'\n",
            "Sentence: 're afraid ', Next Char: 'o'\n",
            "Sentence: ' afraid of', Next Char: ' '\n",
            "Sentence: 'fraid of i', Next Char: 'n'\n",
            "Sentence: 'aid of ins', Next Char: 'e'\n",
            "Sentence: 'd of insec', Next Char: 't'\n",
            "Sentence: 'of insects', Next Char: ' '\n",
            "Sentence: ' insects a', Next Char: 'n'\n",
            "Sentence: 'nsects and', Next Char: ' '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 3: Reconstrucción de Texto a partir de Índices\n",
        "\n",
        "En este ejercicio, se transformará un texto nuevo en secuencias de índices numéricos utilizando los diccionarios creados anteriormente.\n",
        "\n",
        "Esto es útil cuando ya se cuenta con un modelo entrenado y se quiere aplicarlo en un nuevo conjunto de datos. Los pasos de preprocesamiento realizados en los datos de entrenamiento deben aplicarse también al texto nuevo, para que el modelo pueda realizar predicciones o\n",
        "clasificaciones.\n",
        "\n",
        "Aquí, se usará también un token especial `<UKN/>` para representar palabras que no estén en el vocabulario. Normalmente, estos tokens especiales ocupan las primeras posiciones en  los diccionarios, siendo la posición 0.\n",
        "\n",
        "Las variables `word_to_index`, `index_to_word` y `vocabulary` ya están cargadas en el\n",
        "entorno. Además, la variable con el nuevo texto está disponible como new_text. El texto nuevo ha sido impreso para que puedas revisarlo."
      ],
      "metadata": {
        "id": "F85mjrp4Eh3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = [\n",
        "    \"Bazinga!\",\n",
        "    \"Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.\",\n",
        "    \"I'm not crazy, my mother had me tested.\",\n",
        "    \"Smart is the new sexy.\",\n",
        "    \"The Big Bang Theory\"\n",
        "]"
      ],
      "metadata": {
        "id": "xwzk_IKPFe8j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Asignar el índice 0 en caso de que la palabra no se encuentre en el diccionario.\n",
        "- Agregar la oración con los índices a la variable `new_text_split`.\n",
        "- Convertir los índices de vuelta a texto utilizando el diccionario `index_to_word`."
      ],
      "metadata": {
        "id": "eJxCV4J_IFy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos asumir que el vocabulario está dado por las palabras únicas obtenidas del Sheldon Quotes"
      ],
      "metadata": {
        "id": "cfsO9HnXIRaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the sentences and get indexes\n",
        "new_text_split = []\n",
        "for sentence in new_text:\n",
        "  sent_split = []\n",
        "  for wd in sentence.split(' '):\n",
        "    index = word_to_index.get(wd, 0)\n",
        "    sent_split.append(index)\n",
        "  new_text_split.append(sent_split)"
      ],
      "metadata": {
        "id": "Ibjj7n7_IXcj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the second sentence's indexes\n",
        "print(new_text_split[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gulTTPGSIrdM",
        "outputId": "902d8f46-fae9-406f-c59b-ba4eedc462b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 12, 24, 23, 10, 28, 27, 11, 20, 19, 25, 3, 2, 32, 30, 29, 13, 20, 19, 15, 24, 23, 14, 3, 2, 33, 28, 7, 8, 18, 6, 16, 27, 11, 31]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Añado el Token de `<UKN/>` al diccionario `index_to_word`"
      ],
      "metadata": {
        "id": "eDsZvqtML2Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word[0] = '<UKN/>'"
      ],
      "metadata": {
        "id": "gS-f81lmME_e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the sentence converted using the dictionary\n",
        "print(' '.join([index_to_word[index] for index in new_text_split[1]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNSn6vF7Lnn6",
        "outputId": "969693fa-e68e-4e13-b465-6501d489f623"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<UKN/> cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 4: Descripción del Problema: Construcción de un Modelo Secuencial con una capa LSTM\n",
        "\n",
        "En este ejercicio, se practicará el uso de dos clases del módulo keras.models. Se creará un\n",
        "modelo usando la clase `Sequential` y otro modelo con la clase `Model`.\n",
        "\n",
        "La clase Sequential es más fácil de usar porque se asume que las capas están en orden, mientras que la clase `Model` es más flexible y permite múltiples entradas, múltiples salidas y capas compartidas (pesos compartidos).\n",
        "\n",
        "La clase `Model` necesita declarar explícitamente la capa de entrada, mientras que en la clase `Sequential` esto se hace con el parámetro `input_shape`.\n",
        "\n",
        "Los objetos y módulos `Sequential`, `Model`, `Dense`, `Input`, `LSTM` y `np` (numpy) ya están cargados en el entorno."
      ],
      "metadata": {
        "id": "drPGZ4-7MaKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instrucciones  \n",
        "- Instanciar el modelo `Sequential` con el nombre sequential_model.\n",
        "- Agregar una capa `LSTM` y una capa `Dense`, e imprimir el resumen.\n",
        "- Crear una capa de `Input`, agregar capas `LSTM` y `Dense`, y almacenar en `main_output`.\n",
        "- Instanciar el modelo e imprimir su resumen."
      ],
      "metadata": {
        "id": "azWNmiOhM1zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM"
      ],
      "metadata": {
        "id": "Ntii7EEeNIn-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar la clase\n",
        "model = Sequential(name=\"sequential_model\")"
      ],
      "metadata": {
        "id": "bV61EggLNVQh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Una capa LSTM (definiendo la forma de entrada porque es la capa inicial)\n",
        "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ5WNq59Nazm",
        "outputId": "d737c576-b29b-4575-c6dd-53b3938d9571"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Añadir una capa densa con una unidad\n",
        "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))"
      ],
      "metadata": {
        "id": "UuJnSwO6Njy2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "wstocHFHNtpy",
        "outputId": "f03574d5-2b2f-4cd7-aade-4f73eb8636c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ LSTM (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m71,168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ LSTM (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">71,168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m71,297\u001b[0m (278.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,297</span> (278.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m71,297\u001b[0m (278.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,297</span> (278.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "5wtqGkOwN-OH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(None, 10), name=\"input_layer\")\n",
        "lstm_layer = LSTM(128, name=\"LSTM_layer\")(inputs)\n",
        "outputs = Dense(1, activation=\"sigmoid\", name=\"output_layer\")(lstm_layer)\n",
        "model2 = Model(inputs=inputs, outputs=outputs, name=\"Model_model\")\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "FdOEN8HeO4dS",
        "outputId": "b1a9787f-cb78-4986-9014-112088c6efe4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Model_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Model_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ LSTM_layer (\u001b[38;5;33mLSTM\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m71,168\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ LSTM_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">71,168</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m71,297\u001b[0m (278.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,297</span> (278.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m71,297\u001b[0m (278.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">71,297</span> (278.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 5: Preprocesamiento\n",
        "\n",
        "El segundo módulo más importante de Keras es `keras.preprocessing`. Aquí se verá cómo utilizar los módulos y funciones más importantes para preparar datos en bruto al formato de entrada adecuado. Keras proporciona funcionalidades que sustituyen el enfoque\n",
        "de diccionarios que se aprendió anteriormente.\n",
        "\n",
        "Se utilizará el módulo `keras.preprocessing.text.Tokenizer` para crear un diccionario de palabras usando el método `.fit_on_texts()` y transformar los textos en identificadores numéricos que representan el índice de cada palabra en el diccionario mediante el método `.texts_to_sequences()`.\n",
        "\n",
        "Luego, se usará la función `.pad_sequences()` de `keras.preprocessing.sequence` para que todas las secuencias tengan el mismo tamaño (necesario para el modelo) añadiendo ceros en los textos cortos y cortando los textos largos.\n",
        "\n",
        "### Instrucciones\n",
        "-  Importar `Tokenizer` y `pad_sequences` de los módulos relevantes.\n",
        "- Ajustar el objeto `tokenizer` en los datos de muestra almacenados en `texts`.\n",
        "- Transformar los textos en secuencias de índices numéricos usando el método `.texts_to_sequences()`.\n",
        "- Fijar el tamaño de los textos mediante el padding."
      ],
      "metadata": {
        "id": "kLMHdSaEPN6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "_tROmAFFQF2y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the dictionary of indexes\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sheldon_quotes)"
      ],
      "metadata": {
        "id": "yWWZE_s0RulS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change texts into sequence of indexes\n",
        "texts_numeric = tokenizer.texts_to_sequences(sheldon_quotes)\n",
        "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]),\n",
        "len(texts_numeric[1])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUVfXo_IR4s8",
        "outputId": "210ab1d7-31f4-4433-ca81-306af9dca8d8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the sample texts: (11, 35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences\n",
        "texts_pad = pad_sequences(texts_numeric, 60)\n",
        "print(\"Now the texts have length: 60. Let's see the first one\\n{}\\n\".format(texts_pad[0]), len(texts_pad[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNG-l_Z5STv1",
        "outputId": "7bb7122c-119a-4785-e873-e9718bc8db91"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now the texts have length: 60. Let's see the first one\n",
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  8  9 10 11  6 12 13 14 15 16 17]\n",
            " 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts_pad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE-WLIYOS8AU",
        "outputId": "fca63b98-9e36-4d2f-aab2-a06b376c7a11"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  8,  9, 10, 11,  6, 12, 13, 14, 15, 16, 17],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 18,  2,  2, 19,  3,  3,\n",
              "         7,  4,  4, 20,  5,  5, 21,  1,  1, 22,  4,  4, 23,  2,  2, 24,\n",
              "         5,  5, 25,  3,  6, 26, 27, 28, 29,  3,  7,  1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 6: Modelo RNN\n",
        "En este ejercicio, se pone en práctica los módulos de Keras para construir un modelo RNN y utilizarlo para clasificar sentimientos en reseñas de películas.\n",
        "\n",
        "Este primer modelo tiene una capa recurrente con la celda RNN básica: SimpleRNN, y la capa de salida con dos posibles valores: 0, que representa un sentimiento negativo, y 1, que representa un sentimiento positivo.\n",
        "\n",
        "Utilizar el conjunto de datos IMDB. Ya se ha entrenado un modelo y sus pesos están\n",
        "almacenados en el archivo model_weights.h5. Si no se tiene este archivo, omitir la línea y\n",
        "en su lugar entrenar el modelo durante 3 épocas usando el conjunto IMDB. Construir la\n",
        "arquitectura del modelo y utilizar las variables precargadas `x_test` y `y_test` para verificar\n",
        "su rendimiento.\n",
        "### Instrucciones\n",
        "1. Agregar la celda `SimpleRNN` con 128 unidades.\n",
        "2. Agregar una capa `Dense` con una unidad para la clasificación de sentimientos.\n",
        "3. Usar la función de pérdida adecuada para la clasificación binaria.\n",
        "4. Evalúar el modelo en el conjunto de validación preentrenado: `(x_test, y_test)`.\n",
        "\n",
        "- Es necesario además añadir una capa de embedding al modelo para transformar cada  palabra (representada por un índice) en un vector de tamaño `embedding_dim` (en este caso, 32).\n",
        "- Esto cambia la forma de la entrada de `(None, 500)` a `(None, 500, 32)`, que es compatible\n",
        "con la capa `SimpleRNN`.\n",
        "\n",
        "### Parámetros Embedding:\n",
        "- `input_dim=vocab_size`: El tamaño del vocabulario.\n",
        "- `output_dim=embedding_dim`: La dimensión del vector de cada palabra.\n",
        "- `input_length=max_len`: La longitud fija de cada secuencia después de aplicar padding."
      ],
      "metadata": {
        "id": "89EtbzNxTPHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, InputLayer\n",
        "\n",
        "# Configuración de parámetros\n",
        "vocab_size = 10000  # Tamaño del vocabulario\n",
        "maxlen = 500  # Longitud máxima de las secuencias de entrada\n",
        "embedding_dim = 32  # Dimensión de los embeddings"
      ],
      "metadata": {
        "id": "OourzRuUT1Ut"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset de IMDB\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "\n",
        "# Padding de las secuencias para que todas tengan la misma longitud\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXEJxJqdUtYW",
        "outputId": "6fe985d6-de09-4f46-9f9e-5a7036dfb9d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el modelo\n",
        "model = Sequential([\n",
        "    InputLayer(input_shape=(maxlen,)),\n",
        "    Embedding(vocab_size, embedding_dim, input_length=maxlen),  # Capa de embeddings\n",
        "    SimpleRNN(128, return_sequences=False),  # Capa LSTM\n",
        "    Dense(1, activation='sigmoid')  # Capa de salida con una sola neurona\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "3wIt6MLtUucg",
        "outputId": "dcdaecb6-7d0d-4ff8-d10c-647ee9cdbfa6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m320,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m20,608\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">320,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m340,737\u001b[0m (1.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">340,737</span> (1.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m340,737\u001b[0m (1.30 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">340,737</span> (1.30 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Bl3kZHsBU6wf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEOSgPH3U_Fs",
        "outputId": "a6afeda1-0783-432d-dbf8-13b437e5cd02"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.5129 - loss: 0.6975 - val_accuracy: 0.5760 - val_loss: 0.6653\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 24ms/step - accuracy: 0.6619 - loss: 0.6150 - val_accuracy: 0.6354 - val_loss: 0.6319\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - accuracy: 0.7740 - loss: 0.4787 - val_accuracy: 0.7450 - val_loss: 0.5412\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.8251 - loss: 0.3913 - val_accuracy: 0.7214 - val_loss: 0.5892\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.7970 - loss: 0.4407 - val_accuracy: 0.6804 - val_loss: 0.6187\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.7776 - loss: 0.4655 - val_accuracy: 0.6904 - val_loss: 0.6316\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.8127 - loss: 0.4157 - val_accuracy: 0.7008 - val_loss: 0.6623\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.8099 - loss: 0.4118 - val_accuracy: 0.6124 - val_loss: 0.8838\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 24ms/step - accuracy: 0.8421 - loss: 0.3634 - val_accuracy: 0.7092 - val_loss: 0.6797\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.8838 - loss: 0.2818 - val_accuracy: 0.7158 - val_loss: 0.6878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo en el conjunto de prueba\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNc1VxnpVIBv",
        "outputId": "0aabc1d3-2fcd-414b-fecd-8b73866e2ab2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7270 - loss: 0.6636\n",
            "Loss: 0.6552079916000366\n",
            "Accuracy: 0.7283999919891357\n"
          ]
        }
      ]
    }
  ]
}