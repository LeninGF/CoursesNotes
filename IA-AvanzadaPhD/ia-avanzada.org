
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Inteligencia Artificial Generativa
#+date: 2024-10-02
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment
* 2025-04-16 Notas iniciales PEA
** Questions and keywords
- retropropagacion del gradiente
- rmsprop, adam
- test time compute y razonamiento
- algoritmos avanzados para RL
- scientific machine learning :: inyecta conocimiento a priori par
  aque no sea solo a traves de datos. una RNN mapea o construye
  funciones en espacio de caracteristicas las RN Fisicas mapean en un
  espacio de funciones.
- computacion cuantica e IA
- Quedan tres años
** Notes
- Principios y Fundamentos
- No hay primer semestre o segundo semestre
- prsentar un articulo que tenga nivel de congreso tipo B o superior
- tiene que ser en el  tema qeu etstamos haciendo
- articulo ha de ser desarrollo de una idea en el trabajo de
  investigacion
- marco proporciona la metodologia....
- hay que ver el grado de avance....
- caṕ1 y cap2 usar en el tema de investigacion....
- no articulo de juguete.....
*** Libros
- Deep learning bases y conceptos: empieza con el capitulo 7
- checar capitulo 12del libro de bases tiene que ver con transformers
- pagina 395 tiene transformers de vision
- understanding deep learning de smunce prince
- murphy advanced probabilistic machine learning 847 del pdf
  - fundamentos actualizados
  - topicos avanzados
- el enfoque de la asignatura es investigacion basica
** Summary
* 2025-04-22 Capitulo 1 Modelos Generativos de Lenguaje
** Questions and keywords
- Procesamiento de Lenguaje natural
- Modelos multimodal :: usan datos tanto lenguaje y visuales
- lenguajes formales :: mas faciles de procesar
- lenguaje natural :: es dificil de procesar por la maquina
- funcion de probabilidad :: 
- modelo de lenguaje ::
- FF :: red feedforward 
- LSTM :: funciono parcialmente
- token ::
- byte pair encoding ::
- word2vec ::
- de cuanto es la longitud de los embedings
- de cuanto es el tamaño del tokenizador
- influyen los pesos de los ids de los toekens
** Notes
- El boom de gpt es simular comprender el lenguaje
- nlp es complejo y es punto de partida de transformers y mecanismos
  de atención
- los lenguajes formales y artificiales son mas faciles de maniobrar y
  manipular pero no son conocidos o practicos para transmitir mensajes
- estamos dentro dle campo de la percepcion computacional
*** modelo de lenguajes
- todo algorimot de machine learning es una funcion matematica
- un modelo de lenguaje es una funcion que tiene como entrada lenguaje
  y la salida puede ser numeros o lenguaje
- es una funcion matematica
- el lenguaje es ambiguo y por esta razon es preferible usar una
  funcion de probabilidad que una determinstica
- la funcion de probailidad me permite calcular la probabilidad de la
  secuencia de tokens o palabras e.g. el modelo esta feliz
- una distribucion d eprobabilida dsirve para calcular probabilidades
- la distribucion d eprobabilidad sirve para sampling
- los modelos generativos samplean a partir de la funcion de probabilidad
- distribucion discretas sobre todo se usan
- caundo uno no sabe que funcion de distribucion probabilidad usar
  entoence recurro a usar una red neuronal. Ejemplo una FF con una softmax
- uno de los problemas importantes en lenguaje es que las cadenas de
  texto pueden llegar a ser secuencias largas. una palabra que esta
  cientos de pasos atras puede cambiar el signficado de texto
- feed forward no sirvieron para muchos casos
- RNN: LSTM, GRU, Gated
- LSTM era el estado del art hasta 10 años atras
- luego surje LSTM + mecanismo de atención
- finalmente llegan los transformers
- si en el ejemplo las cadenas de texto si no tienen relacion tiene
  que dar probabilidades baja y si tiene relacion tiene probabilidad alta
- entonces la probabilidad condicional refleja como funciona el lenguaje.
- modelos de lenguaje son distribuciones de probabilidad y estas
  distribuciones se representan con RNA
- porque no se usa tokens y no palabras
- tokenization es como un diccionario
- lo que se termina aprendiendo es el vector que representa a cada embediing
- como generamos los tokens?
- un token tiene en español e ingles una equiv de 3/4 de palabra
- hay un punto que en funcion  de las iteraciones que alcanzamos un
  maximo de tokens entonces hay que llegar a que no sean palabras
  individuales ni tampoco llegar a un punto atomizante tanto que no
  hay signficado
- se tomo la idea fusionar bits para formar cadenas para poder llegar
  a la formacion de los tokens. la termiinacion es algo arbitraria
- gpt1 tokenizador es distinto al gpt2
- otra cosa que no es evidente es que un computador tiene que procesar
  numeros. necesito convertir las cadenas en texto en numeros
- consideremos dos palabras que pueden estar juntas en el diccionario
  pero que peuden tener significados distintos: diablo dios. por
  ejemplo si ordeno alfabeticamente no refejaria que esos bojetos son
  como contrarios. por esta razon es que necesito cada palabra se
  represente con un vector de numeros.
- como se hace la codificacion en vectores
- crear un embeding de una imagen y analizar que partes del vector
  tenia que ver con edad joven risa
- modelo de incrustacion correspondiente para cada token. Para el
  modelo del lenguaje ingresa una cadena de vectores
- paper recomendado : a survey of large language models  en arxiv la
  figura 7 es muy interesante.
  1. tomar scrapping d etexto
  2. filtrado y seleccion
  3. dedupicacion es decir quitar repeticiones
  4. privacy reduction quitar identificadores personales
  5. tokenizar 
** Summary

* 2025-04-23 
** Questions and keywords
- modo agente en LLM ::
- pesos sinapticos ::
- hay dudas sobre el costo de deepseek de 6 millones ::
- checkar costo ofertas laborales de openai meta y otras empresas grandes ::
- proyecto starlink ::
- testime compute :: parece que es una manera de obtener mas de lo
  poco que queda (limones)
- FLOPS :: cuantas operaciones en punto flotante se puede hacer con
  single y double precision hay formatos en 16 en machine learning
  para reducir entrenamiento.
- MAUs ::  numeor de nuevos usuarios agregandos por mes
** Notes
- forma de representar palabras como vectores
- los embeddings en esencia aplicar transformacines no lineales a los vectores
- los modelos d elenguaje no pueden crear nuevas cosas.... basicamente
  simulan capacidad de comprension y transforman vectores
- fig 6 del paper de survey of LLM se leen com giga tokens
- la fig 6 muestra el desafio de la actuallidad de los datos con los
  que aprende es decir que el conocimiento no este actualizado. por
  costo computacional no puede analizar toda la data de internet. un
  problema es como obtener datos actualizados
- para que los modelos tengan un conocimiento mas exacto usar los
  repositorios propios de IEEE, Springer ACM
- los modelos no pueden crear nada nuevo con respecto a lo que ya existe.
- pueden ser vistos como unos meros compresores de datos.
- T5 es bastante antiguo
- gpt3 es 2021 a 2022
- otro problema que se tiene qes que la informacion disponible
  completa es sobre los modelos abiertos y un poco antiguos. no hay
  una LLM anclada a la universidad.
- la barrera estaba en leer papers.
- la barrera esta en que no hay papers
- hay tres ejes en LLM: datos, hardware y conocimiento
  - hardware: decenas de miles de gpus
  - conocimiento avanzado: personas altamente especializadas. algo en
    las universidades pero mas en las industrias.
  - datos: 100tos de terabytes tokens
- los tres ejes requieren bastante inversion
- 500 k USD por a;o a personas que saben del tema
*** leyes de escalado de LLM
- LLama 3 cuesta unos 80 millones de dolares.
- presenta la ecuacion 2 para cualcular la funcion de perdida dado el
  tamaño del modelo y la cantidad de datos . los coefcientes A,B \alpha
- mientras mas grande sea mmodelo y mas datos la funcion de perdida es menor
- la generacion de datos sinteticos no es muy efectiva a la hora de
  aportar nueva informacion
- cuanto puede el modelo generar de informacion y lo ideal es que
  genere mas bits de informacion que los bits de informacion que le llega
- no se absorbe conocimiento desde lenguaje
- no se cree que los LLMs sean los unicos detras de una IAGeneral
- resulta que recibimos mas datos que una LLM si observamos desde otra perspectiva
- una persona recibe 500x500 pixel y
- la cantidad de informacion de una persona recibe hasta 25 años
  obtiene mas menos 6 Peta Bytes.
- se recicla algo del conocimiento de modelos previos. no se parte d
  elos pesos aleatorios
- En la formula es costo computacional $C \approx 6ND$ con N tamaño
  del modelo y D el tamaño de los datos (tokens)
- costo de entrenamiento de SOTA LLaMA 3 usa 15.6 TeraTokens. en bytes
  es mas
- hay que revisar en que se refiere los billones y trillones en ingles
  y español
- en lama 3 se usa 16 mil tarjetas h100.  esta tarjeta cuesta mas o
  menos 30 mil dolares.
- tema de gpus ver enlace GPU  nvidia h100 specs
  H100
- la compu del labo endra 48GB nvidia a6000 workstation ya hay un par
  instalado 48GB
- el entrenamiento de una LLM puede tomar 70 dias
- costos viene del costo de alquilar el datacenter mas los
  salarios. se considera como 2 dolares por hora de uso d euna gpu H100.
- mas menos asumiendo el costo de entrenar  va a 75 millones
- toneladas equivalentes de Co2
- parece mucho el gasto de energia y co2 en el entrenamiento
- el costo oculto esta en el uso....y la inferencia cuando se usa la IA.
- cuanto hay de consumo del testeo.
- cuanto es el consumo cuando se manda un prompt
- en contra cuanto invierten en hacer sus modelos en cuanto cuanto gastan
- Es el tamaño del modelo en hiperparametros....mas que la arquitectura
- es como una compresion de la informacion y no puedo generar algo
  nuevo
- el modelo si generaliza lo que no pueden crear nada nuevo...
- hacer como habilitades emergentes.....
- es muy distinto que tenga capacidad de imaginacion o de descubirr cosas
*** Emergent abilities por large language models
- en la figura 2 eje vertical tiene exactitud y en el horizontal el
  costo en FLOPS
- a partir de un punto como $10^22$
- cuando el modelo es significativamente grande el modelo exhibe o es
  mejor que un modelo aleatorio.
- aqui el tama;o importa y mientra mas grande mejor.
** Summary
* 2025-04-29 Introducción a Transformers
** Questions and keywords
- mecanismo de atencion ::
- matriz de atención ::
- encoder ::
- decoder ::
- BLEU ::
- multi head self attention :: esta presente tanto en el encoder como
  el decoder
- cross attention :: ingresa flujo de informacin del encoder y del
  decoder es decir viene nifo de dos lineas
- positional encoding :: indica el orden
- no es claro como se integra KQV en los vectores ::
- masked attention ::
- es el masked attention una matriz triangular ::
- modelos multimodales :: hablan de cross attention
- layer normalization ::
- LORA :: intencionalmente usan dos matrices para no hacer una mas
  grande en el aprendizaje
** Notes
- paper a utilizar es Attention is all you need
- revisar el video de gustavo etrala transformers de 40 minutos
- como 8 científicos anónimos inventaron la ia generativa
- ej el modelo se esta alistando para realizar su *presentacion* en el *desfile*
- ej el modelo se esta alistando para que haga buenas **predicciones**
- en ambos casos modelo tiene el mismo embedding pero en las oraciones
  no hace referencia al mismo objeto
- el contexto permite determinar a que objeto
- en algun lado esta un embedding de persona y el mecanismo de
  atencion va a relacionar en el primer caso el embedding d emodelo
  con el de persona y en el segundo con el de un objeto matematico
- los tokens/palabras que mas inciden son presentacion y desfile
- parece logico que la idea seria sumar algo de cada uno de los
  vectores
- cuanto es un poco y eso se define a travies de matriz de atencion
  mediante pesos
- la idea ya estuvo propuesta por Bengio
- el transformer aplicara transformaciones a los vectores para que los
  vectores representen mejor los conceptos y modelar el lenguaje
  e.g. bert y gpt que son encoder/decoder
- cohere es una empreza fundada por uno de los autores
- ya ninguno esta en google
*** multihead self attention
- matriz con las palabras el modelo esta feliz
- el objetivo es ajuste el vector de la palabra modelo
- los pesos de la matriz indican con cuanto aporta cada uno de los vectores
- el orden de la matriz de atencion no influye
- cada uno de los numeros es un porcentaje que indica cuanto influye
  cada token, como el objetivo es hace run nuevo embedding de modelo
  este se forma como 0.2xel+0.4*modelo+0.2*esta+0.2feliz
- el vector esta formado o relacionado con Query Value y Key --Aclarar esto--
- la matriz de self attention tiene el tamaño de la ventana de la
  secuencia d etexto???
- para calcular la matriz de atencionn necesito todos los tokens
- el mecanismos de vaswani se usa en berts porque requiero todos los
  tokens e.g. clasificacion
- el mecanism de vaswani no se usa en los generativos
- en los generativos necesito usar la masked attention
- en el masked attentnio va descubriendo los valores conforme aparecen
  las palabras
- se pone un ejemplo con la oracion el modelo esta feliz y la idea en
  generativos es que se va descubriendo palabra a palabra quitando las
  influencias de otras
- cross attention. por ejemplo tengo dos modalidades un idioma otro
  idioma y texto. la idea es determinar como cada parte voz
  representado por su embedding y como ajustar el embedding de la palabra
- auto atncion es que los propios tokens de la secuencia influyen en
- definir porciones y caracterizarlas por un embedding???
- al cruzar texto e imagenes o texto y audio como se puede relacionar
  los espacios entre los tokens
*** como funcionaba antes con RNN
- igual tienen encoder y decoder
- las rnn sacan hiddens states que son entradas par alas siguientes rnns
- luego se usaba una etapa de atencion
- en la version que usa atencoin ingresa la salida de un hidden state
  mas las salidas de cada uno de los hidden states particulares
- los hidden states se ven afectados por los valores anteriores
- la informacion solo influye en un sentido
- no hay mecanismo explicito de attention en los hidden states
- computacionalmente son pesasdas las RNN para secuencias largas
- no podian abosrver un contexto amplio
- le dieron el nombre de transformers porque los embeddings van
  cambiando su significado en funcion del procesamiento
*** qkv
- vector de preguntas
- vectotr de llaves
- vector de values
- multihead porque se puedene poner en paralelo
- x1 embedding de el
- wq, wk wv se amprenden en entrenamiento
- se usa la idea como hace consultas en la base de datos
- query representa la consulta
- query pregunta y keys responde con cuanto aporta al significado
- un vector que representa la pregunta cual de estos embeddings deben
  influir mas para redefinir el significado
- los keys en cambio indican con cuanto aportan los embeddings
- keys es con cuanto
- tom yeh buscar universidad colorado boulder machine learning by hand
- hace calculos a mano de temas de machine learning
- los embeddings que entran son despues de aplicar el positional
  encodnig en el ejemplo de TOm Yeh
- se multiplica Wq con la matriz de features
- se asume un valor aprendidos de wq, wk y wv
- hay tantos queries como tokens de entrada
- hay tantos keys como tokens de entrada
- esto es clave ya que no se representa toda la informacin en un solo
  vector comprimido
- ahora la informacion esta distribuida
- em el segundo paso se hace un match entre los keys y queris haciendo K^TQ
- el resultado del producto de K traspuesta con Q inidca cuanto debo
  ajustar el token para ajustar el signfiicado
- luego se hace una dvision para escalar los datos dividiendo por
  factor de escala , en el ejemplo le aproximan a 2, cuando el dk es
  la dimension de los keys
- en la parte de softmax esta aplicando 3 elevado a los valores de la
  anterior matriz
- luego suma los valores de cada columna y luego hace la division
  obteneiendo la matriz de pesos de atencion normalizada
- con la mtriz de atencino se define los nuevos features
- QKV sirven para calcular los pesos
- el ajuste de los embeddings son los values multiplicados por la
  matriz de atencion
- el z1 es el embedding que representa el token el
- la salida del multi head va para addicion y normalizacoin
- los que son aprendibles son el Wq, Wk y el Wv
- hay nucleos especiales para cualcups trigonometri para calcular el
  factor de escalamiento
- se observa que esta aptop par ausarse on gpus
- en el transformer se pone varios bloques
- cada bloque tiene pesos diferentes
- cada cabezal aporta con sus representacions par los embeddings
- luego se concatena los paortes yse multiplica por otra matriz de pesos
- boque de adicion y normalizacion
- normalizacion busca que todas las salidas tenga la misma escala y el
  misma media
- la normalizacion e spor features normalizacion
- se obtiene media y desviacion std de cada vector y la normalizacion
  en transformers es por token por cada embedding no por feature
- la normalizacion en transformers es por embedding y es layer
  normalization restar la media y dividir para la desviacion estandar
  por capa
- batch normalization es diferente y en esa defino un tamano
- el ultimo wo que se pone es dar un formato que permite sumar con los
  embeddings de entrada ya que la concatenacion me produciria una
  matriz un tanto grande o mas grande que la entrada
- en el vector z porque se aumentan parametros para aprender? esto
  tiene una relacion a lo que se hace al final de una layer
  normalization y una batch normalization con el objetivo de ajustar
  para las capas que vienen ya que por ejemplo en el transformer le
  suceden FF que podrian usar cualquier funcion de activacion
  teoricamente
- la historia cuenta que havia mas cosas o compoentnes en el
  transformer pero se retiraron porque se mantuvieron con el mismo rendimeinto
- hay que notar que se usa uniones resuduales
- values son los valores d elos candidatos partieron de los concepts
  de busquedas en sql
- es una heurstica el raiz de dk
- porque razon sacan la $d_k$
- la ecuacion de atencion un paralelo a E = mc2? $Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})$
** Summary

* Enlaces
- Libros Generativa: https://drive.google.com/drive/folders/1fjGzPM0VnL40AQd1AVWJfZO05NXsKTya?usp=sharing
- Libros Machine Learning: https://drive.google.com/drive/folders/0B0w0jIatGZYGZHdNdW9FZ1ZycGs?resourcekey=0-6IrcgeLWNwIXoYq8lQ9e0Q&usp=sharing
- https://platform.openai.com/tokenizer
- https://jalammar.github.io/illustrated-word2vec/
- paper A survey of large language models.
- emergent abilities of large language models
- [[https://youtu.be/HX8IMpnESxk?si=JhPz7Fi-_JNPaNR7][8 cientificos inventan IA generativa]]
* TODO
- [ ] Revisar EDOs y EDPs derivadas ordinarias y derivadas parciales
- [ ] revisar el algoritmo de retro-propagacion del error
- [ ] Principios y Fundamentos
- [ ] Definir la idea con respecto a mi tema de investigacion y el
  PEA: hacer pruebas y experimentacion. Hacer ajustes y trabajo Final.
- [ ] despues de la siguiente semana contar la idea que se ha hecho
  proponer la idea y esta idea contiene tales capitulos del temario
  que vamos a ver. necesito saber que problema y que alternativas de
  solucion van a probar esto seria 28 de abril. es sacar el articulo
  con algo de la materia. que seria el bagage teorico.
- [ ] localizar transformers en los llibros de Marco
- [ ] Presentacion concreta de 20 minutos mas 10 minutos para
  preguntas. Presentar la idea
