
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Topicos Especiales II
#+date: 2025-04-15
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment
* 2025-04-16 Notas iniciales PEA
** Questions and keywords
- retropropagacion del gradiente
- rmsprop, adam
- test time compute y razonamiento
- algoritmos avanzados para RL
- scientific machine learning :: inyecta conocimiento a priori par
  aque no sea solo a traves de datos. una RNN mapea o construye
  funciones en espacio de caracteristicas las RN Fisicas mapean en un
  espacio de funciones.
- computacion cuantica e IA
- Quedan tres años
** Notes
- Principios y Fundamentos
- No hay primer semestre o segundo semestre
- prsentar un articulo que tenga nivel de congreso tipo B o superior
- tiene que ser en el  tema qeu etstamos haciendo
- articulo ha de ser desarrollo de una idea en el trabajo de
  investigacion
- marco proporciona la metodologia....
- hay que ver el grado de avance....
- caṕ1 y cap2 usar en el tema de investigacion....
- no articulo de juguete.....
*** Libros
- Deep learning bases y conceptos: empieza con el capitulo 7
- checar capitulo 12del libro de bases tiene que ver con transformers
- pagina 395 tiene transformers de vision
- understanding deep learning de smunce prince
- murphy advanced probabilistic machine learning 847 del pdf
  - fundamentos actualizados
  - topicos avanzados
- el enfoque de la asignatura es investigacion basica
** Silabo:
CAPÍTULO 1. Tópicos Avanzados sobre Error Backpropagation						
1.1 Revisión del algoritmo de retro-propagación del error
1.2 Cálculo numérico de gradientes
1.3 Cálculo manual de gradientes						
1.4 Cálculo simbólico de gradientes
1.5 Cálculo automático de gradientes
CAPÍTULO 2. Optimización Avanzada para RNAs
2.1 Gradient descent with momentum					
2.2 RMSProp			
2.3 Adaptive Moment Estimation (Adam)
2.4 Métodos Quasi-Newton (BFGS y L-BFGS)
2.5 Hessian-Free Optimization 
CAPÍTULO 3. Test-Time Compute y Razonamiento
3.1 MCTS + Self Play + RNAs (AlphaGo, AlphaGo Zero y MuZero)
3.2 Chain of Thought (CoT)
3.3 Tree of Thoughts (ToT)
3.4 Auto-refinamiento
3.5 LLMs y razonamiento simbólico
CAPÍTULO 4. Algoritmos Avanzados de Reinforcement Learning
4.1 REINFORCE
4.2 Advantage Actor-Critic (A2C)
4.3 Proximal Policy Optimization (PPO)
4.4 RL from Human Feedback (RLHF)
4.5 Aplicaciones de RLHF
CAPÍTULO 5. Introducción a Scientific Machine Learning (SciML)
5.1 Introducción a SciML
5.2 Ecuaciones diferenciales ordinarias (EDOs) y parciales (EDPs)
5.3 Physics-Informed Neural Networks (PINNs)
5.4 Aplicaciones de SciML
CAPÍTULO 6. Tópicos Avanzados de IA
6.1 Manifold Hypothesis
6.2 Reward hypothesis
6.3 Plasticidad en RNAs	
6.4 Artificial General Intelligence (AGI)
6.5 Artificial Super Intelligence (ASI)
6.6 Computación cuántica e IA


** Summary
* 2025-04-22 Capitulo 1 Modelos Generativos de Lenguaje
** Questions and keywords
- Procesamiento de Lenguaje natural
- Modelos multimodal :: usan datos tanto lenguaje y visuales
- lenguajes formales :: mas faciles de procesar
- lenguaje natural :: es dificil de procesar por la maquina
- funcion de probabilidad :: 
- modelo de lenguaje ::
- FF :: red feedforward 
- LSTM :: funciono parcialmente
- token ::
- byte pair encoding ::
- word2vec ::
- de cuanto es la longitud de los embedings
- de cuanto es el tamaño del tokenizador
- influyen los pesos de los ids de los toekens
** Notes
- El boom de gpt es simular comprender el lenguaje
- nlp es complejo y es punto de partida de transformers y mecanismos
  de atención
- los lenguajes formales y artificiales son mas faciles de maniobrar y
  manipular pero no son conocidos o practicos para transmitir mensajes
- estamos dentro dle campo de la percepcion computacional
*** modelo de lenguajes
- todo algorimot de machine learning es una funcion matematica
- un modelo de lenguaje es una funcion que tiene como entrada lenguaje
  y la salida puede ser numeros o lenguaje
- es una funcion matematica
- el lenguaje es ambiguo y por esta razon es preferible usar una
  funcion de probabilidad que una determinstica
- la funcion de probailidad me permite calcular la probabilidad de la
  secuencia de tokens o palabras e.g. el modelo esta feliz
- una distribucion d eprobabilida dsirve para calcular probabilidades
- la distribucion d eprobabilidad sirve para sampling
- los modelos generativos samplean a partir de la funcion de probabilidad
- distribucion discretas sobre todo se usan
- caundo uno no sabe que funcion de distribucion probabilidad usar
  entoence recurro a usar una red neuronal. Ejemplo una FF con una softmax
- uno de los problemas importantes en lenguaje es que las cadenas de
  texto pueden llegar a ser secuencias largas. una palabra que esta
  cientos de pasos atras puede cambiar el signficado de texto
- feed forward no sirvieron para muchos casos
- RNN: LSTM, GRU, Gated
- LSTM era el estado del art hasta 10 años atras
- luego surje LSTM + mecanismo de atención
- finalmente llegan los transformers
- si en el ejemplo las cadenas de texto si no tienen relacion tiene
  que dar probabilidades baja y si tiene relacion tiene probabilidad alta
- entonces la probabilidad condicional refleja como funciona el lenguaje.
- modelos de lenguaje son distribuciones de probabilidad y estas
  distribuciones se representan con RNA
- porque no se usa tokens y no palabras
- tokenization es como un diccionario
- lo que se termina aprendiendo es el vector que representa a cada embediing
- como generamos los tokens?
- un token tiene en español e ingles una equiv de 3/4 de palabra
- hay un punto que en funcion  de las iteraciones que alcanzamos un
  maximo de tokens entonces hay que llegar a que no sean palabras
  individuales ni tampoco llegar a un punto atomizante tanto que no
  hay signficado
- se tomo la idea fusionar bits para formar cadenas para poder llegar
  a la formacion de los tokens. la termiinacion es algo arbitraria
- gpt1 tokenizador es distinto al gpt2
- otra cosa que no es evidente es que un computador tiene que procesar
  numeros. necesito convertir las cadenas en texto en numeros
- consideremos dos palabras que pueden estar juntas en el diccionario
  pero que peuden tener significados distintos: diablo dios. por
  ejemplo si ordeno alfabeticamente no refejaria que esos bojetos son
  como contrarios. por esta razon es que necesito cada palabra se
  represente con un vector de numeros.
- como se hace la codificacion en vectores
- crear un embeding de una imagen y analizar que partes del vector
  tenia que ver con edad joven risa
- modelo de incrustacion correspondiente para cada token. Para el
  modelo del lenguaje ingresa una cadena de vectores
- paper recomendado : a survey of large language models  en arxiv la
  figura 7 es muy interesante.
  1. tomar scrapping d etexto
  2. filtrado y seleccion
  3. dedupicacion es decir quitar repeticiones
  4. privacy reduction quitar identificadores personales
  5. tokenizar 
** Summary

* 2025-04-23 
** Questions and keywords
- modo agente en LLM ::
- pesos sinapticos ::
- hay dudas sobre el costo de deepseek de 6 millones ::
- checkar costo ofertas laborales de openai meta y otras empresas grandes ::
- proyecto starlink ::
- testime compute :: parece que es una manera de obtener mas de lo
  poco que queda (limones)
- FLOPS :: cuantas operaciones en punto flotante se puede hacer con
  single y double precision hay formatos en 16 en machine learning
  para reducir entrenamiento.
- MAUs ::  numeor de nuevos usuarios agregandos por mes
** Notes
- forma de representar palabras como vectores
- los embeddings en esencia aplicar transformacines no lineales a los vectores
- los modelos d elenguaje no pueden crear nuevas cosas.... basicamente
  simulan capacidad de comprension y transforman vectores
- fig 6 del paper de survey of LLM se leen com giga tokens
- la fig 6 muestra el desafio de la actuallidad de los datos con los
  que aprende es decir que el conocimiento no este actualizado. por
  costo computacional no puede analizar toda la data de internet. un
  problema es como obtener datos actualizados
- para que los modelos tengan un conocimiento mas exacto usar los
  repositorios propios de IEEE, Springer ACM
- los modelos no pueden crear nada nuevo con respecto a lo que ya existe.
- pueden ser vistos como unos meros compresores de datos.
- T5 es bastante antiguo
- gpt3 es 2021 a 2022
- otro problema que se tiene qes que la informacion disponible
  completa es sobre los modelos abiertos y un poco antiguos. no hay
  una LLM anclada a la universidad.
- la barrera estaba en leer papers.
- la barrera esta en que no hay papers
- hay tres ejes en LLM: datos, hardware y conocimiento
  - hardware: decenas de miles de gpus
  - conocimiento avanzado: personas altamente especializadas. algo en
    las universidades pero mas en las industrias.
  - datos: 100tos de terabytes tokens
- los tres ejes requieren bastante inversion
- 500 k USD por a;o a personas que saben del tema
*** leyes de escalado de LLM
- LLama 3 cuesta unos 80 millones de dolares.
- presenta la ecuacion 2 para cualcular la funcion de perdida dado el
  tamaño del modelo y la cantidad de datos . los coefcientes A,B \alpha
- mientras mas grande sea mmodelo y mas datos la funcion de perdida es menor
- la generacion de datos sinteticos no es muy efectiva a la hora de
  aportar nueva informacion
- cuanto puede el modelo generar de informacion y lo ideal es que
  genere mas bits de informacion que los bits de informacion que le llega
- no se absorbe conocimiento desde lenguaje
- no se cree que los LLMs sean los unicos detras de una IAGeneral
- resulta que recibimos mas datos que una LLM si observamos desde otra perspectiva
- una persona recibe 500x500 pixel y
- la cantidad de informacion de una persona recibe hasta 25 años
  obtiene mas menos 6 Peta Bytes.
- se recicla algo del conocimiento de modelos previos. no se parte d
  elos pesos aleatorios
- En la formula es costo computacional $C \approx 6ND$ con N tamaño
  del modelo y D el tamaño de los datos (tokens)
- costo de entrenamiento de SOTA LLaMA 3 usa 15.6 TeraTokens. en bytes
  es mas
- hay que revisar en que se refiere los billones y trillones en ingles
  y español
- en lama 3 se usa 16 mil tarjetas h100.  esta tarjeta cuesta mas o
  menos 30 mil dolares.
- tema de gpus ver enlace GPU  nvidia h100 specs
  H100
- la compu del labo endra 48GB nvidia a6000 workstation ya hay un par
  instalado 48GB
- el entrenamiento de una LLM puede tomar 70 dias
- costos viene del costo de alquilar el datacenter mas los
  salarios. se considera como 2 dolares por hora de uso d euna gpu H100.
- mas menos asumiendo el costo de entrenar  va a 75 millones
- toneladas equivalentes de Co2
- parece mucho el gasto de energia y co2 en el entrenamiento
- el costo oculto esta en el uso....y la inferencia cuando se usa la IA.
- cuanto hay de consumo del testeo.
- cuanto es el consumo cuando se manda un prompt
- en contra cuanto invierten en hacer sus modelos en cuanto cuanto gastan
- Es el tamaño del modelo en hiperparametros....mas que la arquitectura
- es como una compresion de la informacion y no puedo generar algo
  nuevo
- el modelo si generaliza lo que no pueden crear nada nuevo...
- hacer como habilitades emergentes.....
- es muy distinto que tenga capacidad de imaginacion o de descubirr cosas
*** Emergent abilities por large language models
- en la figura 2 eje vertical tiene exactitud y en el horizontal el
  costo en FLOPS
- a partir de un punto como $10^22$
- cuando el modelo es significativamente grande el modelo exhibe o es
  mejor que un modelo aleatorio.
- aqui el tama;o importa y mientra mas grande mejor.
** Summary
* 2025-04-29 Introducción a Transformers
** Questions and keywords
- mecanismo de atencion ::
- matriz de atención ::
- encoder ::
- decoder ::
- BLEU ::
- multi head self attention :: esta presente tanto en el encoder como
  el decoder
- cross attention :: ingresa flujo de informacin del encoder y del
  decoder es decir viene nifo de dos lineas
- positional encoding :: indica el orden
- no es claro como se integra KQV en los vectores ::
- masked attention ::
- es el masked attention una matriz triangular ::
- modelos multimodales :: hablan de cross attention
- layer normalization ::
- LORA :: intencionalmente usan dos matrices para no hacer una mas
  grande en el aprendizaje
** Notes
- paper a utilizar es Attention is all you need
- revisar el video de gustavo etrala transformers de 40 minutos
- como 8 científicos anónimos inventaron la ia generativa
- ej el modelo se esta alistando para realizar su *presentacion* en el *desfile*
- ej el modelo se esta alistando para que haga buenas **predicciones**
- en ambos casos modelo tiene el mismo embedding pero en las oraciones
  no hace referencia al mismo objeto
- el contexto permite determinar a que objeto
- en algun lado esta un embedding de persona y el mecanismo de
  atencion va a relacionar en el primer caso el embedding d emodelo
  con el de persona y en el segundo con el de un objeto matematico
- los tokens/palabras que mas inciden son presentacion y desfile
- parece logico que la idea seria sumar algo de cada uno de los
  vectores
- cuanto es un poco y eso se define a travies de matriz de atencion
  mediante pesos
- la idea ya estuvo propuesta por Bengio
- el transformer aplicara transformaciones a los vectores para que los
  vectores representen mejor los conceptos y modelar el lenguaje
  e.g. bert y gpt que son encoder/decoder
- cohere es una empreza fundada por uno de los autores
- ya ninguno esta en google
*** multihead self attention
- matriz con las palabras el modelo esta feliz
- el objetivo es ajuste el vector de la palabra modelo
- los pesos de la matriz indican con cuanto aporta cada uno de los vectores
- el orden de la matriz de atencion no influye
- cada uno de los numeros es un porcentaje que indica cuanto influye
  cada token, como el objetivo es hace run nuevo embedding de modelo
  este se forma como 0.2xel+0.4*modelo+0.2*esta+0.2feliz
- el vector esta formado o relacionado con Query Value y Key --Aclarar esto--
- la matriz de self attention tiene el tamaño de la ventana de la
  secuencia d etexto???
- para calcular la matriz de atencionn necesito todos los tokens
- el mecanismos de vaswani se usa en berts porque requiero todos los
  tokens e.g. clasificacion
- el mecanism de vaswani no se usa en los generativos
- en los generativos necesito usar la masked attention
- en el masked attentnio va descubriendo los valores conforme aparecen
  las palabras
- se pone un ejemplo con la oracion el modelo esta feliz y la idea en
  generativos es que se va descubriendo palabra a palabra quitando las
  influencias de otras
- cross attention. por ejemplo tengo dos modalidades un idioma otro
  idioma y texto. la idea es determinar como cada parte voz
  representado por su embedding y como ajustar el embedding de la palabra
- auto atncion es que los propios tokens de la secuencia influyen en
- definir porciones y caracterizarlas por un embedding???
- al cruzar texto e imagenes o texto y audio como se puede relacionar
  los espacios entre los tokens
*** como funcionaba antes con RNN
- igual tienen encoder y decoder
- las rnn sacan hiddens states que son entradas par alas siguientes rnns
- luego se usaba una etapa de atencion
- en la version que usa atencoin ingresa la salida de un hidden state
  mas las salidas de cada uno de los hidden states particulares
- los hidden states se ven afectados por los valores anteriores
- la informacion solo influye en un sentido
- no hay mecanismo explicito de attention en los hidden states
- computacionalmente son pesasdas las RNN para secuencias largas
- no podian abosrver un contexto amplio
- le dieron el nombre de transformers porque los embeddings van
  cambiando su significado en funcion del procesamiento
*** qkv
- vector de preguntas
- vectotr de llaves
- vector de values
- multihead porque se puedene poner en paralelo
- x1 embedding de el
- wq, wk wv se amprenden en entrenamiento
- se usa la idea como hace consultas en la base de datos
- query representa la consulta
- query pregunta y keys responde con cuanto aporta al significado
- un vector que representa la pregunta cual de estos embeddings deben
  influir mas para redefinir el significado
- los keys en cambio indican con cuanto aportan los embeddings
- keys es con cuanto
- tom yeh buscar universidad colorado boulder machine learning by hand
- hace calculos a mano de temas de machine learning
- los embeddings que entran son despues de aplicar el positional
  encodnig en el ejemplo de TOm Yeh
- se multiplica Wq con la matriz de features
- se asume un valor aprendidos de wq, wk y wv
- hay tantos queries como tokens de entrada
- hay tantos keys como tokens de entrada
- esto es clave ya que no se representa toda la informacin en un solo
  vector comprimido
- ahora la informacion esta distribuida
- em el segundo paso se hace un match entre los keys y queris haciendo K^TQ
- el resultado del producto de K traspuesta con Q inidca cuanto debo
  ajustar el token para ajustar el signfiicado
- luego se hace una dvision para escalar los datos dividiendo por
  factor de escala , en el ejemplo le aproximan a 2, cuando el dk es
  la dimension de los keys
- en la parte de softmax esta aplicando 3 elevado a los valores de la
  anterior matriz
- luego suma los valores de cada columna y luego hace la division
  obteneiendo la matriz de pesos de atencion normalizada
- con la mtriz de atencino se define los nuevos features
- QKV sirven para calcular los pesos
- el ajuste de los embeddings son los values multiplicados por la
  matriz de atencion
- el z1 es el embedding que representa el token el
- la salida del multi head va para addicion y normalizacoin
- los que son aprendibles son el Wq, Wk y el Wv
- hay nucleos especiales para cualcups trigonometri para calcular el
  factor de escalamiento
- se observa que esta aptop par ausarse on gpus
- en el transformer se pone varios bloques
- cada bloque tiene pesos diferentes
- cada cabezal aporta con sus representacions par los embeddings
- luego se concatena los paortes yse multiplica por otra matriz de pesos
- boque de adicion y normalizacion
- normalizacion busca que todas las salidas tenga la misma escala y el
  misma media
- la normalizacion e spor features normalizacion
- se obtiene media y desviacion std de cada vector y la normalizacion
  en transformers es por token por cada embedding no por feature
- la normalizacion en transformers es por embedding y es layer
  normalization restar la media y dividir para la desviacion estandar
  por capa
- batch normalization es diferente y en esa defino un tamano
- el ultimo wo que se pone es dar un formato que permite sumar con los
  embeddings de entrada ya que la concatenacion me produciria una
  matriz un tanto grande o mas grande que la entrada
- en el vector z porque se aumentan parametros para aprender? esto
  tiene una relacion a lo que se hace al final de una layer
  normalization y una batch normalization con el objetivo de ajustar
  para las capas que vienen ya que por ejemplo en el transformer le
  suceden FF que podrian usar cualquier funcion de activacion
  teoricamente
- la historia cuenta que havia mas cosas o compoentnes en el
  transformer pero se retiraron porque se mantuvieron con el mismo rendimeinto
- hay que notar que se usa uniones resuduales
- values son los valores d elos candidatos partieron de los concepts
  de busquedas en sql
- es una heurstica el raiz de dk
- porque razon sacan la $d_k$
- la ecuacion de atencion un paralelo a E = mc2? $Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})$
** Summary
* 2025-04-30 Presentaciones
- sparse autoencoder ::
- dictionary learning ::
- gemma scope :: saes open source
- como usan el sparse autoencoder ::
- hipotesis de representacion lineal ::
- feature splitting ::
- composition of lattens ::
- feature absortion ::
- chunks ::
- que es ralo en sparcity auto encoders ::
- feature exploration ::
- que va a vender  ::
- IRL :: funcion de recompensa inversa
- automatic feature extraction :: 
** Notes
*** Conceptos Trabajo Jonathan
- las caracteristicas del sparse autoencoder tiene efecto en el modelo
- los sparse autoencoders son metodo no supervisado
- spas tiene problemas con conceptos jerarquicos
- verificar que anthropic esta lider en explainable IA
- no hay una estructura jerarquica en el spas
- hay una version matryoshka de sparse autoencoders
- modifica la funcion de costo
- capturar relaciones jerarquicas
- es interesante el grafico porque parece los conceptos como una nube
  de particulas de gas y la idea seria atraerlos y condensarlos como
  que fuera una gravitacion
- idea: que pasa si la relacion entre neuronas no la hago de manera
  lineal pero la hago como una ley de gravedad? pero habria un
  problema con la division por la r cuadrada?
- estan trantando de aportar una interpretabilidad para RL
- que es una pregunta inocente....
- en el tema de matryoshka yo me imagine como varias sparse y
  relacionar entre ellas
- parece que los pesos en la matryoshka es que los pesos no quedan igual
- una variante de algoritmos no es suficiente con un solo ejemplo
* Presentacion
- prompt : You are a PhD research Assistant specialized in machine
  learning, vision transformers, computer vision, image
  segmentation. Your task is to help me to solve questions, conceptual
  problems, locate relevant academic sources to improve my
  research. When asked, provide factual answers supported by research
  papers. Present examples and references that support
  answers. Provide research advice with rigor.
** observaciones doc
- hacer el clasificador si el clasificador sirve para mi problema que
  es el de segmentación
- por ejemplo tratar de determinar como primera parte si ViT sirvió
  para algo o no y por ejemplo si no sirvió usar ViT ponerme a usar
  directo CNN
- reducir complejidad debido a los costos computacionales
- la observación de la arquitectura que hubo es que unet si saca algo
  si le meto una imagen de cualquier tamaño en la entrada. luego hubo
  retractación
- Revisar si resolvemos primero termografía
- el tema de entrenar clasificación si y solo si eso servirá para algo
  de la tesis
- tomar contacto con cedia con modemat y con lasinac
- que no sea de juguete que significa? que sirva para la tesis
- ideas --> podria hacer un reescalado progresivo? como iba lo de los
  superpixels??
- hubo una confusión en cuanto a la imagen de entrada y el paso por la unet
- hay que acabar para pasar la nota y garantizar el paper
- Garantizar los datos
- el fundamento de la idea es importante
- pero para saber que la idea sea brillante es experimentar
- si no puedo evaluar no sirve
- si puedo evaluar pero la idea no da un buen resultado entonces no sirve
- el otro tema es la relevancia. proponer una idea y que no sirva.
- cómo voy a evaluar y qué espero tener para decir que la idea es buena
- **la idea esta clara**, revisar y estimar tiempos porque hay que
  terminar el trabajo. si la idea es muy grande tratar de recortar
  cosas y probar la parte conceptual lo que nos quede algo que sea
  núcleo(i.e. core) y que sirva
- asegurarse de cómo van a hacer los experimentos y que van a medir. y
  midiendo *cuándo puedo decir es una buena idea y cuando no es*; hay que
  llegar a decir que la arquitectura propuesta permite alcanzar tal cosa.
- la validez científica de un modelo esta en que también predice.
- que no sea de juguete porque nos gastaría el tiempo. es decir, tiene
  que ser un ejercicio que aporte a la investigación
* 2025-05-06 Tansformers
** Questions and keywords
- crossattention :: not so clear
- maximum path length :: la loongitud de la trayector entre las
  dependencias de largo rango. mientras mas peque;a son los paths mas
  facil aprender dependencias de largo rango
- self attention restricted :: usa ventanas para mitigar el efecto de
  secuencias muy largas
- pesos sinapticos o numero de parametros ::
- self supervised :: porque el propio dataset me da la etiqueta
- semi-suerpvisado :: es la mezcla del aprendizaje supervisado con no supervisado??
- supervised fine tuning ::
- reinforcemnt learnig from human feedback RLFH :: 
** Notes
- para hacer que las matrices v k y q sean diferentes en los heads del
  attention multi head se aplica transformaciones lineales y esta capa
  lineal es learnable. la capa lineal a la salida del multi head
  attention reduce a tener un solo cabezal
- la FFN son dos capas tiene una relu seguido de una capa lineal
- el encoder recibe todo el texto de una
- el decoder va obteniendo cada uno de los tokens de uno en uno segun
  la retroalimentacion que reciben.
- para la tabla 1 del paper n es el tamaño de la secuencia de texto
- d es el tamaño del embedding
- por capa el mecanismo introduce la complejidad $O(n^2d)$
- k es el tam;o del kernel.
- la segunda columna indica si es paralelizable o no
- cada secuencia tenia 25000 tokens. si tengo mas tokens o si crece la
  longitud de la sentencia crece la selfattention puede ser un problema
- la sugerencia del paper es procesar por ventanas
- Gemini y Llama al parecer son capaces de procesar decenas de
  millones de tokens. no hay una respuesta clara de como esta
  funcionando el mecanismo de atencion actualmente en bloques muy grandes
- el modelo hace calculos de probabilidades condicionales
- La ecuacion se traduce en una red neuronal que predice la
  probabilidad del token n dado los tokens anteriores
- usa un modelo que usa piezas del transformer y que aprende con un
  gran corpus de texto
- por cada tarea sea hace un fine tuning
- el modelo inicial aprende una especia de diccionario y de reglas
  gramaticales. esto es el pretrained
- luego usa aprendizaje supervisado para hacer el fine tuning sobre la tarea
- en el gpt se elimina el cross attention, se queda con el multi head
  , la capa de noramlizacion la  capa de feed forward e interconectar
  varios de estos bloques
- gpt1 usa 12 bloques con embeddings de 768
- la longitud de secuencia es de 512
- el gpt solo usa decoder
- que pasa como rellena cuando solo entra un token?
- para que el modelo sea gpt tiene tres tetaps
  - pre entrenamiento
  - SFT
  - RHLF
- Pre entrenamiento:
  - usan aprendizaje semi-supervisado o self-supervised
  - se usa un gran corpus de texto que genera las secuencias de
    entrenamiento usando una ventana de contexto k=4
  - el modelo tiene que predecir el siguente token
  - con una secuencia pequeña se obtiene una gran cantidad de ejemplos
  - la fully connected layer tiene una softmax con el mismo tamaño
    token
  - la salida es una distribucion de 0s para todos los tokens excepto
    el que se enmascaro
  - se entrena con mini batch gradient descent
  - esto le permite al gpt producir el siguiente token dados los
    tokens previos
  - el modelo era bueno para predecir el token siguiente y mejor que
    otros del estado
- SFT
  - se necesita datos con etiqueta
  - en este caso es un juego de inputs o prompts vs outputs
  - hay dos maneras de hacer esto tomar el corpus y cortar la respuesta
  - la otra es curar la respuesta
  - supervised fine tuning. funcion de costo
  - la salida del GPT se vuelve una secuencia de tokens
  - la idea es tratar de obtener las respuesta de base de conocimiento
  - se usa el pretrained con este dataset curado para entrenar el GPT
  - africa se usa para hacer la generacion de los datasets input
    output o prompt generated
  - en el sft la input seria escribe una frase romantica en 2 palabras
    y la salida es eres genial
  - sft requeire muchos datos etiquetados que es un rpoblema
- RHLF
  - se supone que el modelo da cada vez salidas diferentes
  - RHLF se basa en rankear las respuestas. ejemplo a una pregunta e
    lmodelo sacaba otra pregunta
  - por ejemplo a input hola como estas hay dos respuesta que mas ve y
    estoy bien como estas tu. el RHLF escogia la segunda como mejor
    respuesta
  - un tema que queda aqui en estas etapas de alineamiento es el
    problema de sesgos
  - los anotadores tienen que evaluar las respuestas que da el modelo
  - RLHF entra en que se entrea otra red neuronal artificial que tiene
    el objetivo de ser un revisor
  - el modelo de RLFH usa el dataset de ranking para entrenar un
    modelo de recompensas
  - el modelo de recompensas tiene como entrada dos secuencias de
    texto y como salida tienen 0 a 1
  - 1 satisface el requerimiento
  - 0 no satisface lo que se esta pidiendo a chat gpt
  - una vez entrenado el modelo de recompensas se lo usa para entrenar
    chat gpt
  - para esto se toma un sampling del dataset de rankin g de respuestas
  - y se repite el proceso para taratr d emejorar el rendimiento del chatgpt
- chat gpt tiene tres etapas, SFT y el RLFH son con datos especificos
  de la tarea
- GPT es un modelo autoregresivo es decir dado una entrada el gpt
  predice la etiqueta
- muestreo proporcional a la probabilidad
- para darle creatividad a gpt se le da el muestreo de la softmax que sale
- usualrmente sale el token mas probable
- el cambio aleatorio en un token va a generar cambios en la respuesta
  de salida
- el muestreo permite que el modeloo de una respuesta diferencia
  e.g. cambie el como escribe pero no el contenido es decir si
  pregunto 2+2 siempre dara 4 pero lo escribira de maneras distintas
- el proceso d egeneracion termian cuando aparece el token de finalizacoin
- para evitar calculos repetitivos QKV para los tokens ya pasados eso
  se mantiene porque QKV depnden del propio token lo ineficiente seria
  volver ahacer el calculo de QKV cada vez para todos los tokens. es
  decir se calcula el nuevo token (no muy claro como calcula solo lon nuevo)
- enn el pricing del open ai tiene un cached input que son los tokens
  que estan almacenados que se encuentran listos para hacer calculos
- revisar el alineamiento y el preentranmiento del chat gpt. nos falta
  ingresar en detalles que tiene el paper.
- tambien se revisaran tecnicas distitnas de muestreo

** Summary
*** *Sección 1: Preguntas y Palabras Clave*  
1. *Cross-Attention*: Mecanismo en el /decoder/ del Transformer que
   permite a cada token del /decoder/ enfocarse en tokens específicos
   del /encoder/. Por ejemplo, en traducción, el decoder usa
   cross-attention para alinear palabras del idioma fuente (encoder)
   con las del objetivo (decoder).

2. *Maximum Path Length*: Longitud de la trayectoria entre tokens para
   capturar dependencias de largo alcance. En *self-attention*, el
   camino entre cualquier par de tokens es 1 (completa conectividad),
   mientras que en RNNs es \(O(n)\). Esto facilita aprender relaciones
   distantes.

3. *Self-Attention Restricted*: Técnicas para reducir costos
   computacionales en secuencias largas. Ejemplo: *atención local*
   (ventanas de tokens cercanos) o *atención esparsa* (solo tokens
   clave). Modelos modernos (Gemini, Llama) usan optimizaciones como
   /flash attention/ o /memory caching/.

4. *Número de Parámetros*: En un Transformer, los parámetros incluyen:
   - Embeddings (\(d \times V\), con \(V\) = vocabulario).
   - Matrices \(Q, K, V\) en cada cabeza de atención (\(d \times
     d_k\)).
   - Capas Feed-Forward (FFN: \(d \times 4d\) y \(4d \times d\)).
   Ejemplo: GPT-1 (~117M parámetros) con 12 capas, \(d=768\), y 12
   cabezas.

5. *Self-Supervised Learning*: Aprendizaje donde las etiquetas se
   generan automáticamente del dataset (ej: predecir el siguiente
   token). *No es semi-supervisado* (este mezcla datos etiquetados y
   no etiquetados).

6. *Supervised Fine-Tuning (SFT)*: Fase donde se ajusta el modelo
   pre-entrenado con datos etiquetados específicos (ej: pares de
   pregunta-respuesta).

7. *Reinforcement Learning from Human Feedback (RLHF)*:
   - *Paso 1*: Humanos rankean respuestas del modelo.
   - *Paso 2*: Se entrena un *modelo de recompensa* para predecir
     preferencias humanas.
   - *Paso 3*: Se optimiza el modelo con RL (ej: PPO) usando el modelo
     de recompensa como guía.

---

*** *Resumen de Notas (Corregido y Ampliado)*  
**** *Arquitectura del Transformer*  
El Transformer usa *multi-head attention* para procesar
secuencias. Cada cabeza aplica transformaciones lineales a \(Q, K, V\)
(parámetros aprendibles), permitiendo capturar distintos tipos de
dependencias. La salida de las cabezas se concatena y pasa por una
capa lineal.
- *Feed-Forward Network (FFN)*: Dos capas lineales con ReLU (\(d
  \rightarrow 4d \rightarrow d\)).
- *Encoder vs. Decoder*:
  - /Encoder/: Procesa toda la secuencia de entrada simultáneamente
    (bidireccional).
  - /Decoder/: Genera tokens autoregresivamente (uno a la vez), usando
    /masked self-attention/ para evitar "ver" tokens futuros.

**** *Optimización y Desafíos*  
- *Complejidad Computacional*: La self-attention tiene complejidad
  \(O(n^2d)\), limitando su uso en secuencias largas. Soluciones:
  atención por ventanas o caching de claves/valores (ej: KV-caching en
  GPT).
- *Longitud de Secuencia*: Modelos modernos (Gemini) manejan millones
  de tokens usando técnicas como /sparse attention/ o /hierarchical
  processing/.

**** *Entrenamiento de GPT*  
1. *Pre-entrenamiento*:
   - *Self-Supervised*: Predicción del siguiente token en corpus no
     etiquetado (ej: ventana de contexto de 512 tokens en GPT-1, *no
     k=4*).
   - Función de pérdida: Cross-entropy entre la salida (distribución
     de probabilidad sobre tokens) y el token objetivo.

2. *Supervised Fine-Tuning (SFT)*:
   - Se ajusta el modelo con datos etiquetados (ej: pares de
     instrucción-respuesta).
   - Ejemplo: Entrada = "Escribe una frase romántica en 2 palabras",
     Salida = "Eres genial".

3. *RLHF*:
   - *Modelo de Recompensa*: Clasifica respuestas (ej: 1 = buena, 0 =
     mala) o compara pares (ej: respuesta A > B).
   - *Alineamiento*: El modelo ajusta sus salidas para maximizar la
     recompensa aprendida, mitigando sesgos o toxicidad.

**** *Generación de Texto*  
- *Autoregresión*: El modelo genera tokens secuencialmente, usando la
  salida anterior como entrada.
- *Muestreo*:
  - /Greedy/: Token con mayor probabilidad (poco creativo).
  - /Temperatura/: Suaviza la distribución para mayor diversidad.
  - /Top-k/p/: Muestrea de los \(k\) tokens más probables o acumula
    probabilidad hasta \(p\).
- *Finalización*: La generación termina al producir un token especial
  (ej: =<eos>=).

**** *Correcciones Importantes*  
1. *Pre-entrenamiento no es semi-supervisado*: Es *self-supervised*
   (etiquetas generadas internamente).
2. *Modelo de Recompensa en RLHF*: No clasifica en 0/1, sino que
   asigna un score continuo (ej: 0.7 indica calidad media).
3. *KV-Caching*: Durante la generación, se almacenan claves/valores de
   tokens previos para evitar recalcularlos, reduciendo costos.

Este resumen integra los conceptos clave, corrige errores, y
contextualiza el funcionamiento de modelos como GPT. ¡Buen estudio!

* 2025-05-13 Improving Language Understanding by generative pretraining
** Questions and keywords
- cross entropy ::
- como funciona una red neuronal en aprendizaje no supervisado ::
- W_e: embeddings ::
-  :: 
** Notes
- es el paper del gpt1
- usa SGD
- utiliza el transformer decoder
- $C_{bin} = \sum_{i=1}^N(-y_ilog(p_i)-(1-y_i)log(1-p_i))$
- al ser binario significa que los dos terminos no aparecen
  simultaneamente ya que $y \in [0,1]$
- $C_{mul} = -\sum_{i=1}^N \mathbb{I}[y_i=j]log(P[y_i=j|X_i])$
- $\mathbb{I} es la funcion indicador
- en el gpt se trata de hacer maximizacion
- el crossentropy es un problema de minimizacion
- la ecaucion del gpt no tiene funcion indicador
- el modelo es una probabilidad conjunnta que viene con el producto de probabilidades
- $max_{\theta}[p(u_1,\dots, u_n) = \pi^nP_{\theta}(u_i|u1,u2,\dots, u_{i-1})]$: funcion de
  verosimilitud que da la probabilidad conjunta
- lo que se desea es que si tengo una secuencia d etokens que
  pertenece al lenguaje se busca que la distribucion de probabilidad
  sea alta. el valor de probabilidad debe ser alto lo que significa maximizar
- busco el $\theta$ que maximice la verosimilidad
- cuando tengo un problema de optimizacion o de maximizacion sobre una
  producto lo que es complicado
- funciones asintotaticamente crecientes
- la idea es transformar multipliciones en sumas
- al poner el logaritmo en ambos casos, la red neuronal no va a
  cambiar y se obtiene la ecuacion 1 del GPT porque los productos se
  transforman en sumas
- la mutliplicacion de todas las probabilidades condicionales de una secuencia
- desde el token 2 tengo tokens previos
- peroo la ecuacion 1 del paper tiene una ventana k
- lograr un suficiente contexto sin salirme del rendimiento
- tiene sentido la ecuacion 1 del paper del gpt?
- el profesor hace un grafico que tiene una black box que es el LLM y
  que recibe de entrada Hola ¿Cómo
- las salidas son P(a|hola ¿como) = 0
- P(estás|hola ¿como) = 0.55
- P(te|hola ¿como) = 0.45
- P(zapato|hola ¿como) = 0.01
- de toda la distribuion condicional que tengo solo tomo la
  probabilidad del token siguiente y no dee todos los demas
- es una logica interesante que es diferente de aprendizaje supervisado
- solo tomo la porbabilidad que me da el modelo de lemguaje sobre el
  siguiente token
- se dibuja dos boxes una de LLM y le sigue una de FC+Softmax
- la entrada a la LLM son los embedings de cada una d elas palabras
  Hola ¿cómo
- el sguendo bloque FC+Softwmax toma el $h_l^m$ que seria el ultimo
  embedding como entrada de la capa FC+softmax y ahi obtiene un W_y
- U puede ser un 1xk que multiplica por la matrix We
- cuanto tiene que dar el h0 que es el token inicial?
- no entiendo porque en el final de la ecuacion 2 se descarta el
  contenido se toma la salida de la ultima capa de la red para ir a la softmax
- logsig???? en que se usa esta funcion o es la sigmoide?
- en este caso lo adecuado era la mexcla pre-entrenado y el fine tuning
- tambien ajustan el modelo de lemguaje cuando hacne el fine tuning
- aqui el profesor pone una preguta de que no queda claro como hacen
  todo es entrenamiento
- la ecuacion 2 dice que todas esas operaciones se hacen par atodos
  los tokens [|v| n]
- hace rlas operaciones de 2 por cada token de entrada y el masking
  permite obtener de una sola pasada el resultado para no tner que
  estar haciendo a cada rato el procesamiento
- hay detalles escondidos.
- leer el modelo GPT1 pre entrenado
- video de LLM a partir de caracteres andrew karpathy build gpt from
  scratch in code, spelled out
- el gpt1 esta usando el fine tuning no un suerpvised fine tuning y no
  tiene por tanto la parte de chat sino un modelo de lenguaje que
  entiende la tarea
- falta la figura del asistente que aparece en GPT2 y GPT3.
** Summary
* 2025-05-14 GPT Transformers Parametros de Configuracion
** Questions and keywords
- rumores cierre de tensorflow? :: There are no official announcements
  about TensorFlow being discontinued. TensorFlow remains one of the
  most widely used deep learning frameworks, though PyTorch has gained
  significant popularity in research. Google continues to support
  TensorFlow, with updates and new features being released.
- problemas en tensorflow al hacer tarea simple como predecir una
  imagen :: TensorFlow is a powerful framework, but beginners may
  encounter issues such as:
  - **Shape mismatches** (input dimensions not matching model
    expectations).
  - **Preprocessing errors** (incorrect normalization or resizing).
  - **Model architecture issues** (wrong layer configurations).
  - **Hardware/GPU compatibility problems**.
- consultar la softmax con temperatura :: The **softmax with temperature** modifies the standard softmax function to control the entropy (randomness) of the output distribution. The formula is:

\[
\text{Softmax}(z_i, T) = \frac{e^{z_i / T}}{\sum_{j} e^{z_j / T}}
\]

- **High temperature (T > 1)**: Flattens the distribution, increasing entropy (more randomness).
- **Low temperature (T < 1)**: Sharpens the distribution, favoring high-probability outputs.
- **T = 1**: Standard softmax.
- el valor de 0.7 parece heuristicamente el mejor. un valor mas alto
  generaria una mayor alucinación. 
- few shot learning ::
- zero shot learning ::
- chain of thoght ::
** Notes
- GPT2 calculado desde 1.5*4 da las 6 gigas
- GPT base tiene 117*4 da los 479Megas
- la funcion softmax con temperatura añade un cociente para dividir
  los valores. la idea es relacionar entropia y temperatura como en fisica.
- el hiperparametro controla que tan alto tiene la entropia
- alta temperatura hara que tenga alta entropiaa
- una baja temperatura va a dismiuir la entropia
- a nivel probabilistico cuando tengo una alta entropia: la
  distribucion uniforme es la que da la maxima entropia
- una alta temperatura va a destruir la gaussiana y va hacer que todos
  los tokens tengan la mmisma probabilidad
- una baja temperatura hara que eventos que sean mas probables sean
  mas probables o y los que sean mas improbables se vayan a 0
- una temperatura alta hace que nuestro softmax se altere borrando l
  ainformacion y devolviendo la distribucion uniforme
- baja temperatura produce un efecto contrario
- se usa en montecarlo research el muestreo con temperatura
- con una temperatura muy baja el modelo seria deterministico
- Se hace un experimentousando el AI SDK y al setear la temperatura en
  un valor de 2 el modelo escribe cualquier cosa. La locura y la
  creatividad tiene algo de relación
- tomar en cuenta que en la ecuacion el T=1 no se usa
- top p es un umbral de probabilidad que permite ordenar los tokens y
  se toman los tokens cuya suma igual al valor de P
- por defecto esta en 1 y considera que todos los tokens son candidatos
- un top P bajo con un valor de temperatura alto, el modelo deberia
  ser altampente repetitivo
- el top k es un rankeo y selecciona los k palabras con probabilidad
  mas alta
- gpt2 usa espacio en blanco mas token
- gpt1 usa token mas espacio en blanco y si hay varios espacios toma
  uno solo 1
- en few shot learning se da unos ejemplos de lo que se espera y luego
  se le solicita al modelo que de una respuesta
- el experimento del colab en su parte 6 es el punto de partida de
  formar un asistente ya que no se esta haciendo aprendizaje de
  maquina de manera tradicional con una tupla de ejemplos con sus etiquetas
- se observa de donde nace el tema de chain of thought
- lo interesante es que con few short learning el modelo empieza a dar
  resultados bastantes congruentes e interesantes
- se realiza el print del modelo gpt2 y se observa que el tama;o de la
  secuencia es 1024 y el tamano del embedding es 1600
** Summary
*** 3. Softmax with Temperature
The **softmax with temperature** modifies the standard softmax function to control the entropy (randomness) of the output distribution. The formula is:

\[
\text{Softmax}(z_i, T) = \frac{e^{z_i / T}}{\sum_{j} e^{z_j / T}}
\]

- **High temperature (T > 1)**: Flattens the distribution, increasing entropy (more randomness).
- **Low temperature (T < 1)**: Sharpens the distribution, favoring high-probability outputs.
- **T = 1**: Standard softmax.

**Example in NLP**:  
- **Low T (e.g., 0.1)**: Model confidently picks the most likely next word.  
- **High T (e.g., 2.0)**: Model generates more diverse (but potentially nonsensical) text.
- *Example* (NLP):
     - \( T = 0.1 \): Model outputs "The cat sat" (deterministic).
     - \( T = 2.0 \): Model outputs "The jazz pineapple danced" (creative).
*** 4. Why is 0.7 Heuristically the Best Temperature?
Empirical studies suggest:
- **T ≈ 0.7** balances creativity and coherence.
- **Higher T (e.g., 1.0+)** increases "hallucinations" (nonsensical outputs).
- **Lower T (e.g., 0.1)** makes outputs deterministic and repetitive.
- Optimal \( T \approx 0.7 \) balances creativity/coherence.
- \( T > 1 \) risks hallucinations; \( T \ll 1 \) causes repetition.
*** 5. Few-Shot Learning
- **Definition**: The model is given a few examples (shots) of a task before making predictions.
- **Example**:  
  Prompt:  
  ```
  Translate English to French:  
  "Hello" → "Bonjour"  
  "Goodbye" → "Au revoir"  
  "Thank you" → ?
  ```
  Model predicts: "Merci".

*** 6. Zero-Shot Learning
- **Definition**: The model performs a task without any explicit examples.
- **Example**:  
  Prompt:  
  `"Translate 'Hello' to French."`  
  Model outputs: `"Bonjour"`.

*** 7. Chain of Thought (CoT)
- **Definition**: The model breaks down reasoning step-by-step before giving an answer.
- **Example**:  
  Prompt:  
  ```
  Q: If Alice has 3 apples and gives Bob 1, how many does she have left?  
  A: Alice started with 3 apples. After giving 1 to Bob, she has 3 - 1 = 2 apples left.
  ```
** Corrections and Notes Summary

*** Model Sizes
- **GPT-2**:  
  - Parameters: ~1.5B → Size ≈ 1.5B * 4 bytes ≈ **6 GB** (correct).  
- **GPT-1 Base**:  
  - Parameters: 117M → Size ≈ 117M * 4 bytes ≈ **468 MB** (noted as 479MB, minor rounding difference).

*** Softmax with Temperature
- Corrected: "alta entropiaa" → "alta entropía".  
- Key Points:  
  - **High T** → Uniform distribution (max entropy).  
  - **Low T** → Peaked distribution (low entropy).  
  - Used in **Monte Carlo sampling** and **LLM text generation**.

*** Top-p (Nucleus Sampling) and Top-k
- **Top-p**: Selects the smallest set of tokens whose cumulative probability ≥ *p*.  
  - *p = 1* → All tokens considered.  
  - *p = 0.9* → Only the most likely tokens covering 90% probability mass.  
- **Top-k**: Selects the *k* highest-probability tokens.  
- **Interaction**:  
  - High *T* + low *top-p* → Repetitive but random outputs.  
  - Low *T* + high *top-p* → Deterministic and coherent.

*** Tokenization in GPT-1 vs. GPT-2
- **GPT-1**: Merges multiple spaces into one.  
- **GPT-2**: Treats spaces as separate tokens.  

*** Few-Shot Learning and Chain of Thought
- **Few-shot** enables models to generalize from minimal examples.  
- **CoT** emerges when models explain reasoning (e.g., "Let’s think step by step...").  

*** GPT-2 Architecture Notes
- **Sequence length**: 1024 tokens.  
- **Embedding size**: 1600 (likely a typo; GPT-2 Large uses 1600, but base is 768).  

---

*** Final Takeaways**
- **Temperature** controls output diversity (high = creative, low = stable).  
- **Few/Zero-shot learning** reduces the need for fine-tuning.  
- **Chain of Thought** improves interpretability.  
- **TensorFlow** remains active despite competition from PyTorch.  

* 2025-05-22 Revision de avances de trabajo
** Recomendaciones
- aterrizar la idea
- estamos al 25% del semestre
- el congreso sirve por colocar un deadline
- el congreso de Springer es "prostituido" se puede conseguir algo de
  mejor calidad
- puede ser adecuado separar la idea
*** Sobre el modelo
- limitar a un solo dataset
- probar a futuro con mas datasets
- el tamaño del modelo debe ser acorde al dataset
- se supone que los pesos pre entrenados de imagenes naturales en
  segmentacion no deberían ser utiles
- comparar la misma arquitectura emepzando con pesos aleatorios con
  los pesos pre entrenados
- definir la idea y escribir la metodología porque conforme se sigue
  leyendo se va a querer seguir probando mas y mas cosas sin aterrizar
*** Preguntas que se pueden hacer al profesor
- Sirve data augmentation en transformers? Pienso que dado que un ViT
  usa patches y los pregunta entre si no sé si sirva tener imagenes rotadas
- Sobre repetir las imagenes parece que se ve como inevitable para una
  red pre entrenada. aqui podria ser el tema de usar scratch. Si es adecuado.
- hay que ver el ranking de la conferencia ejemplo A*
- hay que realizar la SLR para el tema de lo de marco. esto seria lo primero.
- antes de ponerse a trabajar terminar la revision de la literatura y congelar
- presentar la idea preliminar
- continuar trabajando la idea para llegar a una publicacion de revista
- la participacion en el congreso puede ser significativa en cuanto al
  feed back que se obtenga del congreso considerando que hagan un
  trabjo serio.
- no es significativo académicamente el congreso. Tener presente el
  tema de los rankings
*** Esquema para evaluar el trabajo
- cual es la idea:
- como va a evaluar la idea
- que algoritmo usar para probar
- que se ha encontrado en revision de literatura
- como medir que Modelo se desempeña mejor. Aqui tendria que venir la estadistica.
- con qué datos va a entrenar?
- el 25% de trabajo es la idea y la revision de la literatura. el otro
  25% seria la metodologia el 25% es entrenar y validar y el otro 25%
  seria evaluar resultados.
- no quiero que pase que hay algo que se esta proponiendo y que ya
  esta hecho... la idea de la slr para que el probelam sea problema y
  adapten la solcion el problema no se desea adaptar o forzar el problema
- al ser un congreso especifico. tambien considerar el nivel.
- para que le sirva para el Doctorado.
- identificar el problema y proponer algo que valga la pena.
** About procedings
where can I find information about rankings of procedings 

To find information about rankings of proceedings (conference proceedings), consider these resources:

1. **CORE Conference Rankings** (http://www.core.edu.au/conference-portal):
   - Widely used for computer science and related fields.
   - Conferences are graded (A*, A, B, C).

2. **Google Scholar Metrics** (https://scholar.google.com/citations?view_op=top_venues):
   - Lists top publications (journals & conferences) by h5-index.

3. **Scimago Journal & Country Rank (SJR)** (https://www.scimagojr.com):
   - Offers rankings for journals and some conference proceedings.

4. **Conference Portal (Microsoft Academic; archive)**:
   - While deprecated, some rankings are still referenced online.

5. **Institutional or Departmental Lists**:
   - Many universities maintain their own lists of recommended/ranked proceedings for specific disciplines.

6. **DBLP** (https://dblp.org):
   - Useful for identifying major conferences, though doesn't rank them directly.

Choose the resource based on your field, as rankings can vary widely
by subject.

* 2025-05-27
** Questions and keywords
- buscar como funciona positional embedding
** Notes
- GPT2 tiene 1.6 billones de parametros.
- GPT2 el tamaño del archivo es 6.6
*** Supervised Fine Tuning
- V es el vocabulario
- en el 2.2 se describe la matriz de embedings 2-1 del gpt-2. se
  analiz ala dimensiones para un ejemplo.
- los positional encodeing son calculados previos
- para ingresar secuencias mas grandes que el tamaño maximo 1024 de gpt2
- porque 1024?
- el vector de positional encodings toma cada una de las posiciones y
  en funcioni de eso añade el vector de embeddings
- dropout de 0.1 usa como probabilidad para eliminar componentes. la
  division por 1-p permite escalar . la idea es que el promedio antes
  y despues del dropout sea el mismo
- en layer normalization se normaliza a lo largo de las componetes del vector
- cada una de las W de Q, K y V se calculan cocatenada
- el vector de bias tambien es concatenado
- el layer normalizado no le cambia al vector es una matriz
- para sacar el resultado del bloque de atencion se les separa luego
  por eso 1600x3 da 4800
- el calculo que falta es el resultado del bloque de atencion
- no es necesario hacer padding par acompletar secuecnias. parece que
  en los transformers es independiente del tamaño de la secuencia
- attnout solo pone el resultado despues de calcular el bloque de selfattention
- se debe tomar enc uenca que ingresa un residual
- se supone que si los positional econdings no esetuvieran ya
  guardados se podria manejar el transformer para cualquier tamaño
- cada cabezal calcula un vector de 64 componentes 1600/64 da 25
  bloques o cabezales
- 
** Summary
* 2025-05-28 GPT Supervised Fine Tuning
** Questions and keywords
- revisar lo que es la mascara causal
- como se relaciona ocn el unsupervised
** Notes
- se previene la division por cero
- la matriz de mascara causal es una matriz triangular cuyo objetivo
  es evitar que GPT sea un BERT
- el valor de -infinito se tiene que obsrevar que al ingresar a la
  softmax nos da $e^{-\infty}\to 0$
- Los prompts seran X
- Las respuetas deseadas y
- superindice es el numero de ejemplos
- el subindice es la posicion de la secuencia
- para hacer el supervised FT a la entrada del modelo ingresa el
  prompt y la respuesta
- en este contexto al trabajar por batch se debe tener el mismo tama;o
  esto obliga a emliminar un token si suponemos que el numero de
  palabras mas simbolos entre prompt y respuesta tiene un valor
  fijo. se suele eliminar el primer tokem
- por ejemplo si faltan cuatro tokens para completar la secuencia se
  rellenan con tokens especiales <end>
- en el SFT no interesa la secuencia del prompt:
  - Hola como estas? Estoy muy bien como estas tu?
    L=-log[P_\theta(Estoy|Hola Como estas?)]-...-log[P(?|Hola como
    estas? Estoy muy bien Como estas tu?)]
  - sobre que esenario puedo evaluar al modelo como asistente:
    literatura, programa, conocimiento
- uno de los problemas es que a ningun modelo se le ha entrenado a que
  diga no sé. no dice no se cuando no puede.
- es diferente que el caso etico que le han restringido para las
  barreras de seguridad
- hasta aqui el GPT ha entendido el lenguaje con el pre-trainig
- el SFT lo convierte como asistente para que aprenda sobre disciplinas
- estadisticamente no hay sufciente datos para decir que generaliza
- en el caso del gpt si bien la cantidad de datos es alta, no son
  estadisticamente descriptivos de lo que se quiere hacer con el modelo.
- la cantidad de datos de SFT se estima como un 3% del total de datos
  que se usa para pre-entrenamiento
- Deepseek no usa SFT usan RLHF
- buscar en google chat gpt pipeline training
- quizas el problema de alucinaciones del modelo es un problema
  estadistico cuantos datos necesito para tener una muestra suficiente
  para que sea esetadisticamente representativo
- [[https://openai.com/index/chatgpt/][enlace al pipeline de entrenamiento de chat gpt 4]]
*** Step 2 del trainig del gpt collect comparison data and train a reward model
- eliminar la ultima capa lineal y el softmax y tomar los vectores
  transformados de los tokens de entrada
- el modelo de recompensa se entrena con datos X y y
- nos quedams con los embeddings transformados
- el modelo de recompensas toma la ultima salida h_T que devuelve un
  escalar y los h de 1 a T-1 se descartan
- porque se toma el ultimo
- no se toma el promedio de todos porque es un paso mas de calcular
- el token a predeci esta embebido la informacion anterior
- esto es porque para calcular el h_T embedding tuvo que calcularse
  todo lo anterior
- No es markoviano, para markoviano el WE_T seria suficiente para
  predecir la salida del sistema
*** Entrenamiento del modelo de recompensa
- se necesita un ejercito de anotadores que dado un prompt X tiene
  respuestas 1,2,...4
- los anotadores leen las respuestas y van rankeando la mejor respuesta
- El papel del modelo de recompensas es hacer lo que los humanos hacen
  pero no dan un score sino que ellos ordenan de mejor a pear
- el modelo tiene que ser entrenado tal que si la respuesta a es mejor
  que b, entonces la recomepensa con a sea mayor a la de b
- $y^{<a>}\ge y^{<b>} \to R(X,y^{<a>}) \ge R(X,y^{<a>})$
- La ecuacion de optimizacion del modelo de bradley-Terry guarda
  relacion en como pasar preferencias a scores. es decir si algo es
  mejor que algo se puede cuantificar de con la logsig de las recompensas
- bradley-terry pasa preferencias a scores
- colocar la ecuacion del cuaderno
- el objetivo es minimizar
- porque se hace de la resta y de la funcion sigmoide
- como en el ejemplo de clase hay cuatro respuestas a rankear se suma
  las comparaciones de todos los pares,
- sin embargo se hace un numero comparaciones como la combinacion de 4
  en 2
- sumo todas las combinaciones y divido para la combinacion de 4 en 2
- la notacion es complicada
- no entiendo para que esta ultima etapa
- La ecuacion a minimizar es el modelo de recompensas
- porque usan la funcion sigmoide. porque no hay opcion de empates se
  escoge la una opcion o la otra.
- el simbolo no es el mayor que sino uno parecido
- por esta razon se relaciona con la logsig
- hay otros procedimientos de RLHF añadiendo mas capas
*** Proximal Policy Optimization PPO
- porque se llama RLHF?
- el termino a partir de beta busca que no cambie tanto
- la divergencia KL mide que tan diferentes son dos distribuciones de probabilidad
- el beta es un coeficiente de penalizacion
- se busca que la respuesta sea buena y que las distribuciones no
  cambie tanto
- la idea de esto es evitar el catstrophy forgetting
- porque usa una funcion de recompensa
- el modelo de referencia deberia ser suficientemente bueno para que
  tenga efectividad aplicar
- el porque es esperanza
- en datos y computacional es dificil
- que es clipping y de que van las variantes del PPO
- deep seek usa GPPO
- hay problema cuando se hace un modelo de optimizacion con
  restricciones porque no puede usarse descenso del gradietne
- El segundo termino evita que se olvide el modelo lo que ya sabia
- entre los problemas de optimizacion asoma divergencia KL, cross
  entropy,
- deep seek
- diferenciacion automatica
** Summary
* 2025-06-03 Proximal Policy Optimization
** Questions and keywords
- surrogate :: heredada
- off policy ::
- on policy ::
- advantage function ::
- $\tau$ :: es una trayectoria $\tau =
  s_0,a_0,r_1,s_1,a_1,r_2,s_2,\dots,s_{T}$ siguiendo una
  $\pi_{\theta}(a \mid s)$
- Policy Gradient Theorem ::
- estimador :: no puedo calcular exactamente un valor
- como crear en matlab una red graficamente ::
- como obtener la derivada cuando es una entrada de n y una salida logsig ::
- cual es la libreria FFN de N capas :: no habia limitantes para los
  tam;aos pero sin salirse de la arquitectura
- decaying weight ::
- revisar el calculo de la diferencia KL ::
- what is an actor-critic ::
- what is an PPO actor-critic :: 
** Notes
- el paper propone una nueva funcion objetivo que va por mini batch
- difdrencia entre aprendizaje supervisado con rl. en supervisado no
  interesa el orden en el tiempo que vienen los datos.
- en rl interesa aprender segun el tiempo
- lo desafiante es entrenar un metodo que guarda la secuencia, pero en
  minibatch se pierde esa secuencia. es decir a pesar que se pierde la
  secuencia se optimiza o aprende la politica
- el minibatch puede tener secuencias de diferentes episodios
- en rl no se habla de funcion de costo como en ANN sino de una
  funcion objetivo de ganancia
- suponga que usamos una ANN que recibe un estado y devuelve una
  distribucion de probabilidades sobre la politica dadas las acciones
  y estados
- resolver le problema de optimizacion se puede usar descenso del
  gradiente para obtener los $\theta = \underset{\theta}{argmax}J(\theta)$
- el probllema es que en la practica no se puede acceder a $R(\tau)$
  que sería la función de recompensa del ambiente.
- hay un teorema en RL que nos dice que el problema de optimizacion
  planteado es similar a resolver otro problema de optimizacion
- La expresion usa la politica cuya recompensa es conocida porque es
  la respeusta a la ANN
- todas las trayectorias se promedian
- la ANN es la pol[itica
- la r depende de la accion que la da la ann.
- los valores de recompensa son empiricos por lo que accedo a un
  estimado de la recompensa
- por tantno la ecuacion 4 en los apuntes es un estimador \(\hat{g}\)
- en aprendizaje supervisado trabajamos con el valor exacto del
  gradiente en RL se trabao con valores aproximados del gradiente
- en rl los datos cae a "cuentas gotas" tipo sparse
- las trayectorias de los estados estan condicoinados
- en aprendizaje supervisado asumimos IID: independientes e
  identicamente distribuids
- en rl en no son independientes los datos ni identicamente
  distribuidos porque cada iteracion las politicas son diferentes
- el algoritmo de retropropagacion inicialmente calculaba a mano
- como funciona la derivacion automatica actualmente en redes que usan
  tensorflow o pytorch
- diferenciacion automatica
- la derivacion automatica
- el software tiene la funcion de costo y se obtiene los valores del
  gradiente de la funcion de costo
- nunca se ha entrenado con desenso del gradiente con restriccinoes
- la expresion del paaper tiene una red neuronal actual y una anterior
  pero se sujeta a restricciones
- la restriccion tiene que entrar en la funcion para poder usa
  rdesenso de gradiente. vea la ecuacion 3 del paper de PPO
- el beta permite discriminar a que termino se le da importacia
- los terminos $\pi_{\theta}$ actual y $\pi_{\theta_{old}}$ son
  probabilidades que salen de la red neuronal. pero el old es hace
  unas iteraciones anteriores y la otra es la actual. En los LLM seria
  como tnener la salida del LLM luego del SFT y la actual seria la que
  se recibe de RLHF
- como se calcula la diferencia KL
- el clipping recorta el valor de salida ya que la expresion del
  cociente de la salida de las dos RNA si la una es 1 y la otra tiende
  a 0 se puede hacer un valor muy grande casi infinito
- value network ::
- que es PPO como actor critic
- se requeire ua tercera una red neuronal
- alta varianza bajo vias
- diferencia temporal baja varianza alto viaz
- montecarlo bias 0 varianza alta diff temporal es lo contrario
- bias variance trade off
- en una LLM operan al menos tres redes neuronales
- que es el openai gym

** Summary
* 2025-06-10 Numerical Differenciation
** Questions and keywords
- derivación manual :: obtención de expresiones algebraicas de manera manual
- algoritmo de retropropagacion del error :: es derivar a mano usando
  la regla de la cadena
- regla de la cadena ::
- diferneciacion simbolica :: es calcular mediante software
- swell expression :: hinchazon de expresion???
- diferenciacion numerica ::
- no comprendo porque diferenciacion numerica no serviria para relu o lazos ::
- machine epsilon :: 
** Notes
- revisar las librerias de las feedforward de Marco
- las derivadas estaban calculadas a mano e implementadas
- ya en redes convolucionales es complicado hacer las derivadas
- manualmente se obtiene expresiones para la diferenciacion de
  funciones. se puede observar la complejidad algebraica.
- se programa a que es igual ciertas derivadas primitivas en el software
- el problema es que no puede hacer simplificaciones.
- derivacion simbolica no usa simplificaciones y las expresiones
  crecen. generando el expression swell
- en redes neironales tratamos de resolver $\theta = \underset{\theta}{argmin}J(\theta)$
- el desenso gradiente $\theta^{(new)}=\theta^{(old)}-\alpha \nabla_{\theta}J(\theta)$
- para obtener el gradiente hay varios mediatos
  - manual: obtengo expresion cerrada. su principal problema es que es
    dificil de calcular. el calculo es exacto
  - numerico: no obtengo la expresion cerrada sino el numero. tiene
    una alta complejidad temporal y es aproximado
    - errores truncamiento
    - errores de redondeo
    - $O(n^2)$
  - simbolico: su principalmente es calculos repetitivos y expression
    swelling. se hacen los calculos porque no hay simplificacion. sin
    embargo es exacto. sirve para expresiones cerradas. no lazos.
  - automatico: es exacto en el calculo
- el calculo numerico del gradiente consisten en dibujar un eje de
  coordenadas x y f(x) y obtener la pendiente, se observa que
  obteniendo la tangente del angulo $\phi$ entre los puntos $x_0$ y
  $x_0+h$ es similar a la definicion de la derivada sin aun poner el
  limite
  $tan(\phi)=\frac{f(x_0+h)-f(x_0)}{x_0+h-x_0}$
  $\frac{df}{dx}\mid_{x_0} = lim_{h\to 0} [tan(\phi)]$
- para calcular el gradiente de f con respecto a x cuando tenemos una
  funcion de n variables evaluado en $x_0$, se tiene que perturbar una
  a una las variable sindependientes y por eso el vector es 0 en todos
  los puntos excepto en donde se calcula el gradiente
- en codigo se usa un lazo for para pasar N veces por cada uno de las
  variables calculando y tiene un orden de complejidad $O(n^2)$
- en una red neuronal las n variables es muy grande
- la complejidad espacial para el calculo numero el espacio es O(n)
- en el calculo numerico se tiene varios problemas:
  - orden cuadratico
  - errores de truncameinto
  - errores de redondeo
- la serie de taylor nos permite expandir una derivada en una serie
- el hacer la $h$ muy pequena se llega al error de redondeo porque no
  habra como representar le numero
- el error de truncamiento es de orden h que tiene que ver con el
  resto de terminos que no considero para aproximar la derivada y
  dejarla usando unos primeros terminos de la serie de Tyalor
- al subir al cuadrado h, ell numero se hace mas pequeño
- para disminuir el error de truncamiento se hace h al cuadrado
- usando diferencias centrales se puede obtener que el termino de
  error de truncamiento se acerque a O(h^2)
- Error de redondeo :: tiene que ver con machine epsilon. cuando se
  baja el h el error de redondeo sube
- en el libro el grafico tiene una v
- tipicamente los valores del epsilon son $10^{-7}$
*** Diferenciacion Automatica
- es de dos tipos:
  - hacia adelante
  - en reversa: usado en RNA
- recuerde que RNA usa optimizacion por gradientes. y de ahi el
  interes en obtener estos metodos para ayudar la diferenciacicon.
- recuerde el problema con SVMs.
** Summary
* 2025-06-11 Automatic Differentiation
** Questions and keywords
- network designer ::
- numeros duales ::
- que es una funcion trascendental :: 
** Notes
*** Automatic Differenciation Forward-Model
- considere el ejemplo en el cuaderno para obtener una derivada
  $f(x_1,x_2)$ con respecto a $x_1$ evaluado en $(x_1,x_2)=(2,5)$
- se definen variables auxiliares se definen hasta $v-0$ dependiendo
  del numero de variables
- se realiza un grafo con las operaciones necesarias hasta obtener la expresion
- las operaciones se agrupan por pares
- es forward mode porque se van derivando los terminos en el orden que
  se procesa el grafo
- hay que calcular las derivadas de cada vi puntp con respecto a la $x_j$
- lo interesante es que no tuve que aplicar regla de la cadena
- se pone las operaciones basicas
- se pone la funcion y su derivada i.e. entrada y su derivada
- el calculo del gradiente se reduce a realizar los reemplazos de los
  valroes de las derivadas en las expresiones que van quedando
- el metodo no da la expresion de la derivada pero me da el valor
  exacto de la derivada
- este no es el metodo que se usa en las librerias de RNA
- network designer nadie me salva de colocar la funcion y la derivada
  de la funcion
- hay un algoritmo contra intuitivo para programar este resultado y
  usa nnumeros duales
*** Numeros Duales
- un numero dual $\epsilon$ debe ser distinto de 0 y su cuadrado debe
  ser 0
  - $\epsilon \neq 0$
  - $\epsilon^2 = 0$
- sea la expresion $f(x) = x^2+3x+5$
- si calculamos $f(x+\epsilon) = (x+\epsilon)^2+3(x+\epsilon)+5$
- al parecer en funciones trascendentales no se sabe si esto
  funcionara para llegar a la derivada. puede que si puede que no. Sí
  vale para sin and cos.
- tiene relacion con las series de Taylor
- los numeros duales es otra forma de calcular la derivacion hacia adelante
- permite ir calculando las derivadas con reemplazos y me olvido que
  estoy derivando operamos y listo
- se obtiene el numero pero no la expresion exacta de la derivada
- si requiero la derivada como expresion se requiere el simbolic
  differentation o derivacion manual
- la serie de Taylor se observa com muy potente parecido a Serie de Fourier
*** Automatic Differentiation Reverse Mode
- Consideremos la misma expresion $f(x_1,x_2)=ln(x_1)+x_1x_2-sin(x_2)$
  obtener la derivada parcia de f con respecto a $x_1$ evaluada en 2,5
- se hace el grafo de solucion
- se debe calcular las variables complementarias $v_i = $
- se calcula en reversa orque se va desde la utima expresion hacia la
  primera es decir desde la variable complementaria
- la retropropagacion del error es un caso espcial de la automatic
  differentation en reverse mode
- si bien se aplica la regla de la cadena se facilta el calculo
- si asumimos que queremos calcular el gradiente de una funcion $J:
  \mathbb{R}^n \to \mathbb{R}^m$
- la complejidad del calculo del gradiente depende de una constante c
  ques es menor a 6 y dependen de n y m
- se prefiere el metodo de reversa porque en RNA m = 1
- este emtodo es general mientras que el backpropagation es especifico
  para RNA. recuerde que en una RNA m termina siendo un numero
** Summary
* 2025-06-18 observaciones proyecto
- es mas elegante trabajar el transformer de video
- es mas elegante trabajar el modelo hybrido usando la convolucional
  3d que generaria un feature map con informacion en 3 canales que
  podria ingresar a la red del transformador
- los problemas que se tiene es que tanto convolucionad como
  transformer interpretan la informacion de manera espacial sin
  considerar el tiempo
- problema serio forzar los canales para mandar a tener datos
- problema que se reducen los datos a nivel paciente...
- generar sintetitocs?
- acelerar para tener resultados y escribir el paper
- siguiente semana clase y resultados
* Enlaces
- Libros Generativa: https://drive.google.com/drive/folders/1fjGzPM0VnL40AQd1AVWJfZO05NXsKTya?usp=sharing
- Libros Machine Learning: https://drive.google.com/drive/folders/0B0w0jIatGZYGZHdNdW9FZ1ZycGs?resourcekey=0-6IrcgeLWNwIXoYq8lQ9e0Q&usp=sharing
- https://platform.openai.com/tokenizer
- https://jalammar.github.io/illustrated-word2vec/
- paper A survey of large language models.
- emergent abilities of large language models
- [[https://youtu.be/HX8IMpnESxk?si=JhPz7Fi-_JNPaNR7][8 cientificos inventan IA generativa]]
- [[https://ai-sdk.dev/]]
- [[https://colab.research.google.com/drive/1JitcCWOqOqdylobAbaFMlrySHrgVD58H#scrollTo=hfi0Oe0SDNFR][enlace-demo-gpt1-gpt2-colab]]
- [[https://colab.research.google.com/drive/1741crQT9xjA3vZIblL50MNYtHsYUqdBX?usp=sharing][codigo original Marco Benalcazar colab]]
- buscar google layer normalization explained
* TODO [31%]
- [ ] Revisar EDOs y EDPs derivadas ordinarias y derivadas parciales
- [ ] revisar el algoritmo de retro-propagacion del error
- [ ] Principios y Fundamentos
- [ ] Definir la idea con respecto a mi tema de investigacion y el
  PEA: hacer pruebas y experimentacion. Hacer ajustes y trabajo Final.
- [X] despues de la siguiente semana contar la idea que se ha hecho
  proponer la idea y esta idea contiene tales capitulos del temario
  que vamos a ver. necesito saber que problema y que alternativas de
  solucion van a probar esto seria 28 de abril. es sacar el articulo
  con algo de la materia. que seria el bagage teorico.
- [ ] localizar transformers en los libros de Marco
- [X] Presentacion concreta de 20 minutos mas 10 minutos para
  preguntas. Presentar la idea
- [X] Pensar en dond publicar
- [ ] Presentar la idea mas definida mas concreta
- [ ] Leer paper Improving language with generative el paper del GPT
- [X] Siguiente semana cambio de horario a jueves y viernes
- [X] Martes pasa el jueves de 2 a 4
- [ ] Escribir la idea que voy a desarrollar. Considerar el congreso
- [ ] Hacer una revisión de la literatura para el tema
- [ ] Iniciar fase de experimentación.
- [ ] Mostrar el miércoles 4 de junio resultados preliminares 15
  minutos cada uno el miércoles.
