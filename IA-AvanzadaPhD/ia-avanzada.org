
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Inteligencia Artificial Generativa
#+date: 2024-10-02
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment
* 2025-04-16 Notas iniciales PEA
** Questions and keywords
- retropropagacion del gradiente
- rmsprop, adam
- test time compute y razonamiento
- algoritmos avanzados para RL
- scientific machine learning :: inyecta conocimiento a priori par
  aque no sea solo a traves de datos. una RNN mapea o construye
  funciones en espacio de caracteristicas las RN Fisicas mapean en un
  espacio de funciones.
- computacion cuantica e IA
- Quedan tres años
** Notes
- Principios y Fundamentos
- No hay primer semestre o segundo semestre
- prsentar un articulo que tenga nivel de congreso tipo B o superior
- tiene que ser en el  tema qeu etstamos haciendo
- articulo ha de ser desarrollo de una idea en el trabajo de
  investigacion
- marco proporciona la metodologia....
- hay que ver el grado de avance....
- caṕ1 y cap2 usar en el tema de investigacion....
- no articulo de juguete.....
*** Libros
- Deep learning bases y conceptos: empieza con el capitulo 7
- checar capitulo 12del libro de bases tiene que ver con transformers
- pagina 395 tiene transformers de vision
- understanding deep learning de smunce prince
- murphy advanced probabilistic machine learning 847 del pdf
  - fundamentos actualizados
  - topicos avanzados
- el enfoque de la asignatura es investigacion basica
** Summary
* 2025-04-22 Capitulo 1 Modelos Generativos de Lenguaje
** Questions and keywords
- Procesamiento de Lenguaje natural
- Modelos multimodal :: usan datos tanto lenguaje y visuales
- lenguajes formales :: mas faciles de procesar
- lenguaje natural :: es dificil de procesar por la maquina
- funcion de probabilidad :: 
- modelo de lenguaje ::
- FF :: red feedforward 
- LSTM :: funciono parcialmente
- token ::
- byte pair encoding ::
- word2vec ::
- de cuanto es la longitud de los embedings
- de cuanto es el tamaño del tokenizador
- influyen los pesos de los ids de los toekens
** Notes
- El boom de gpt es simular comprender el lenguaje
- nlp es complejo y es punto de partida de transformers y mecanismos
  de atención
- los lenguajes formales y artificiales son mas faciles de maniobrar y
  manipular pero no son conocidos o practicos para transmitir mensajes
- estamos dentro dle campo de la percepcion computacional
*** modelo de lenguajes
- todo algorimot de machine learning es una funcion matematica
- un modelo de lenguaje es una funcion que tiene como entrada lenguaje
  y la salida puede ser numeros o lenguaje
- es una funcion matematica
- el lenguaje es ambiguo y por esta razon es preferible usar una
  funcion de probabilidad que una determinstica
- la funcion de probailidad me permite calcular la probabilidad de la
  secuencia de tokens o palabras e.g. el modelo esta feliz
- una distribucion d eprobabilida dsirve para calcular probabilidades
- la distribucion d eprobabilidad sirve para sampling
- los modelos generativos samplean a partir de la funcion de probabilidad
- distribucion discretas sobre todo se usan
- caundo uno no sabe que funcion de distribucion probabilidad usar
  entoence recurro a usar una red neuronal. Ejemplo una FF con una softmax
- uno de los problemas importantes en lenguaje es que las cadenas de
  texto pueden llegar a ser secuencias largas. una palabra que esta
  cientos de pasos atras puede cambiar el signficado de texto
- feed forward no sirvieron para muchos casos
- RNN: LSTM, GRU, Gated
- LSTM era el estado del art hasta 10 años atras
- luego surje LSTM + mecanismo de atención
- finalmente llegan los transformers
- si en el ejemplo las cadenas de texto si no tienen relacion tiene
  que dar probabilidades baja y si tiene relacion tiene probabilidad alta
- entonces la probabilidad condicional refleja como funciona el lenguaje.
- modelos de lenguaje son distribuciones de probabilidad y estas
  distribuciones se representan con RNA
- porque no se usa tokens y no palabras
- tokenization es como un diccionario
- lo que se termina aprendiendo es el vector que representa a cada embediing
- como generamos los tokens?
- un token tiene en español e ingles una equiv de 3/4 de palabra
- hay un punto que en funcion  de las iteraciones que alcanzamos un
  maximo de tokens entonces hay que llegar a que no sean palabras
  individuales ni tampoco llegar a un punto atomizante tanto que no
  hay signficado
- se tomo la idea fusionar bits para formar cadenas para poder llegar
  a la formacion de los tokens. la termiinacion es algo arbitraria
- gpt1 tokenizador es distinto al gpt2
- otra cosa que no es evidente es que un computador tiene que procesar
  numeros. necesito convertir las cadenas en texto en numeros
- consideremos dos palabras que pueden estar juntas en el diccionario
  pero que peuden tener significados distintos: diablo dios. por
  ejemplo si ordeno alfabeticamente no refejaria que esos bojetos son
  como contrarios. por esta razon es que necesito cada palabra se
  represente con un vector de numeros.
- como se hace la codificacion en vectores
- crear un embeding de una imagen y analizar que partes del vector
  tenia que ver con edad joven risa
- modelo de incrustacion correspondiente para cada token. Para el
  modelo del lenguaje ingresa una cadena de vectores
- paper recomendado : a survey of large language models  en arxiv la
  figura 7 es muy interesante.
  1. tomar scrapping d etexto
  2. filtrado y seleccion
  3. dedupicacion es decir quitar repeticiones
  4. privacy reduction quitar identificadores personales
  5. tokenizar 
** Summary

* 2025-04-23 
** Questions and keywords
- modo agente en LLM ::
- pesos sinapticos ::
- hay dudas sobre el costo de deepseek de 6 millones ::
- checkar costo ofertas laborales de openai meta y otras empresas grandes ::
- proyecto starlink ::
- testime compute :: parece que es una manera de obtener mas de lo
  poco que queda (limones)
- FLOPS :: cuantas operaciones en punto flotante se puede hacer con
  single y double precision hay formatos en 16 en machine learning
  para reducir entrenamiento.
- MAUs ::  numeor de nuevos usuarios agregandos por mes
** Notes
- forma de representar palabras como vectores
- los embeddings en esencia aplicar transformacines no lineales a los vectores
- los modelos d elenguaje no pueden crear nuevas cosas.... basicamente
  simulan capacidad de comprension y transforman vectores
- fig 6 del paper de survey of LLM se leen com giga tokens
- la fig 6 muestra el desafio de la actuallidad de los datos con los
  que aprende es decir que el conocimiento no este actualizado. por
  costo computacional no puede analizar toda la data de internet. un
  problema es como obtener datos actualizados
- para que los modelos tengan un conocimiento mas exacto usar los
  repositorios propios de IEEE, Springer ACM
- los modelos no pueden crear nada nuevo con respecto a lo que ya existe.
- pueden ser vistos como unos meros compresores de datos.
- T5 es bastante antiguo
- gpt3 es 2021 a 2022
- otro problema que se tiene qes que la informacion disponible
  completa es sobre los modelos abiertos y un poco antiguos. no hay
  una LLM anclada a la universidad.
- la barrera estaba en leer papers.
- la barrera esta en que no hay papers
- hay tres ejes en LLM: datos, hardware y conocimiento
  - hardware: decenas de miles de gpus
  - conocimiento avanzado: personas altamente especializadas. algo en
    las universidades pero mas en las industrias.
  - datos: 100tos de terabytes tokens
- los tres ejes requieren bastante inversion
- 500 k USD por a;o a personas que saben del tema
*** leyes de escalado de LLM
- LLama 3 cuesta unos 80 millones de dolares.
- presenta la ecuacion 2 para cualcular la funcion de perdida dado el
  tamaño del modelo y la cantidad de datos . los coefcientes A,B \alpha
- mientras mas grande sea mmodelo y mas datos la funcion de perdida es menor
- la generacion de datos sinteticos no es muy efectiva a la hora de
  aportar nueva informacion
- cuanto puede el modelo generar de informacion y lo ideal es que
  genere mas bits de informacion que los bits de informacion que le llega
- no se absorbe conocimiento desde lenguaje
- no se cree que los LLMs sean los unicos detras de una IAGeneral
- resulta que recibimos mas datos que una LLM si observamos desde otra perspectiva
- una persona recibe 500x500 pixel y
- la cantidad de informacion de una persona recibe hasta 25 años
  obtiene mas menos 6 Peta Bytes.
- se recicla algo del conocimiento de modelos previos. no se parte d
  elos pesos aleatorios
- En la formula es costo computacional $C \approx 6ND$ con N tamaño
  del modelo y D el tamaño de los datos (tokens)
- costo de entrenamiento de SOTA LLaMA 3 usa 15.6 TeraTokens. en bytes
  es mas
- hay que revisar en que se refiere los billones y trillones en ingles
  y español
- en lama 3 se usa 16 mil tarjetas h100.  esta tarjeta cuesta mas o
  menos 30 mil dolares.
- tema de gpus ver enlace GPU  nvidia h100 specs
  H100
- la compu del labo endra 48GB nvidia a6000 workstation ya hay un par
  instalado 48GB
- el entrenamiento de una LLM puede tomar 70 dias
- costos viene del costo de alquilar el datacenter mas los
  salarios. se considera como 2 dolares por hora de uso d euna gpu H100.
- mas menos asumiendo el costo de entrenar  va a 75 millones
- toneladas equivalentes de Co2
- parece mucho el gasto de energia y co2 en el entrenamiento
- el costo oculto esta en el uso....y la inferencia cuando se usa la IA.
- cuanto hay de consumo del testeo.
- cuanto es el consumo cuando se manda un prompt
- en contra cuanto invierten en hacer sus modelos en cuanto cuanto gastan
- Es el tamaño del modelo en hiperparametros....mas que la arquitectura
- es como una compresion de la informacion y no puedo generar algo
  nuevo
- el modelo si generaliza lo que no pueden crear nada nuevo...
- hacer como habilitades emergentes.....
- es muy distinto que tenga capacidad de imaginacion o de descubirr cosas
*** Emergent abilities por large language models
- en la figura 2 eje vertical tiene exactitud y en el horizontal el
  costo en FLOPS
- a partir de un punto como $10^22$
- cuando el modelo es significativamente grande el modelo exhibe o es
  mejor que un modelo aleatorio.
- aqui el tama;o importa y mientra mas grande mejor.
** Summary
* Enlaces
- Libros Generativa: https://drive.google.com/drive/folders/1fjGzPM0VnL40AQd1AVWJfZO05NXsKTya?usp=sharing
- Libros Machine Learning: https://drive.google.com/drive/folders/0B0w0jIatGZYGZHdNdW9FZ1ZycGs?resourcekey=0-6IrcgeLWNwIXoYq8lQ9e0Q&usp=sharing
- https://platform.openai.com/tokenizer
- https://jalammar.github.io/illustrated-word2vec/
- paper A survey of large language models.
- emergent abilities of large language models
* TODO
- [ ] Revisar EDOs y EDPs derivadas ordinarias y derivadas parciales
- [ ] revisar el algoritmo de retro-propagacion del error
- [ ] Principios y Fundamentos
- [ ] Definir la idea con respecto a mi tema de investigacion y el
  PEA: hacer pruebas y experimentacion. Hacer ajustes y trabajo Final.
- [ ] despues de la siguiente semana contar la idea que se ha hecho
  proponer la idea y esta idea contiene tales capitulos del temario
  que vamos a ver. necesito saber que problema y que alternativas de
  solucion van a probar esto seria 28 de abril. es sacar el articulo
  con algo de la materia. que seria el bagage teorico.
- [ ] localizar transformers en los llibros de Marco
- [ ] Presentacion concreta de 20 minutos mas 10 minutos para
  preguntas. Presentar la idea
