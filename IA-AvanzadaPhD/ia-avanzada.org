
#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Inteligencia Artificial Generativa
#+date: 2024-10-02
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment
* 2025-04-16 Notas iniciales PEA
** Questions and keywords
- retropropagacion del gradiente
- rmsprop, adam
- test time compute y razonamiento
- algoritmos avanzados para RL
- scientific machine learning :: inyecta conocimiento a priori par
  aque no sea solo a traves de datos. una RNN mapea o construye
  funciones en espacio de caracteristicas las RN Fisicas mapean en un
  espacio de funciones.
- computacion cuantica e IA
- Quedan tres años
** Notes
- Principios y Fundamentos
- No hay primer semestre o segundo semestre
- prsentar un articulo que tenga nivel de congreso tipo B o superior
- tiene que ser en el  tema qeu etstamos haciendo
- articulo ha de ser desarrollo de una idea en el trabajo de
  investigacion
- marco proporciona la metodologia....
- hay que ver el grado de avance....
- caṕ1 y cap2 usar en el tema de investigacion....
- no articulo de juguete.....
*** Libros
- Deep learning bases y conceptos: empieza con el capitulo 7
- checar capitulo 12del libro de bases tiene que ver con transformers
- pagina 395 tiene transformers de vision
- understanding deep learning de smunce prince
- murphy advanced probabilistic machine learning 847 del pdf
  - fundamentos actualizados
  - topicos avanzados
- el enfoque de la asignatura es investigacion basica
** Silabo:
CAPÍTULO 1. Tópicos Avanzados sobre Error Backpropagation						
1.1 Revisión del algoritmo de retro-propagación del error
1.2 Cálculo numérico de gradientes
1.3 Cálculo manual de gradientes						
1.4 Cálculo simbólico de gradientes
1.5 Cálculo automático de gradientes
CAPÍTULO 2. Optimización Avanzada para RNAs
2.1 Gradient descent with momentum					
2.2 RMSProp			
2.3 Adaptive Moment Estimation (Adam)
2.4 Métodos Quasi-Newton (BFGS y L-BFGS)
2.5 Hessian-Free Optimization 
CAPÍTULO 3. Test-Time Compute y Razonamiento
3.1 MCTS + Self Play + RNAs (AlphaGo, AlphaGo Zero y MuZero)
3.2 Chain of Thought (CoT)
3.3 Tree of Thoughts (ToT)
3.4 Auto-refinamiento
3.5 LLMs y razonamiento simbólico
CAPÍTULO 4. Algoritmos Avanzados de Reinforcement Learning
4.1 REINFORCE
4.2 Advantage Actor-Critic (A2C)
4.3 Proximal Policy Optimization (PPO)
4.4 RL from Human Feedback (RLHF)
4.5 Aplicaciones de RLHF
CAPÍTULO 5. Introducción a Scientific Machine Learning (SciML)
5.1 Introducción a SciML
5.2 Ecuaciones diferenciales ordinarias (EDOs) y parciales (EDPs)
5.3 Physics-Informed Neural Networks (PINNs)
5.4 Aplicaciones de SciML
CAPÍTULO 6. Tópicos Avanzados de IA
6.1 Manifold Hypothesis
6.2 Reward hypothesis
6.3 Plasticidad en RNAs	
6.4 Artificial General Intelligence (AGI)
6.5 Artificial Super Intelligence (ASI)
6.6 Computación cuántica e IA


** Summary
* 2025-04-22 Capitulo 1 Modelos Generativos de Lenguaje
** Questions and keywords
- Procesamiento de Lenguaje natural
- Modelos multimodal :: usan datos tanto lenguaje y visuales
- lenguajes formales :: mas faciles de procesar
- lenguaje natural :: es dificil de procesar por la maquina
- funcion de probabilidad :: 
- modelo de lenguaje ::
- FF :: red feedforward 
- LSTM :: funciono parcialmente
- token ::
- byte pair encoding ::
- word2vec ::
- de cuanto es la longitud de los embedings
- de cuanto es el tamaño del tokenizador
- influyen los pesos de los ids de los toekens
** Notes
- El boom de gpt es simular comprender el lenguaje
- nlp es complejo y es punto de partida de transformers y mecanismos
  de atención
- los lenguajes formales y artificiales son mas faciles de maniobrar y
  manipular pero no son conocidos o practicos para transmitir mensajes
- estamos dentro dle campo de la percepcion computacional
*** modelo de lenguajes
- todo algorimot de machine learning es una funcion matematica
- un modelo de lenguaje es una funcion que tiene como entrada lenguaje
  y la salida puede ser numeros o lenguaje
- es una funcion matematica
- el lenguaje es ambiguo y por esta razon es preferible usar una
  funcion de probabilidad que una determinstica
- la funcion de probailidad me permite calcular la probabilidad de la
  secuencia de tokens o palabras e.g. el modelo esta feliz
- una distribucion d eprobabilida dsirve para calcular probabilidades
- la distribucion d eprobabilidad sirve para sampling
- los modelos generativos samplean a partir de la funcion de probabilidad
- distribucion discretas sobre todo se usan
- caundo uno no sabe que funcion de distribucion probabilidad usar
  entoence recurro a usar una red neuronal. Ejemplo una FF con una softmax
- uno de los problemas importantes en lenguaje es que las cadenas de
  texto pueden llegar a ser secuencias largas. una palabra que esta
  cientos de pasos atras puede cambiar el signficado de texto
- feed forward no sirvieron para muchos casos
- RNN: LSTM, GRU, Gated
- LSTM era el estado del art hasta 10 años atras
- luego surje LSTM + mecanismo de atención
- finalmente llegan los transformers
- si en el ejemplo las cadenas de texto si no tienen relacion tiene
  que dar probabilidades baja y si tiene relacion tiene probabilidad alta
- entonces la probabilidad condicional refleja como funciona el lenguaje.
- modelos de lenguaje son distribuciones de probabilidad y estas
  distribuciones se representan con RNA
- porque no se usa tokens y no palabras
- tokenization es como un diccionario
- lo que se termina aprendiendo es el vector que representa a cada embediing
- como generamos los tokens?
- un token tiene en español e ingles una equiv de 3/4 de palabra
- hay un punto que en funcion  de las iteraciones que alcanzamos un
  maximo de tokens entonces hay que llegar a que no sean palabras
  individuales ni tampoco llegar a un punto atomizante tanto que no
  hay signficado
- se tomo la idea fusionar bits para formar cadenas para poder llegar
  a la formacion de los tokens. la termiinacion es algo arbitraria
- gpt1 tokenizador es distinto al gpt2
- otra cosa que no es evidente es que un computador tiene que procesar
  numeros. necesito convertir las cadenas en texto en numeros
- consideremos dos palabras que pueden estar juntas en el diccionario
  pero que peuden tener significados distintos: diablo dios. por
  ejemplo si ordeno alfabeticamente no refejaria que esos bojetos son
  como contrarios. por esta razon es que necesito cada palabra se
  represente con un vector de numeros.
- como se hace la codificacion en vectores
- crear un embeding de una imagen y analizar que partes del vector
  tenia que ver con edad joven risa
- modelo de incrustacion correspondiente para cada token. Para el
  modelo del lenguaje ingresa una cadena de vectores
- paper recomendado : a survey of large language models  en arxiv la
  figura 7 es muy interesante.
  1. tomar scrapping d etexto
  2. filtrado y seleccion
  3. dedupicacion es decir quitar repeticiones
  4. privacy reduction quitar identificadores personales
  5. tokenizar 
** Summary

* 2025-04-23 
** Questions and keywords
- modo agente en LLM ::
- pesos sinapticos ::
- hay dudas sobre el costo de deepseek de 6 millones ::
- checkar costo ofertas laborales de openai meta y otras empresas grandes ::
- proyecto starlink ::
- testime compute :: parece que es una manera de obtener mas de lo
  poco que queda (limones)
- FLOPS :: cuantas operaciones en punto flotante se puede hacer con
  single y double precision hay formatos en 16 en machine learning
  para reducir entrenamiento.
- MAUs ::  numeor de nuevos usuarios agregandos por mes
** Notes
- forma de representar palabras como vectores
- los embeddings en esencia aplicar transformacines no lineales a los vectores
- los modelos d elenguaje no pueden crear nuevas cosas.... basicamente
  simulan capacidad de comprension y transforman vectores
- fig 6 del paper de survey of LLM se leen com giga tokens
- la fig 6 muestra el desafio de la actuallidad de los datos con los
  que aprende es decir que el conocimiento no este actualizado. por
  costo computacional no puede analizar toda la data de internet. un
  problema es como obtener datos actualizados
- para que los modelos tengan un conocimiento mas exacto usar los
  repositorios propios de IEEE, Springer ACM
- los modelos no pueden crear nada nuevo con respecto a lo que ya existe.
- pueden ser vistos como unos meros compresores de datos.
- T5 es bastante antiguo
- gpt3 es 2021 a 2022
- otro problema que se tiene qes que la informacion disponible
  completa es sobre los modelos abiertos y un poco antiguos. no hay
  una LLM anclada a la universidad.
- la barrera estaba en leer papers.
- la barrera esta en que no hay papers
- hay tres ejes en LLM: datos, hardware y conocimiento
  - hardware: decenas de miles de gpus
  - conocimiento avanzado: personas altamente especializadas. algo en
    las universidades pero mas en las industrias.
  - datos: 100tos de terabytes tokens
- los tres ejes requieren bastante inversion
- 500 k USD por a;o a personas que saben del tema
*** leyes de escalado de LLM
- LLama 3 cuesta unos 80 millones de dolares.
- presenta la ecuacion 2 para cualcular la funcion de perdida dado el
  tamaño del modelo y la cantidad de datos . los coefcientes A,B \alpha
- mientras mas grande sea mmodelo y mas datos la funcion de perdida es menor
- la generacion de datos sinteticos no es muy efectiva a la hora de
  aportar nueva informacion
- cuanto puede el modelo generar de informacion y lo ideal es que
  genere mas bits de informacion que los bits de informacion que le llega
- no se absorbe conocimiento desde lenguaje
- no se cree que los LLMs sean los unicos detras de una IAGeneral
- resulta que recibimos mas datos que una LLM si observamos desde otra perspectiva
- una persona recibe 500x500 pixel y
- la cantidad de informacion de una persona recibe hasta 25 años
  obtiene mas menos 6 Peta Bytes.
- se recicla algo del conocimiento de modelos previos. no se parte d
  elos pesos aleatorios
- En la formula es costo computacional $C \approx 6ND$ con N tamaño
  del modelo y D el tamaño de los datos (tokens)
- costo de entrenamiento de SOTA LLaMA 3 usa 15.6 TeraTokens. en bytes
  es mas
- hay que revisar en que se refiere los billones y trillones en ingles
  y español
- en lama 3 se usa 16 mil tarjetas h100.  esta tarjeta cuesta mas o
  menos 30 mil dolares.
- tema de gpus ver enlace GPU  nvidia h100 specs
  H100
- la compu del labo endra 48GB nvidia a6000 workstation ya hay un par
  instalado 48GB
- el entrenamiento de una LLM puede tomar 70 dias
- costos viene del costo de alquilar el datacenter mas los
  salarios. se considera como 2 dolares por hora de uso d euna gpu H100.
- mas menos asumiendo el costo de entrenar  va a 75 millones
- toneladas equivalentes de Co2
- parece mucho el gasto de energia y co2 en el entrenamiento
- el costo oculto esta en el uso....y la inferencia cuando se usa la IA.
- cuanto hay de consumo del testeo.
- cuanto es el consumo cuando se manda un prompt
- en contra cuanto invierten en hacer sus modelos en cuanto cuanto gastan
- Es el tamaño del modelo en hiperparametros....mas que la arquitectura
- es como una compresion de la informacion y no puedo generar algo
  nuevo
- el modelo si generaliza lo que no pueden crear nada nuevo...
- hacer como habilitades emergentes.....
- es muy distinto que tenga capacidad de imaginacion o de descubirr cosas
*** Emergent abilities por large language models
- en la figura 2 eje vertical tiene exactitud y en el horizontal el
  costo en FLOPS
- a partir de un punto como $10^22$
- cuando el modelo es significativamente grande el modelo exhibe o es
  mejor que un modelo aleatorio.
- aqui el tama;o importa y mientra mas grande mejor.
** Summary
* 2025-04-29 Introducción a Transformers
** Questions and keywords
- mecanismo de atencion ::
- matriz de atención ::
- encoder ::
- decoder ::
- BLEU ::
- multi head self attention :: esta presente tanto en el encoder como
  el decoder
- cross attention :: ingresa flujo de informacin del encoder y del
  decoder es decir viene nifo de dos lineas
- positional encoding :: indica el orden
- no es claro como se integra KQV en los vectores ::
- masked attention ::
- es el masked attention una matriz triangular ::
- modelos multimodales :: hablan de cross attention
- layer normalization ::
- LORA :: intencionalmente usan dos matrices para no hacer una mas
  grande en el aprendizaje
** Notes
- paper a utilizar es Attention is all you need
- revisar el video de gustavo etrala transformers de 40 minutos
- como 8 científicos anónimos inventaron la ia generativa
- ej el modelo se esta alistando para realizar su *presentacion* en el *desfile*
- ej el modelo se esta alistando para que haga buenas **predicciones**
- en ambos casos modelo tiene el mismo embedding pero en las oraciones
  no hace referencia al mismo objeto
- el contexto permite determinar a que objeto
- en algun lado esta un embedding de persona y el mecanismo de
  atencion va a relacionar en el primer caso el embedding d emodelo
  con el de persona y en el segundo con el de un objeto matematico
- los tokens/palabras que mas inciden son presentacion y desfile
- parece logico que la idea seria sumar algo de cada uno de los
  vectores
- cuanto es un poco y eso se define a travies de matriz de atencion
  mediante pesos
- la idea ya estuvo propuesta por Bengio
- el transformer aplicara transformaciones a los vectores para que los
  vectores representen mejor los conceptos y modelar el lenguaje
  e.g. bert y gpt que son encoder/decoder
- cohere es una empreza fundada por uno de los autores
- ya ninguno esta en google
*** multihead self attention
- matriz con las palabras el modelo esta feliz
- el objetivo es ajuste el vector de la palabra modelo
- los pesos de la matriz indican con cuanto aporta cada uno de los vectores
- el orden de la matriz de atencion no influye
- cada uno de los numeros es un porcentaje que indica cuanto influye
  cada token, como el objetivo es hace run nuevo embedding de modelo
  este se forma como 0.2xel+0.4*modelo+0.2*esta+0.2feliz
- el vector esta formado o relacionado con Query Value y Key --Aclarar esto--
- la matriz de self attention tiene el tamaño de la ventana de la
  secuencia d etexto???
- para calcular la matriz de atencionn necesito todos los tokens
- el mecanismos de vaswani se usa en berts porque requiero todos los
  tokens e.g. clasificacion
- el mecanism de vaswani no se usa en los generativos
- en los generativos necesito usar la masked attention
- en el masked attentnio va descubriendo los valores conforme aparecen
  las palabras
- se pone un ejemplo con la oracion el modelo esta feliz y la idea en
  generativos es que se va descubriendo palabra a palabra quitando las
  influencias de otras
- cross attention. por ejemplo tengo dos modalidades un idioma otro
  idioma y texto. la idea es determinar como cada parte voz
  representado por su embedding y como ajustar el embedding de la palabra
- auto atncion es que los propios tokens de la secuencia influyen en
- definir porciones y caracterizarlas por un embedding???
- al cruzar texto e imagenes o texto y audio como se puede relacionar
  los espacios entre los tokens
*** como funcionaba antes con RNN
- igual tienen encoder y decoder
- las rnn sacan hiddens states que son entradas par alas siguientes rnns
- luego se usaba una etapa de atencion
- en la version que usa atencoin ingresa la salida de un hidden state
  mas las salidas de cada uno de los hidden states particulares
- los hidden states se ven afectados por los valores anteriores
- la informacion solo influye en un sentido
- no hay mecanismo explicito de attention en los hidden states
- computacionalmente son pesasdas las RNN para secuencias largas
- no podian abosrver un contexto amplio
- le dieron el nombre de transformers porque los embeddings van
  cambiando su significado en funcion del procesamiento
*** qkv
- vector de preguntas
- vectotr de llaves
- vector de values
- multihead porque se puedene poner en paralelo
- x1 embedding de el
- wq, wk wv se amprenden en entrenamiento
- se usa la idea como hace consultas en la base de datos
- query representa la consulta
- query pregunta y keys responde con cuanto aporta al significado
- un vector que representa la pregunta cual de estos embeddings deben
  influir mas para redefinir el significado
- los keys en cambio indican con cuanto aportan los embeddings
- keys es con cuanto
- tom yeh buscar universidad colorado boulder machine learning by hand
- hace calculos a mano de temas de machine learning
- los embeddings que entran son despues de aplicar el positional
  encodnig en el ejemplo de TOm Yeh
- se multiplica Wq con la matriz de features
- se asume un valor aprendidos de wq, wk y wv
- hay tantos queries como tokens de entrada
- hay tantos keys como tokens de entrada
- esto es clave ya que no se representa toda la informacin en un solo
  vector comprimido
- ahora la informacion esta distribuida
- em el segundo paso se hace un match entre los keys y queris haciendo K^TQ
- el resultado del producto de K traspuesta con Q inidca cuanto debo
  ajustar el token para ajustar el signfiicado
- luego se hace una dvision para escalar los datos dividiendo por
  factor de escala , en el ejemplo le aproximan a 2, cuando el dk es
  la dimension de los keys
- en la parte de softmax esta aplicando 3 elevado a los valores de la
  anterior matriz
- luego suma los valores de cada columna y luego hace la division
  obteneiendo la matriz de pesos de atencion normalizada
- con la mtriz de atencino se define los nuevos features
- QKV sirven para calcular los pesos
- el ajuste de los embeddings son los values multiplicados por la
  matriz de atencion
- el z1 es el embedding que representa el token el
- la salida del multi head va para addicion y normalizacoin
- los que son aprendibles son el Wq, Wk y el Wv
- hay nucleos especiales para cualcups trigonometri para calcular el
  factor de escalamiento
- se observa que esta aptop par ausarse on gpus
- en el transformer se pone varios bloques
- cada bloque tiene pesos diferentes
- cada cabezal aporta con sus representacions par los embeddings
- luego se concatena los paortes yse multiplica por otra matriz de pesos
- boque de adicion y normalizacion
- normalizacion busca que todas las salidas tenga la misma escala y el
  misma media
- la normalizacion e spor features normalizacion
- se obtiene media y desviacion std de cada vector y la normalizacion
  en transformers es por token por cada embedding no por feature
- la normalizacion en transformers es por embedding y es layer
  normalization restar la media y dividir para la desviacion estandar
  por capa
- batch normalization es diferente y en esa defino un tamano
- el ultimo wo que se pone es dar un formato que permite sumar con los
  embeddings de entrada ya que la concatenacion me produciria una
  matriz un tanto grande o mas grande que la entrada
- en el vector z porque se aumentan parametros para aprender? esto
  tiene una relacion a lo que se hace al final de una layer
  normalization y una batch normalization con el objetivo de ajustar
  para las capas que vienen ya que por ejemplo en el transformer le
  suceden FF que podrian usar cualquier funcion de activacion
  teoricamente
- la historia cuenta que havia mas cosas o compoentnes en el
  transformer pero se retiraron porque se mantuvieron con el mismo rendimeinto
- hay que notar que se usa uniones resuduales
- values son los valores d elos candidatos partieron de los concepts
  de busquedas en sql
- es una heurstica el raiz de dk
- porque razon sacan la $d_k$
- la ecuacion de atencion un paralelo a E = mc2? $Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})$
** Summary
* 2025-04-30 Presentaciones
- sparse autoencoder ::
- dictionary learning ::
- gemma scope :: saes open source
- como usan el sparse autoencoder ::
- hipotesis de representacion lineal ::
- feature splitting ::
- composition of lattens ::
- feature absortion ::
- chunks ::
- que es ralo en sparcity auto encoders ::
- feature exploration ::
- que va a vender  ::
- IRL :: funcion de recompensa inversa
- automatic feature extraction :: 
** Notes
*** Conceptos Trabajo Jonathan
- las caracteristicas del sparse autoencoder tiene efecto en el modelo
- los sparse autoencoders son metodo no supervisado
- spas tiene problemas con conceptos jerarquicos
- verificar que anthropic esta lider en explainable IA
- no hay una estructura jerarquica en el spas
- hay una version matryoshka de sparse autoencoders
- modifica la funcion de costo
- capturar relaciones jerarquicas
- es interesante el grafico porque parece los conceptos como una nube
  de particulas de gas y la idea seria atraerlos y condensarlos como
  que fuera una gravitacion
- idea: que pasa si la relacion entre neuronas no la hago de manera
  lineal pero la hago como una ley de gravedad? pero habria un
  problema con la division por la r cuadrada?
- estan trantando de aportar una interpretabilidad para RL
- que es una pregunta inocente....
- en el tema de matryoshka yo me imagine como varias sparse y
  relacionar entre ellas
- parece que los pesos en la matryoshka es que los pesos no quedan igual
- una variante de algoritmos no es suficiente con un solo ejemplo
* Presentacion
- prompt : You are a PhD research Assistant specialized in machine
  learning, vision transformers, computer vision, image
  segmentation. Your task is to help me to solve questions, conceptual
  problems, locate relevant academic sources to improve my
  research. When asked, provide factual answers supported by research
  papers. Present examples and references that support
  answers. Provide research advice with rigor.
** observaciones doc
- hacer el clasificador si el clasificador sirve para mi problema que
  es el de segmentación
- por ejemplo tratar de determinar como primera parte si ViT sirvió
  para algo o no y por ejemplo si no sirvió usar ViT ponerme a usar
  directo CNN
- reducir complejidad debido a los costos computacionales
- la observación de la arquitectura que hubo es que unet si saca algo
  si le meto una imagen de cualquier tamaño en la entrada. luego hubo
  retractación
- Revisar si resolvemos primero termografía
- el tema de entrenar clasificación si y solo si eso servirá para algo
  de la tesis
- tomar contacto con cedia con modemat y con lasinac
- que no sea de juguete que significa? que sirva para la tesis
- ideas --> podria hacer un reescalado progresivo? como iba lo de los
  superpixels??
- hubo una confusión en cuanto a la imagen de entrada y el paso por la unet
- hay que acabar para pasar la nota y garantizar el paper
- Garantizar los datos
- el fundamento de la idea es importante
- pero para saber que la idea sea brillante es experimentar
- si no puedo evaluar no sirve
- si puedo evaluar pero la idea no da un buen resultado entonces no sirve
- el otro tema es la relevancia. proponer una idea y que no sirva.
- cómo voy a evaluar y qué espero tener para decir que la idea es buena
- **la idea esta clara**, revisar y estimar tiempos porque hay que
  terminar el trabajo. si la idea es muy grande tratar de recortar
  cosas y probar la parte conceptual lo que nos quede algo que sea
  núcleo(i.e. core) y que sirva
- asegurarse de cómo van a hacer los experimentos y que van a medir. y
  midiendo *cuándo puedo decir es una buena idea y cuando no es*; hay que
  llegar a decir que la arquitectura propuesta permite alcanzar tal cosa.
- la validez científica de un modelo esta en que también predice.
- que no sea de juguete porque nos gastaría el tiempo. es decir, tiene
  que ser un ejercicio que aporte a la investigación
* 2025-05-06 Tansformers
** Questions and keywords
- crossattention :: not so clear
- maximum path length :: la loongitud de la trayector entre las
  dependencias de largo rango. mientras mas peque;a son los paths mas
  facil aprender dependencias de largo rango
- self attention restricted :: usa ventanas para mitigar el efecto de
  secuencias muy largas
- pesos sinapticos o numero de parametros ::
- self supervised :: porque el propio dataset me da la etiqueta
- semi-suerpvisado :: es la mezcla del aprendizaje supervisado con no supervisado??
- supervised fine tuning ::
- reinforcemnt learnig from human feedback RLFH :: 
** Notes
- para hacer que las matrices v k y q sean diferentes en los heads del
  attention multi head se aplica transformaciones lineales y esta capa
  lineal es learnable. la capa lineal a la salida del multi head
  attention reduce a tener un solo cabezal
- la FFN son dos capas tiene una relu seguido de una capa lineal
- el encoder recibe todo el texto de una
- el decoder va obteniendo cada uno de los tokens de uno en uno segun
  la retroalimentacion que reciben.
- para la tabla 1 del paper n es el tamaño de la secuencia de texto
- d es el tamaño del embedding
- por capa el mecanismo introduce la complejidad $O(n^2d)$
- k es el tam;o del kernel.
- la segunda columna indica si es paralelizable o no
- cada secuencia tenia 25000 tokens. si tengo mas tokens o si crece la
  longitud de la sentencia crece la selfattention puede ser un problema
- la sugerencia del paper es procesar por ventanas
- Gemini y Llama al parecer son capaces de procesar decenas de
  millones de tokens. no hay una respuesta clara de como esta
  funcionando el mecanismo de atencion actualmente en bloques muy grandes
- el modelo hace calculos de probabilidades condicionales
- La ecuacion se traduce en una red neuronal que predice la
  probabilidad del token n dado los tokens anteriores
- usa un modelo que usa piezas del transformer y que aprende con un
  gran corpus de texto
- por cada tarea sea hace un fine tuning
- el modelo inicial aprende una especia de diccionario y de reglas
  gramaticales. esto es el pretrained
- luego usa aprendizaje supervisado para hacer el fine tuning sobre la tarea
- en el gpt se elimina el cross attention, se queda con el multi head
  , la capa de noramlizacion la  capa de feed forward e interconectar
  varios de estos bloques
- gpt1 usa 12 bloques con embeddings de 768
- la longitud de secuencia es de 512
- el gpt solo usa decoder
- que pasa como rellena cuando solo entra un token?
- para que el modelo sea gpt tiene tres tetaps
  - pre entrenamiento
  - SFT
  - RHLF
- Pre entrenamiento:
  - usan aprendizaje semi-supervisado o self-supervised
  - se usa un gran corpus de texto que genera las secuencias de
    entrenamiento usando una ventana de contexto k=4
  - el modelo tiene que predecir el siguente token
  - con una secuencia pequeña se obtiene una gran cantidad de ejemplos
  - la fully connected layer tiene una softmax con el mismo tamaño
    token
  - la salida es una distribucion de 0s para todos los tokens excepto
    el que se enmascaro
  - se entrena con mini batch gradient descent
  - esto le permite al gpt producir el siguiente token dados los
    tokens previos
  - el modelo era bueno para predecir el token siguiente y mejor que
    otros del estado
- SFT
  - se necesita datos con etiqueta
  - en este caso es un juego de inputs o prompts vs outputs
  - hay dos maneras de hacer esto tomar el corpus y cortar la respuesta
  - la otra es curar la respuesta
  - supervised fine tuning. funcion de costo
  - la salida del GPT se vuelve una secuencia de tokens
  - la idea es tratar de obtener las respuesta de base de conocimiento
  - se usa el pretrained con este dataset curado para entrenar el GPT
  - africa se usa para hacer la generacion de los datasets input
    output o prompt generated
  - en el sft la input seria escribe una frase romantica en 2 palabras
    y la salida es eres genial
  - sft requeire muchos datos etiquetados que es un rpoblema
- RHLF
  - se supone que el modelo da cada vez salidas diferentes
  - RHLF se basa en rankear las respuestas. ejemplo a una pregunta e
    lmodelo sacaba otra pregunta
  - por ejemplo a input hola como estas hay dos respuesta que mas ve y
    estoy bien como estas tu. el RHLF escogia la segunda como mejor
    respuesta
  - un tema que queda aqui en estas etapas de alineamiento es el
    problema de sesgos
  - los anotadores tienen que evaluar las respuestas que da el modelo
  - RLHF entra en que se entrea otra red neuronal artificial que tiene
    el objetivo de ser un revisor
  - el modelo de RLFH usa el dataset de ranking para entrenar un
    modelo de recompensas
  - el modelo de recompensas tiene como entrada dos secuencias de
    texto y como salida tienen 0 a 1
  - 1 satisface el requerimiento
  - 0 no satisface lo que se esta pidiendo a chat gpt
  - una vez entrenado el modelo de recompensas se lo usa para entrenar
    chat gpt
  - para esto se toma un sampling del dataset de rankin g de respuestas
  - y se repite el proceso para taratr d emejorar el rendimiento del chatgpt
- chat gpt tiene tres etapas, SFT y el RLFH son con datos especificos
  de la tarea
- GPT es un modelo autoregresivo es decir dado una entrada el gpt
  predice la etiqueta
- muestreo proporcional a la probabilidad
- para darle creatividad a gpt se le da el muestreo de la softmax que sale
- usualrmente sale el token mas probable
- el cambio aleatorio en un token va a generar cambios en la respuesta
  de salida
- el muestreo permite que el modeloo de una respuesta diferencia
  e.g. cambie el como escribe pero no el contenido es decir si
  pregunto 2+2 siempre dara 4 pero lo escribira de maneras distintas
- el proceso d egeneracion termian cuando aparece el token de finalizacoin
- para evitar calculos repetitivos QKV para los tokens ya pasados eso
  se mantiene porque QKV depnden del propio token lo ineficiente seria
  volver ahacer el calculo de QKV cada vez para todos los tokens. es
  decir se calcula el nuevo token (no muy claro como calcula solo lon nuevo)
- enn el pricing del open ai tiene un cached input que son los tokens
  que estan almacenados que se encuentran listos para hacer calculos
- revisar el alineamiento y el preentranmiento del chat gpt. nos falta
  ingresar en detalles que tiene el paper.
- tambien se revisaran tecnicas distitnas de muestreo

** Summary
*** *Sección 1: Preguntas y Palabras Clave*  
1. *Cross-Attention*: Mecanismo en el /decoder/ del Transformer que
   permite a cada token del /decoder/ enfocarse en tokens específicos
   del /encoder/. Por ejemplo, en traducción, el decoder usa
   cross-attention para alinear palabras del idioma fuente (encoder)
   con las del objetivo (decoder).

2. *Maximum Path Length*: Longitud de la trayectoria entre tokens para
   capturar dependencias de largo alcance. En *self-attention*, el
   camino entre cualquier par de tokens es 1 (completa conectividad),
   mientras que en RNNs es \(O(n)\). Esto facilita aprender relaciones
   distantes.

3. *Self-Attention Restricted*: Técnicas para reducir costos
   computacionales en secuencias largas. Ejemplo: *atención local*
   (ventanas de tokens cercanos) o *atención esparsa* (solo tokens
   clave). Modelos modernos (Gemini, Llama) usan optimizaciones como
   /flash attention/ o /memory caching/.

4. *Número de Parámetros*: En un Transformer, los parámetros incluyen:
   - Embeddings (\(d \times V\), con \(V\) = vocabulario).
   - Matrices \(Q, K, V\) en cada cabeza de atención (\(d \times
     d_k\)).
   - Capas Feed-Forward (FFN: \(d \times 4d\) y \(4d \times d\)).
   Ejemplo: GPT-1 (~117M parámetros) con 12 capas, \(d=768\), y 12
   cabezas.

5. *Self-Supervised Learning*: Aprendizaje donde las etiquetas se
   generan automáticamente del dataset (ej: predecir el siguiente
   token). *No es semi-supervisado* (este mezcla datos etiquetados y
   no etiquetados).

6. *Supervised Fine-Tuning (SFT)*: Fase donde se ajusta el modelo
   pre-entrenado con datos etiquetados específicos (ej: pares de
   pregunta-respuesta).

7. *Reinforcement Learning from Human Feedback (RLHF)*:
   - *Paso 1*: Humanos rankean respuestas del modelo.
   - *Paso 2*: Se entrena un *modelo de recompensa* para predecir
     preferencias humanas.
   - *Paso 3*: Se optimiza el modelo con RL (ej: PPO) usando el modelo
     de recompensa como guía.

---

*** *Resumen de Notas (Corregido y Ampliado)*  
**** *Arquitectura del Transformer*  
El Transformer usa *multi-head attention* para procesar
secuencias. Cada cabeza aplica transformaciones lineales a \(Q, K, V\)
(parámetros aprendibles), permitiendo capturar distintos tipos de
dependencias. La salida de las cabezas se concatena y pasa por una
capa lineal.
- *Feed-Forward Network (FFN)*: Dos capas lineales con ReLU (\(d
  \rightarrow 4d \rightarrow d\)).
- *Encoder vs. Decoder*:
  - /Encoder/: Procesa toda la secuencia de entrada simultáneamente
    (bidireccional).
  - /Decoder/: Genera tokens autoregresivamente (uno a la vez), usando
    /masked self-attention/ para evitar "ver" tokens futuros.

**** *Optimización y Desafíos*  
- *Complejidad Computacional*: La self-attention tiene complejidad
  \(O(n^2d)\), limitando su uso en secuencias largas. Soluciones:
  atención por ventanas o caching de claves/valores (ej: KV-caching en
  GPT).
- *Longitud de Secuencia*: Modelos modernos (Gemini) manejan millones
  de tokens usando técnicas como /sparse attention/ o /hierarchical
  processing/.

**** *Entrenamiento de GPT*  
1. *Pre-entrenamiento*:
   - *Self-Supervised*: Predicción del siguiente token en corpus no
     etiquetado (ej: ventana de contexto de 512 tokens en GPT-1, *no
     k=4*).
   - Función de pérdida: Cross-entropy entre la salida (distribución
     de probabilidad sobre tokens) y el token objetivo.

2. *Supervised Fine-Tuning (SFT)*:
   - Se ajusta el modelo con datos etiquetados (ej: pares de
     instrucción-respuesta).
   - Ejemplo: Entrada = "Escribe una frase romántica en 2 palabras",
     Salida = "Eres genial".

3. *RLHF*:
   - *Modelo de Recompensa*: Clasifica respuestas (ej: 1 = buena, 0 =
     mala) o compara pares (ej: respuesta A > B).
   - *Alineamiento*: El modelo ajusta sus salidas para maximizar la
     recompensa aprendida, mitigando sesgos o toxicidad.

**** *Generación de Texto*  
- *Autoregresión*: El modelo genera tokens secuencialmente, usando la
  salida anterior como entrada.
- *Muestreo*:
  - /Greedy/: Token con mayor probabilidad (poco creativo).
  - /Temperatura/: Suaviza la distribución para mayor diversidad.
  - /Top-k/p/: Muestrea de los \(k\) tokens más probables o acumula
    probabilidad hasta \(p\).
- *Finalización*: La generación termina al producir un token especial
  (ej: =<eos>=).

**** *Correcciones Importantes*  
1. *Pre-entrenamiento no es semi-supervisado*: Es *self-supervised*
   (etiquetas generadas internamente).
2. *Modelo de Recompensa en RLHF*: No clasifica en 0/1, sino que
   asigna un score continuo (ej: 0.7 indica calidad media).
3. *KV-Caching*: Durante la generación, se almacenan claves/valores de
   tokens previos para evitar recalcularlos, reduciendo costos.

Este resumen integra los conceptos clave, corrige errores, y
contextualiza el funcionamiento de modelos como GPT. ¡Buen estudio!

* Enlaces
- Libros Generativa: https://drive.google.com/drive/folders/1fjGzPM0VnL40AQd1AVWJfZO05NXsKTya?usp=sharing
- Libros Machine Learning: https://drive.google.com/drive/folders/0B0w0jIatGZYGZHdNdW9FZ1ZycGs?resourcekey=0-6IrcgeLWNwIXoYq8lQ9e0Q&usp=sharing
- https://platform.openai.com/tokenizer
- https://jalammar.github.io/illustrated-word2vec/
- paper A survey of large language models.
- emergent abilities of large language models
- [[https://youtu.be/HX8IMpnESxk?si=JhPz7Fi-_JNPaNR7][8 cientificos inventan IA generativa]]
* TODO
- [ ] Revisar EDOs y EDPs derivadas ordinarias y derivadas parciales
- [ ] revisar el algoritmo de retro-propagacion del error
- [ ] Principios y Fundamentos
- [ ] Definir la idea con respecto a mi tema de investigacion y el
  PEA: hacer pruebas y experimentacion. Hacer ajustes y trabajo Final.
- [ ] despues de la siguiente semana contar la idea que se ha hecho
  proponer la idea y esta idea contiene tales capitulos del temario
  que vamos a ver. necesito saber que problema y que alternativas de
  solucion van a probar esto seria 28 de abril. es sacar el articulo
  con algo de la materia. que seria el bagage teorico.
- [ ] localizar transformers en los llibros de Marco
- [X] Presentacion concreta de 20 minutos mas 10 minutos para
  preguntas. Presentar la idea
- [ ] Pensar en dond publicar
- [ ] Presentar la idea mas definida mas concreta
- [ ] Leer paper Improving language with generative el paper del GPT
