{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeninGF/CoursesNotes/blob/main/MarkovRL/00-TragaMonedas-RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597b61a1-ccaf-44cf-8c91-9f2f73285a9b",
      "metadata": {
        "id": "597b61a1-ccaf-44cf-8c91-9f2f73285a9b"
      },
      "source": [
        "# Problema Tragamonedas\n",
        "\n",
        "Se necesita:\n",
        "\n",
        "- implementar el entorno\n",
        "- implementar las políticas\n",
        "- actualizar la función de valor (promedio)\n",
        "- se usa ascenso del gradiente para actualizar la politica de GBA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "103b40a9",
      "metadata": {
        "id": "103b40a9"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import trange # avance de las iteraciones\n",
        "from typing import List # autocompletar en el notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff539d7",
      "metadata": {
        "id": "bff539d7"
      },
      "source": [
        "## Clases para actualizar las políticas\n",
        "\n",
        "Política promedio, política con una constante y la del gradiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6b5704de",
      "metadata": {
        "id": "6b5704de"
      },
      "outputs": [],
      "source": [
        "class ActualizarPolitica:\n",
        "    def actualizar(self, politica, accion, recompensa):\n",
        "        pass\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "class ActualizarMediaAritmetica(ActualizarPolitica):\n",
        "    def __init__(self, num_acciones:int):\n",
        "        self.num_acciones = num_acciones\n",
        "        self.contador_accion = np.zeros(num_acciones) # incrementar el contador de la accion que se ha tomado\n",
        "    def actualizar(self, politica:\"Politica\", accion:int, recompensa:float):\n",
        "        self.contador_accion[accion]+=1 # el indice es la accion y asi se incrementa cada accion\n",
        "        # cada accion tiene su propia q\n",
        "        politica.q_est[accion]+=(recompensa-politica.q_est[accion])/self.contador_accion[accion]\n",
        "    def reset(self):\n",
        "        self.contador_accion = np.zeros(self.num_acciones)\n",
        "\n",
        "class ActualizarConstante(ActualizarPolitica):\n",
        "    def __init__(self, num_acciones:int, constante:float):\n",
        "        self.num_acciones = num_acciones\n",
        "        self.constante = constante\n",
        "        self.contador_accion = np.zeros(num_acciones) # incrementar el contador de la accion que se ha tomado\n",
        "    def actualizar(self, politica:\"Politica\", accion:int, recompensa:float):\n",
        "        politica.q_est[accion]+=self.constante*(recompensa-politica.q_est[accion]) # no se observa completo\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class ActualizarGradiente(ActualizarPolitica):\n",
        "    def __init__(self, num_acciones:int, con_baseline:bool, constante:float):\n",
        "        self.num_acciones = num_acciones\n",
        "        self.con_baseline = con_baseline\n",
        "        self.constante = constante\n",
        "        self.recompensa_media = 0\n",
        "        self.tiempo = 0\n",
        "\n",
        "    def actualizar(self, politica:\"Politica\", accion:int, recompensa:float):\n",
        "        self.tiempo += 1\n",
        "        self.recompensa_media+=(recompensa-self.recompensa_media)/self.tiempo\n",
        "        one_hot = np.zeros(self.num_acciones)\n",
        "        one_hot[accion] = 1\n",
        "        if self.con_baseline:\n",
        "            baseline = self.recompensa_media\n",
        "        else:\n",
        "            baseline = 0\n",
        "        exp_h = np.exp(politica.q_est)\n",
        "        prob_accion = exp_h/np.sum(exp_h) # softmax\n",
        "        politica.q_est+=self.constante*(recompensa-baseline)*(one_hot-prob_accion) # no se observa completo\n",
        "    def reset(self):\n",
        "        self.recompensa_media = 0\n",
        "        self.tiempo = 0\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f847c7",
      "metadata": {
        "id": "c6f847c7"
      },
      "source": [
        "## Políticas\n",
        "\n",
        "- GBA\n",
        "- UCB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "be96d702",
      "metadata": {
        "id": "be96d702"
      },
      "outputs": [],
      "source": [
        "class Politica:\n",
        "    def __init__(self, actualizar_politica:ActualizarPolitica, num_acciones:int, iniciales:float):\n",
        "        self.actualizar_politica = actualizar_politica\n",
        "        self.n = num_acciones\n",
        "        self.iniciales = iniciales\n",
        "        self.q_est = np.zeros(self.n)+self.iniciales\n",
        "    def actuar(self):\n",
        "        raise NotImplementedError(\"Actuar no Implementado\")\n",
        "    def avanzar(self, accion:int, recompensa:float):\n",
        "        raise NotImplementedError(\"Avanzar no implementado\")\n",
        "    def reset(self):\n",
        "        self.q_est = np.zeros(self.n)+self.iniciales\n",
        "        self.actualizar_politica.reset()\n",
        "\n",
        "class PoliticaEpsilonGreedy(Politica):\n",
        "    def __init__(self, actualizar_politica: ActualizarPolitica, num_acciones: int, epsilon: float, iniciales: float=0.):\n",
        "        super().__init__(actualizar_politica, num_acciones, iniciales)\n",
        "        self.epsilon = epsilon\n",
        "    def actuar(self):\n",
        "        if np.random.rand()< self.epsilon:\n",
        "            return np.random.randint(0, self.n)\n",
        "        q_max = np.max(self.q_est)\n",
        "        return np.random.choice(np.where(self.q_est==q_max)[0])\n",
        "    def avanzar(self, accion: int, recompensa: float):\n",
        "        self.actualizar_politica.actualizar(self, accion, recompensa)\n",
        "\n",
        "\n",
        "class PoliticaUCB(Politica):\n",
        "    # extremo superior de confianza\n",
        "    def __init__(self, actualizar_politica: ActualizarPolitica, num_acciones: int, UCB_param:float, iniciales: float=0.):\n",
        "        super().__init__(actualizar_politica, num_acciones, iniciales)\n",
        "        self.UCB_param = UCB_param\n",
        "        self.tiempo = 0\n",
        "        self.contador_accion = np.zeros(self.n)\n",
        "    def actuar(self):\n",
        "        UCB_est = self.q_est+self.UCB_param*np.sqrt(np.log(self.tiempo)/(self.contador_accion+1e-5)) # numero agregado para evitar div por 0\n",
        "        q_max = np.max(UCB_est)\n",
        "        return  np.random.choice(np.where(UCB_est==q_max)[0])\n",
        "    def avanzar(self, accion:int, recompensa:float):\n",
        "        self.tiempo+=1\n",
        "        self.contador_accion[accion]+=1\n",
        "        self.actualizar_politica.actualizar(self, accion, recompensa)\n",
        "    def reset(self):\n",
        "        super().reset()\n",
        "        self.contador_accion = np.zeros(self.n)\n",
        "        self.tiempo = 0\n",
        "\n",
        "class PoliticaGradiente(Politica):\n",
        "    def __init__(self, actualizar_politica: ActualizarPolitica, num_acciones: int, iniciales: float=0):\n",
        "        super().__init__(actualizar_politica, num_acciones, iniciales)\n",
        "        self.q_est =np.zeros(self.n)\n",
        "    def actuar(self):\n",
        "        exp_h = np.exp(self.q_est)\n",
        "        prob_accion = exp_h/np.sum(exp_h)\n",
        "        return np.random.choice(np.arange(0, self.n), p=prob_accion)\n",
        "    def avanzar(self, accion: int, recompensa: float):\n",
        "        # super().avanzar(accion, recompensa)\n",
        "        self.actualizar_politica.actualizar(self, accion, recompensa)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "878ce7c0",
      "metadata": {
        "id": "878ce7c0"
      },
      "source": [
        "## Entorno\n",
        "\n",
        "modelo de las maquinas traga monedas\n",
        "cada maquina tiene una distribucion de probabilidad desconcida\n",
        "las distribuciones de cada maquina empiezan en 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d76d1eac",
      "metadata": {
        "id": "d76d1eac"
      },
      "outputs": [],
      "source": [
        "class Entorno:\n",
        "    def __init__(self, num_acciones, recompensa_verdadera=0.0):\n",
        "        self.n = num_acciones\n",
        "        self.recompensa_verdadera = recompensa_verdadera\n",
        "        self.q_verdadera = np.random.randn(self.n)+self.recompensa_verdadera # distribucion normal desplazada\n",
        "        self.mejor_accion = np.argmax(self.q_verdadera)\n",
        "    def reset(self):\n",
        "        self.q_verdadera = np.random.randn(self.n)+self.recompensa_verdadera\n",
        "        self.mejor_accion = np.argmax(self.q_verdadera)\n",
        "    def recompensa(self, accion:int):\n",
        "        return np.random.rand()+self.q_verdadera[accion]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9bf25f4",
      "metadata": {
        "id": "e9bf25f4"
      },
      "source": [
        "## Simulación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7feac2e6",
      "metadata": {
        "id": "7feac2e6"
      },
      "outputs": [],
      "source": [
        "# iterar las politicas , tiempo cuantas iteraciones, intentos\n",
        "k = 10 # num de bandidos\n",
        "\n",
        "def simulador(intentos, tiempo, politicas:List[Politica], entorno:Entorno):\n",
        "    recompensas = np.zeros(len(politicas), intentos, tiempo)\n",
        "    contador_mejor_accion = np.zeros(recompensas.shape)\n",
        "    for p, politica in enumerate(politicas):\n",
        "        for i in trange(intentos):\n",
        "            politica.reset()\n",
        "            entorno.reset()\n",
        "            for t in range(tiempo):\n",
        "                accion = politica.actuar()\n",
        "                recompesa = entorno.recompensa(accion)\n",
        "                politica.avanzar(accion, recompesa)\n",
        "                recompensas[p,i,t] = recompesa\n",
        "                if accion == entorno.mejor_accion:\n",
        "                    contador_mejor_accion[p,i,t] = 1\n",
        "    media_contador_mejor_accion = contador_mejor_accion.mean(axis=-1)\n",
        "    media_recompensas = recompensas.mean(axis=-1)\n",
        "    return media_contador_mejor_accion, media_recompensas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f9f0763",
      "metadata": {
        "id": "1f9f0763"
      },
      "source": [
        "## Plots\n",
        "\n",
        "### Comparando Política Greedy para diferentes $\\epsilon$\n",
        "\n",
        "porque el numero de acciones se hace igual al numero de intentos y de bandidos\n",
        "\n",
        "**continuar desde 1:09:06**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7cc5a29c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "7cc5a29c",
        "outputId": "62992f50-189b-4864-a900-333846cd01a3"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Cannot interpret '2000' as a data type",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-84e224bbfa01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbandidos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPoliticaEpsilonGreedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActualizarMediaAritmetica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mentorno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntorno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcontador_mejor_accion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecompensas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulador\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintentos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiempo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandidos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentorno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-516d82cd13b9>\u001b[0m in \u001b[0;36msimulador\u001b[0;34m(intentos, tiempo, politicas, entorno)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimulador\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintentos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiempo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoliticas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPolitica\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentorno\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEntorno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrecompensas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoliticas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintentos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtiempo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcontador_mejor_accion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecompensas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolitica\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoliticas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '2000' as a data type"
          ]
        }
      ],
      "source": [
        "intentos = 2000; tiempo = 1000\n",
        "epsilons = [0,0.1,0.01]\n",
        "# bandidos = [\n",
        "#     PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k), k, epsilons[0]),\n",
        "#     PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k), k, epsilons[1]),\n",
        "#     PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k), k, epsilons[2]),\n",
        "# ]\n",
        "bandidos = [PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k),k,ep) for ep in epsilons]\n",
        "entorno = Entorno(k)\n",
        "contador_mejor_accion, recompensas = simulador(intentos, tiempo, bandidos, entorno)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2318d6fb",
      "metadata": {
        "id": "2318d6fb"
      },
      "source": [
        "### Comparación con y sin Optimismo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3beb6ad3",
      "metadata": {
        "id": "3beb6ad3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0dcaf106",
      "metadata": {
        "id": "0dcaf106"
      },
      "source": [
        "### Comparación UCB (extremo superior) vs Epsilon Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0499d2ff",
      "metadata": {
        "id": "0499d2ff"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9b924873",
      "metadata": {
        "id": "9b924873"
      },
      "source": [
        "### Comparación gradiente con o sin baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98415ea5",
      "metadata": {
        "id": "98415ea5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}