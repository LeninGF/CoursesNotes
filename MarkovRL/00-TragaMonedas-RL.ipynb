{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "597b61a1-ccaf-44cf-8c91-9f2f73285a9b",
   "metadata": {},
   "source": [
    "# Problema Tragamonedas\n",
    "\n",
    "Se necesita:\n",
    "\n",
    "- implementar el entorno\n",
    "- implementar las políticas\n",
    "- actualizar la función de valor (promedio)\n",
    "- se usa ascenso del gradiente para actualizar la politica de GBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103b40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange # avance de las iteraciones\n",
    "from typing import List # autocompletar en el notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff539d7",
   "metadata": {},
   "source": [
    "## Clases para actualizar las políticas\n",
    "\n",
    "Política promedio, política con una constante y la del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5704de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActualizarPolitica:\n",
    "    def actualizar(self, politica, accion, recompensa):\n",
    "        pass\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "class ActualizarMediaAritmetica(ActualizarPolitica):\n",
    "    def __init__(self, num_acciones:int):\n",
    "        self.num_acciones = num_acciones\n",
    "        self.contador_accion = np.zeros(num_acciones) # incrementar el contador de la accion que se ha tomado\n",
    "    def actualizar(self, politica:\"Politica\", accion:int, recompensa:float):\n",
    "        self.contador_accion[accion]+=1 # el indice es la accion y asi se incrementa cada accion\n",
    "        # cada accion tiene su propia q\n",
    "        politica.q_est[accion]+=(recompensa-politica.q_est[accion])/self.contador_accion[accion]\n",
    "    def reset(self):\n",
    "        self.contador_accion = np.zeros(self.num_acciones)\n",
    "\n",
    "class ActualizarConstante(ActualizarPolitica):\n",
    "    def __init__(self, num_acciones:int, constante:float):\n",
    "        self.num_acciones = num_acciones\n",
    "        self.constante = constante\n",
    "        self.contador_accion = np.zeros(num_acciones) # incrementar el contador de la accion que se ha tomado\n",
    "    def actualizar(self, politica:\"Politica\", accion:int, recompensa:float):\n",
    "        politica.q_est[accion]+=self.constante*(recompensa-politica.q_est[accion]) # no se observa completo\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ActualizarGradiente(ActualizarPolitica):\n",
    "    def __init__(self, num_acciones:int, con_baseline:bool, constante:float):\n",
    "        self.num_acciones = num_acciones\n",
    "        self.con_baseline = con_baseline\n",
    "        self.constante = constante\n",
    "        self.recompensa_media = 0\n",
    "        self.tiempo = 0\n",
    "        \n",
    "    def actualizar(self, politica:\"Politica\", accion:int, recompensa:float):\n",
    "        self.tiempo += 1\n",
    "        self.recompensa_media+=(recompensa-self.recompensa_media)/self.tiempo\n",
    "        one_hot = np.zeros(self.num_acciones)\n",
    "        one_hot[accion] = 1\n",
    "        if self.con_baseline:\n",
    "            baseline = self.recompensa_media\n",
    "        else:\n",
    "            baseline = 0\n",
    "        exp_h = np.exp(politica.q_est)\n",
    "        prob_accion = exp_h/np.sum(exp_h) # softmax\n",
    "        politica.q_est+=self.constante*(recompensa-baseline)*(one_hot-prob_accion) # no se observa completo\n",
    "    def reset(self):\n",
    "        self.recompensa_media = 0\n",
    "        self.tiempo = 0\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f847c7",
   "metadata": {},
   "source": [
    "## Políticas\n",
    "\n",
    "- GBA\n",
    "- UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be96d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Politica:\n",
    "    def __init__(self, actualizar_politica:ActualizarPolitica, num_acciones:int, iniciales:float):\n",
    "        self.actualizar_politica = actualizar_politica\n",
    "        self.n = num_acciones\n",
    "        self.iniciales = iniciales\n",
    "        self.q_est = np.zeros(self.n)+self.iniciales\n",
    "    def actuar(self):\n",
    "        raise NotImplementedError(\"Actuar no Implementado\")\n",
    "    def avanzar(self, accion:int, recompensa:float):\n",
    "        raise NotImplementedError(\"Avanzar no implementado\")\n",
    "    def reset(self):\n",
    "        self.q_est = np.zeros(self.n)+self.iniciales\n",
    "        self.actualizar_politica.reset()\n",
    "\n",
    "class PoliticaEpsilonGreedy(Politica):\n",
    "    def __init__(self, actualizar_politica: ActualizarPolitica, num_acciones: int, epsilon: float, iniciales: float=0.):\n",
    "        super().__init__(actualizar_politica, num_acciones, iniciales)\n",
    "        self.epsilon = epsilon\n",
    "    def actuar(self):\n",
    "        if np.random.rand()< self.epsilon:\n",
    "            return np.random.randint(0, self.n)\n",
    "        q_max = np.max(self.q_est)    \n",
    "        return np.random.choice(np.where(self.q_est==q_max)[0])\n",
    "    def avanzar(self, accion: int, recompensa: float):\n",
    "        self.actualizar_politica.actualizar(self, accion, recompensa)\n",
    "\n",
    "\n",
    "class PoliticaUCB(Politica):\n",
    "    # extremo superior de confianza\n",
    "    def __init__(self, actualizar_politica: ActualizarPolitica, num_acciones: int, UCB_param:float, iniciales: float=0.):\n",
    "        super().__init__(actualizar_politica, num_acciones, iniciales)\n",
    "        self.UCB_param = UCB_param\n",
    "        self.tiempo = 0\n",
    "        self.contador_accion = np.zeros(self.n)\n",
    "    def actuar(self):\n",
    "        UCB_est = self.q_est+self.UCB_param*np.sqrt(np.log(self.tiempo)/(self.contador_accion+1e-5)) # numero agregado para evitar div por 0\n",
    "        q_max = np.max(UCB_est)\n",
    "        return  np.random.choice(np.where(UCB_est==q_max)[0])\n",
    "    def avanzar(self, accion:int, recompensa:float):\n",
    "        self.tiempo+=1\n",
    "        self.contador_accion[accion]+=1\n",
    "        self.actualizar_politica.actualizar(self, accion, recompensa)\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.contador_accion = np.zeros(self.n)\n",
    "        self.tiempo = 0\n",
    "\n",
    "class PoliticaGradiente(Politica):\n",
    "    def __init__(self, actualizar_politica: ActualizarPolitica, num_acciones: int, iniciales: float=0):\n",
    "        super().__init__(actualizar_politica, num_acciones, iniciales)\n",
    "        self.q_est =np.zeros(self.n)\n",
    "    def actuar(self):\n",
    "        exp_h = np.exp(self.q_est)\n",
    "        prob_accion = exp_h/np.sum(exp_h)\n",
    "        return np.random.choice(np.arange(0, self.n), p=prob_accion)\n",
    "    def avanzar(self, accion: int, recompensa: float):\n",
    "        # super().avanzar(accion, recompensa)\n",
    "        self.actualizar_politica.actualizar(self, accion, recompensa)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ce7c0",
   "metadata": {},
   "source": [
    "## Entorno\n",
    "\n",
    "modelo de las maquinas traga monedas\n",
    "cada maquina tiene una distribucion de probabilidad desconcida\n",
    "las distribuciones de cada maquina empiezan en 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76d1eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entorno:\n",
    "    def __init__(self, num_acciones, recompensa_verdadera=0.0):\n",
    "        self.n = num_acciones\n",
    "        self.recompensa_verdadera = recompensa_verdadera\n",
    "        self.q_verdadera = np.random.randn(self.n)+self.recompensa_verdadera # distribucion normal desplazada\n",
    "        self.mejor_accion = np.argmax(self.q_verdadera) \n",
    "    def reset(self):\n",
    "        self.q_verdadera = np.random.randn(self.n)+self.recompensa_verdadera\n",
    "        self.mejor_accion = np.argmax(self.q_verdadera)\n",
    "    def recompensa(self, accion:int):\n",
    "        return np.random.rand()+self.q_verdadera[accion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf25f4",
   "metadata": {},
   "source": [
    "## Simulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7feac2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterar las politicas , tiempo cuantas iteraciones, intentos \n",
    "k = 10 # num de bandidos\n",
    "\n",
    "def simulador(intentos, tiempo, politicas:List[Politica], entorno:Entorno):\n",
    "    recompensas = np.zeros(len(politicas), intentos, tiempo)\n",
    "    contador_mejor_accion = np.zeros(recompensas.shape)\n",
    "    for p, politica in enumerate(politicas):\n",
    "        for i in trange(intentos):\n",
    "            politica.reset()\n",
    "            entorno.reset()\n",
    "            for t in range(tiempo):\n",
    "                accion = politica.actuar()\n",
    "                recompesa = entorno.recompensa(accion)\n",
    "                politica.avanzar(accion, recompesa)\n",
    "                recompensas[p,i,t] = recompesa\n",
    "                if accion == entorno.mejor_accion:\n",
    "                    contador_mejor_accion[p,i,t] = 1\n",
    "    media_contador_mejor_accion = contador_mejor_accion.mean(axis=-1)\n",
    "    media_recompensas = recompensas.mean(axis=-1)\n",
    "    return media_contador_mejor_accion, media_recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f0763",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "### Comparando Política Greedy para diferentes $\\epsilon$\n",
    "\n",
    "porque el numero de acciones se hace igual al numero de intentos y de bandidos\n",
    "\n",
    "**continuar desde 1:09:06**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc5a29c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '2000' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m bandidos \u001b[38;5;241m=\u001b[39m [PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k),k,ep) \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m epsilons]\n\u001b[1;32m      9\u001b[0m entorno \u001b[38;5;241m=\u001b[39m Entorno(k)\n\u001b[0;32m---> 10\u001b[0m contador_mejor_accion, recompensas \u001b[38;5;241m=\u001b[39m \u001b[43msimulador\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintentos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiempo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandidos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentorno\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36msimulador\u001b[0;34m(intentos, tiempo, politicas, entorno)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimulador\u001b[39m(intentos, tiempo, politicas:List[Politica], entorno:Entorno):\n\u001b[0;32m----> 5\u001b[0m     recompensas \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpoliticas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintentos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtiempo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     contador_mejor_accion \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(recompensas\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, politica \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(politicas):\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '2000' as a data type"
     ]
    }
   ],
   "source": [
    "intentos = 2000; tiempo = 1000\n",
    "epsilons = [0,0.1,0.01]\n",
    "# bandidos = [\n",
    "#     PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k), k, epsilons[0]),\n",
    "#     PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k), k, epsilons[1]),\n",
    "#     PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k), k, epsilons[2]),\n",
    "# ]\n",
    "bandidos = [PoliticaEpsilonGreedy(ActualizarMediaAritmetica(k),k,ep) for ep in epsilons]\n",
    "entorno = Entorno(k)\n",
    "contador_mejor_accion, recompensas = simulador(intentos, tiempo, bandidos, entorno)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318d6fb",
   "metadata": {},
   "source": [
    "### Comparación con y sin Optimismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb6ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dcaf106",
   "metadata": {},
   "source": [
    "### Comparación UCB (extremo superior) vs Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499d2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b924873",
   "metadata": {},
   "source": [
    "### Comparación gradiente con o sin baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98415ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
