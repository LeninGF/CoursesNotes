{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Jack\n",
    "- Coder: Lenin G. Falconi\n",
    "## objetivos\n",
    "- Explorar Muestreo de Importancia\n",
    "- Explorar inicios que exploran\n",
    "\n",
    "## Mecanica del Juego\n",
    "- Dos partes, jugador y dealer\n",
    "- la suma de cartes debe ser $\\leq 21$\n",
    "- gana el que este mas cerca de 21\n",
    "- Empates: misma cantidad\n",
    "- Recompensas: \n",
    "    - si el jugador gana +1\n",
    "    - si empata 0\n",
    "    - si pierde -1\n",
    "- acciones: pedir cartas / plantar\n",
    "\n",
    "## Cartas:\n",
    "- 1 a 10 vale 1 a 10\n",
    "- JQK vale 10\n",
    "- A vale 1 o 11 (estrategia)\n",
    "- As es *util* si usa como 11. Un boolean identifica si el As vale 11\n",
    "## Ejemplo:\n",
    "- Cadena de estados: 2 A 3 A 9 K\n",
    "- `2+11=13`\n",
    "- `13+3=16`\n",
    "- `16+11=27-10` # considero al inicial como 1\n",
    "- `17+9 = 26-10` el otro As se vuelve 1\n",
    "- `16+10 = 26`, perdi\n",
    "## Programa Dealer\n",
    "- Tiene una politica fija\n",
    "- acciones: pedir carta mientras $\\leq 17$\n",
    "- se planta si $suma \\geq 17$\n",
    "- Cartas se reemplazan para el juego.... vuelven todas ..... no se quedan fuera algunas. Esto es para prevenir el conteo o memorizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **acciones:** pedir, plantar\n",
    "- **estado:**`[cartas del jugador, la carta del lider, as_util]`\n",
    "- $\\gamma = 1$, \n",
    "- $G = \\sum R$ el retorno es la suma de recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCION_PEDIR = 0\n",
    "ACCION_PLANTAR = 1\n",
    "ACCIONES = [ACCION_PEDIR, ACCION_PLANTAR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politica Dealer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLITICA_DEALER = np.zeros(22) # cuantas sumas posibles hay\n",
    "for i in range(12,17):\n",
    "    POLITICA_DEALER[i] = ACCION_PEDIR\n",
    "for i in range(17,22):\n",
    "    POLITICA_DEALER[i] = ACCION_PLANTAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politica Jugador\n",
    "se usara metodos dentro y fuera de politica\n",
    "\n",
    "hallar la politica deseada optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLITICA_JUGADOR = np.zeros(22, dtype=np.int64)\n",
    "for i in range(12,20):\n",
    "    POLITICA_JUGADOR[i]= ACCION_PEDIR\n",
    "POLITICA_JUGADOR[20] = ACCION_PLANTAR\n",
    "POLITICA_JUGADOR[21] = ACCION_PLANTAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de la funcion par ala politica objetivo del jugador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politica_objetivo_jugador(as_util_jugador, suma_jugador, carta_dealer):\n",
    "    return POLITICA_JUGADOR[suma_jugador]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de la funcion para la politica de comportamiento del jugador\n",
    "revisar distribucion binomial simula a la moneda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def politica_comportamiento_jugador(as_util_jugador, suma_jugador, carta_dealer):\n",
    "    if np.random.binomial(1,0.5)==1:\n",
    "        return ACCION_PLANTAR\n",
    "    return ACCION_PEDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedir carta\n",
    "\n",
    "accion fija del entorno\n",
    "1 al 10 , jqk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedir_carta():\n",
    "    carta = np.random.randint(1,14)\n",
    "    # jqk se pasan a 10\n",
    "    carta = min(carta, 10)\n",
    "    return carta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor de la carta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valor_carta(carta_id):\n",
    "    return 11 if carta_id ==1 else carta_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion Jugar\n",
    "\n",
    "IN:\n",
    "\n",
    "- politica de jugador\n",
    "- estado inicial\n",
    "- accion inicial\n",
    "\n",
    "OUT:\n",
    "- cadena de estados que ocurrieron durante el juego i.e. trayerctoria de estados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jugar(politica_jugador, estado_inicial=None, accion_inicial=None):\n",
    "    # estadus del jugador\n",
    "    suma_jugador = 0\n",
    "    trayectoria_jugador = []\n",
    "    as_util_jugador = False\n",
    "    # estatus del dealer\n",
    "    carta1_dealer = 0\n",
    "    carta2_dealer = 0\n",
    "    as_util_dealer = False\n",
    "    ###### inicializacion del primer estado \n",
    "    # generar estado inicial si no es dado\n",
    "    if estado_inicial is None:\n",
    "        while suma_jugador < 12:\n",
    "            carta = pedir_carta()\n",
    "            suma_jugador += valor_carta(carta)\n",
    "            if suma_jugador > 21:\n",
    "                assert suma_jugador == 22\n",
    "                suma_jugador -= 10\n",
    "            else:\n",
    "                as_util_jugador |= (1==carta)\n",
    "        carta1_dealer = pedir_carta()\n",
    "        carta2_dealer = pedir_carta()\n",
    "    # empezar el juego desde un estado inicial dado\n",
    "    else:\n",
    "        as_util_jugador, suma_jugador, carta1_dealer = estado_inicial\n",
    "        carta2_dealer = pedir_carta()\n",
    "    estado = [as_util_jugador, suma_jugador, carta1_dealer]\n",
    "    # Iniciar la mano del dealer\n",
    "    suma_dealer = valor_carta(carta1_dealer) + valor_carta(carta2_dealer)\n",
    "    as_util_dealer = 1 in (carta1_dealer, carta2_dealer)\n",
    "    # comprobar si la suma pasa de 21\n",
    "    if suma_dealer > 21:\n",
    "        assert suma_dealer == 22\n",
    "        suma_dealer -= 10\n",
    "    assert suma_dealer <= 21\n",
    "    assert suma_jugador <= 21\n",
    "    # #################\n",
    "    # empezar el juego\n",
    "    while True:\n",
    "        if accion_inicial is not None:\n",
    "            accion = accion_inicial\n",
    "            accion_inicial = None\n",
    "        else:\n",
    "            accion = politica_jugador(as_util_jugador, suma_jugador, carta1_dealer)\n",
    "        # almacenar la trayectoria para utilizar MC y Muestreo de importancia\n",
    "        trayectoria_jugador.append([(as_util_jugador, suma_jugador, carta1_dealer), accion]) # checar vid\n",
    "        if accion == ACCION_PLANTAR:\n",
    "            break\n",
    "        carta = pedir_carta()\n",
    "        # coidgo para contar el numero de As usables en la mano\n",
    "        # el boolean as_utilsolo me dice si es usado como 1 o no\n",
    "        contar_as = int(as_util_jugador)\n",
    "        if carta == 1:\n",
    "            contar_as+=1\n",
    "        # Decidimos si usar nuestro As como 11 o uno\n",
    "        suma_jugador += valor_carta(carta)\n",
    "        while suma_jugador > 21 and contar_as:\n",
    "            suma_jugador -= 10\n",
    "            contar_as-=1\n",
    "        # Jugador Pierde\n",
    "        if suma_jugador > 21:\n",
    "            return estado, -1, trayectoria_jugador\n",
    "        # si no pierdo voy a turno del dealer\n",
    "        assert suma_jugador <=21\n",
    "        as_util_jugador = (contar_as==1)\n",
    "        while True:\n",
    "            accion = POLITICA_DEALER[suma_dealer]\n",
    "            if accion == ACCION_PLANTAR:\n",
    "                break\n",
    "            otra_carta = pedir_carta()\n",
    "            contar_as = int(as_util_dealer)\n",
    "            if otra_carta == 1:\n",
    "                contar_as +=1\n",
    "            suma_dealer +=valor_carta(otra_carta)\n",
    "            while suma_dealer > 21 and contar_as:\n",
    "                suma_dealer -=10\n",
    "                contar_as -=1\n",
    "            if suma_dealer > 21:\n",
    "                return estado, 1, trayectoria_jugador\n",
    "            as_util_dealer = (contar_as == 1)\n",
    "    # en caso de que ningunopierda comparamos parafinalizar la mano i.e. el episodio\n",
    "        assert suma_jugador <= 21 and suma_dealer <=21\n",
    "        if suma_jugador > suma_dealer:\n",
    "            return estado, 1, trayectoria_jugador\n",
    "        elif suma_jugador == suma_dealer:\n",
    "            return estado, 0, trayectoria_jugador\n",
    "        else:\n",
    "            return estado, -1, trayectoria_jugador\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montecarlo con inicios que exploran\n",
    "\n",
    "- porque la dimension $10\\times 10\\times 2\\times 2$\n",
    "- hace evaluacion y control\n",
    "- `contar_pares_estado_accion = np.ones((10,10,2,2))` cuenta cuantas veces un estado determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(episodios):\n",
    "    valores_estado_accion = np.zeros((10,10,2,2)) # (suma_jugador, carta_deleaer, as_util, accion)\n",
    "    contar_pares_estado_accion = np.ones((10,10,2,2)) # empezamos en 1, evadir division para 0\n",
    "    # se actualiza montecarlo con greedy\n",
    "    # politica greedy\n",
    "    def politica_comportamiento(as_util, suma_jugador, carta_dealer):\n",
    "        # no me agrada una funcion dentro de otra funcion. se podria escribir mejor\n",
    "        as_util = int(as_util)\n",
    "        suma_jugador -=12\n",
    "        carta_dealer -= 1  # porque estos decrementos?\n",
    "        # obtener media de los valores\n",
    "        valores_ = valores_estado_accion[suma_jugador, carta_dealer, as_util, :]/\\\n",
    "            contar_pares_estado_accion[suma_jugador, carta_dealer, as_util, :]\n",
    "        # se devuelve valor que ha dado el maximo por cado estado\n",
    "        # random choice rompe empates de manera aleatoria\n",
    "        return np.random.choice([accion_ for accion_, valor_ in enumerate(valores_) if valor_ == np.max(valores_)])\n",
    "    # jugar una cantidad de episodios para visualizar las iteraciones\n",
    "    for episodio in tqdm(range(episodios)):\n",
    "        # para cada episodio empezamos con un (estado/accion) random:\n",
    "        estado_inicial = [bool(np.random.choice([0,1])),\n",
    "                          np.random.choice(range(12,22)),\n",
    "                          np.random.choice(range(1,11))]\n",
    "        accion_inicial = np.random.choice(ACCIONES)\n",
    "        # se usa politica de comportamiento cuando hay episodios sino se inicia con la politica objetivo\n",
    "        politica_actual = politica_comportamiento if episodio else politica_objetivo_jugador\n",
    "        _, recompensa, trayectoria = jugar(politica_actual, estado_inicial, accion_inicial) # type: ignore\n",
    "        check_primera_visita = set()\n",
    "        for (as_util, suma_jugador, carta_dealer), accion in trayectoria:\n",
    "            as_util = int(as_util)\n",
    "            suma_jugador -= 12\n",
    "            carta_dealer -= 1\n",
    "            estado_accion = (as_util, suma_jugador, carta_dealer, accion)\n",
    "            if estado_accion in check_primera_visita:\n",
    "                continue\n",
    "            check_primera_visita.add(estado_accion)\n",
    "            # estamos con gamma = 1 por lo que no hay descuento\n",
    "            valores_estado_accion[suma_jugador, carta_dealer, as_util, accion] += recompensa\n",
    "            contar_pares_estado_accion[suma_jugador, carta_dealer, as_util, accion] += 1\n",
    "    return valores_estado_accion / contar_pares_estado_accion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montecarlo fuera de politica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fuera_politica(episodios):\n",
    "    estado_inicial = [True, 13, 2] # as util, suma jugador, carta dealer\n",
    "    rhos = [] # se acumula para hacer os calculos\n",
    "    retornos = []\n",
    "\n",
    "    for i in range(0, episodios):\n",
    "        # _ significa estado\n",
    "        _, recompensa, trayectoria_jugador = jugar(politica_comportamiento_jugador,\n",
    "                                                   estado_inicial=estado_inicial) # type: ignore\n",
    "        # razon de importancia\n",
    "        numerador = 1.0\n",
    "        denominador = 1.0\n",
    "        # para cada parte de la trayectoria ejecutar el calculo de rhos\n",
    "        for (as_util, suma_jugador, carta_jugador), accion in trayectoria_jugador:\n",
    "            if accion == politica_objetivo_jugador(as_util, suma_jugador, carta_jugador):\n",
    "                # politica objetivo jugador es determinista\n",
    "                denominador *=0.5 # es 0.5 por la probabilidad que se puso 0.5 cara/sello pido/planto\n",
    "            else:\n",
    "                numerador = 0.0\n",
    "                break\n",
    "        rho = numerador/denominador\n",
    "        rhos.append(rho)\n",
    "        retornos.append(recompensa)\n",
    "    rhos = np.asarray(rhos)\n",
    "    retornos = np.asarray(retornos)\n",
    "    # los retornos se han de multiplicar por los rhos para el muestreo ponderado y para elmuestreo normal\n",
    "    retornos_ponderados = rhos * retornos\n",
    "    rhos = np.add.accumulate(rhos)\n",
    "    retornos_ponderados = np.add.accumulate(retornos_ponderados)\n",
    "    # realizar elmuestro ordinario\n",
    "    muestreo_ordinario = retornos_ponderados/np.arange(1,episodios+1)\n",
    "    # muestreo ponderado puede tener divisiones por 0\n",
    "    # uando rho sea 0 el resultado sera 0\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        muestreo_ponderado = np.where(rhos != 0, retornos_ponderados/rhos, 0)\n",
    "    return muestreo_ordinario, muestreo_ponderado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para graficar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_val_mc_es():\n",
    "    # montecarlo politica inicios que exploran valores de estado\n",
    "    valores_estado_accion = monte_carlo_es(5000)\n",
    "    # dos poiliticas 1. as util y 0. no as util\n",
    "    # valores accion estado optimos\n",
    "    valores_estado_no_as_util = np.max(valores_estado_accion[:,:,0,:], axis=-1)\n",
    "    valores_estado_si_as_util = np.max(valores_estado_accion[:,:,1,:], axis=-1)\n",
    "    # politica optima\n",
    "    # opengo la accion que me dio el mejor valor\n",
    "    accion_no_as_util = np.argmax(valores_estado_accion[:,:,0,:],axis=-1)\n",
    "    accion_si_as_util = np.argmax(valores_estado_accion[:,:,1,:],axis=-1)\n",
    "    # Para graficar\n",
    "    imagenes = [accion_si_as_util,\n",
    "                valores_estado_si_as_util,\n",
    "                accion_no_as_util,\n",
    "                valores_estado_no_as_util]\n",
    "    titulos = [\"Politica optima con AS util\",\n",
    "               \"valores optimos con AS Util\",\n",
    "               \"POlitica Optima sin As Util\",\n",
    "               \"valores Optimos sin As Util\"]\n",
    "    _, axes = plt.subplots(2,2,figsize=(40,30))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for imagen, titulo, axis in zip(imagenes, titulos, axes):\n",
    "        fig = sns.heatmap(np.flipud(imagen), \n",
    "                          cmap=\"YlGnBu\",\n",
    "                          ax=axis, \n",
    "                          xticklabels=range(1,11), # type: ignore\n",
    "                          yticklabels=list(reversed(range(12,22)))) # type: ignore\n",
    "        fig.set_ylabel('Suma del Jugador', fontsize=30)\n",
    "        fig.set_xlabel('Mano del Dealer', fontsize=30)\n",
    "        fig.set_title(titulo, fontsize=30)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafica del muestreo ordinario vs el ponderado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muestreo_ordinario_ponderado():\n",
    "    valor_optimo = -0.277726  # este valor dedonde?\n",
    "    episodios = 10000\n",
    "    repeticiones = 100\n",
    "    error_ordinario = np.zeros(episodios)\n",
    "    error_ponderado = np.zeros(episodios)\n",
    "    for i in tqdm(range(0, repeticiones)):\n",
    "        muestreo_ordinario_, muestreo_ponderado_ = mc_fuera_politica(episodios=episodios)\n",
    "        error_ordinario += np.power(muestreo_ordinario_ - valor_optimo,2)\n",
    "        error_ponderado += np.power(muestreo_ponderado_ - valor_optimo,2)\n",
    "    error_ordinario /= repeticiones\n",
    "    error_ponderado /= repeticiones\n",
    "    plt.plot(np.arange(1,episodios+1), error_ordinario, color='green', labels='Muestreo de Importancia Ordinario')\n",
    "    plt.plot(np.arange(1,episodios+1), error_ponderado, color='red', labels='Muestreo de Importancia Ponderado')\n",
    "    plt.ylim(-0.1,5)\n",
    "    plt.xlabel('Episodios (escala logaritmica)')\n",
    "    plt.ylabel(f'Error cuadratico promedio\\n(promedio de {repeticiones} repeticiones)')\n",
    "    plt.xscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpol_val_mc_es\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m muestreo_ordinario_ponderado()\n",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m, in \u001b[0;36mpol_val_mc_es\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpol_val_mc_es\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# montecarlo politica inicios que exploran valores de estado\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     valores_estado_accion \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo_es\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# dos poiliticas 1. as util y 0. no as util\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# valores accion estado optimos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     valores_estado_no_as_util \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(valores_estado_accion[:,:,\u001b[38;5;241m0\u001b[39m,:], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mmonte_carlo_es\u001b[1;34m(episodios)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# se usa politica de comportamiento cuando hay episodios sino se inicia con la politica objetivo\u001b[39;00m\n\u001b[0;32m     25\u001b[0m politica_actual \u001b[38;5;241m=\u001b[39m politica_comportamiento \u001b[38;5;28;01mif\u001b[39;00m episodio \u001b[38;5;28;01melse\u001b[39;00m politica_objetivo_jugador\n\u001b[1;32m---> 26\u001b[0m _, recompensa, trayectoria \u001b[38;5;241m=\u001b[39m jugar(politica_actual, estado_inicial, accion_inicial) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     27\u001b[0m check_primera_visita \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (as_util, suma_jugador, carta_dealer), accion \u001b[38;5;129;01min\u001b[39;00m trayectoria:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "pol_val_mc_es()\n",
    "muestreo_ordinario_ponderado()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
