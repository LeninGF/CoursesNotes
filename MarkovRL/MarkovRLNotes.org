#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Inteligencia Artificial Generativa
#+date: 2024-10-02
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment

* Resources:
- [[https://www.davidsilver.uk/teaching/][UCL Course on RL]]
- [[https://www.youtube.com/watch?v=2pWv7GOvuf0][YouTube videos]]
- [[https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf][Reinforcement Learning Algorithms Csaba Szepesvari]]
  - 
* 2025-03-06 Introducción Aprendizaje por Refuerzo
** Questions and keywords
- Reinforcement Learning :: paradigma de aprendizaje en Machine Learning
- Machine learning :: es una disciplina de la Inteligencia Artificial
  en donde se busca que una maquina aprenda a través de datos
  históricos sin necesidad de una programación explícita.
- paradigma :: marco conceptual que engloba una manera de representar
  o concebir el dominio de un problema. Machine Learning se puede
  considerar un paradigma en la medida que propone una manera
  alternativa de abordar el desarrollo de sistemas informáticos
  (e.g. sin una programación explícita). Reinforcement Learning
  también se puede considerar un paradigma de los tipos de
  aprendizaje, ya que a diferencia del supervisado y no supervisado,
  RL esencialmente se interesa por la interacción de un agente en un
  entorno.
- Agente :: programa que aprende por prueba y error obtiene una manera
  óptima de realizar una tarea
- Inteligencia Artificial ::  cualquier máquina (software) que imita
  /función/ que realiza un ser humano. Campo de las ciencias de la
  computación que busca crear sistemas capaces de resolver tareas
  mediante el aprendizaje a través de datos. Característica que le
  permite funcionar como una inteligencia de origen no humano. Si bien
  puede imitar ciertas capacidades humanas, su objetivo no es replicar
  la inteligencia humana. Más bien creo que lo mejor es la
  *identificación automática de patrones* en los datos.
- programación dinámica :: enfoque para resolver problemas de
  optimización complejos mediante la división en subproblemas más
  pequeños y superpuestos donde las soluciones se almacenan para
  evitar recalculo innecesario. Depende de memorización o tabulación.
- planeación al futuro :: se refiere a sistemas que cambian con el tiempo
- aprender con sistema online :: el sistema aprende y se adapta de
  manera continua mientras interactúa con su entorno o recibe nuevos
  datos. Actualiza su modelo en tiempo real. En general, la aplicación
  se da en sistemas donde el modelo debe actualizarse con los datos
  del entorno ya que los patrones de los datos cambian con el tiempo.
- sisteam offline :: aprende con un conjunto de datos estático y luego
  se despliegan. Su mejora se hace enfrentando nuevamente ajustes a
  través de datos estáticos tipo snapshot.
- no me queda claro que se pueda usar en sistemas de recomendación RL ::
- escenario o entorno ::
- comportamiento emergente ::
- cómo se enlazan las acciones de los agentes con los dispositivos electrónicos :: en
  clase se habla de controlar un volante, podría ser una válvula, cómo
  traduzco lo que me devuelve el agente en señales que puedan ir a los
  actuadores
- estado :: ¿tiene relación con los automatas finitos o maquinas de estado?
- rolling horizonte:: ????
- cuál es el dilema de la exploración y la explotación :: en RL
- puede RL enfrentar problemas NP completos como el de la mochila o el
  traveling salesman
- se puede usar RL para problemas de optimización lineal :: replantear
  la recompensa para que las soluciones esten dentro del conjunto de enteros
- es muy indicado para problemas distribuidos en nodos
- valor esperado :: esperanza pero asumio como el promedio para el
  problema o media aritmética
- esperanza :: 
** Notes
- RL es un paradigma de aprendizaje de machine learning.
  - No hay un supervisor sino una señal de recompensa o reward
  - El feedback o retroalimentación es retrasado
  - El tiempo importa. Los datos son secuenciales no de tipo  i.i.d.
- No hay instrucciones explícitas
- las reglas se descubren
- las estrategias desarrolladas para RL son métodos iterativos
  esencialmente
- Está relacionado con teoría del control(óptimo) y programación
  dinámica
- recibe tanto conceptos de machine learning como de deep learning
- Machine learning clásico:
  - KNN
  - Regresión
  - Clasificadores (e.g. regresión logística)
  - SVM
- Aprendizaje clásico no tiene la idea de *planeación a futuro*
- Cuándo usar RL:
  - EL se usa cuando se puede aprender con el sistema online
  - RL se usa cuando el tiempo importa y los procesos no son estáticos
    sino que cambian con el tiempo
  - RL se usa cuando los problemas son de horizonte continuo/infinito
    en el tiempo
- Por ejemplo sistemas de temperatura que varán con el tiempo
- Sistemas de recomendación manejan aprendizaje por refuerzo
- El objetivo del RL es *maximizar la recompensa esperada a largo plazo*
- es encontrar un conjunto de acciones que de la máxima recompensa
*** Formalismo
- Agente: aplicación o software o maquina que **interactúa** con un
  **entorno**
- El agente ejecuta acciones.
- El entorno: es una descripción del sistema (i.e. sensores)
- El estado: descripción exhaustiva por el entorno
- Recompensa: manera de evaluar que tal va
- Tareas: hay tareas continuas e.g. manejar el automóvil. La suspende
  el usuario Otro ejemplo sistemas HVAC de control de temperatura
- Tareas episódicas: son tareas con objetivos que se debe cumplir en
  ciertas iteraciones y repetimos hasta obtener buen
  rendimiento. E.g. video juegos
- Entornos:
  - parcialmente observables: poker con el paso del tiempo gano
    información pero hay limitación
  - completamente observable: e.g ajedrez. Cada que se mueve las
    piezas cambia el estado
- La recompensa se puede ver en un objetivo principal que puede
  dividirse en pequeñas recompensas. La suma de recompensas lleva al
  objetivo de ganar. Pero hay casos en que puede existir un sacrificio
  de mini objetivos con el objeto de ganar.
- la planificación es sacrificar pequeñas recompensas con la idea de
  obtener o asegurar la victoria
- Sobre las recompensas, todo objetivo se puede expresar como la
  maximizaron de una recompensa acumulada *esperada*
- se debe encontrar una manera de expresar el problema a resolver en
  función de una recompensa.
- recompensa acumulada -> planeación
- recompensa acumulada esperada -> valor esperado (probabilidades)
*** Ejemplos de recompensas
- hvac: medir el confort edel usuario maximizar si se gasta mucha
  energia puede ser negativo
- en video jeugo si gano es + si pierdo -
- en la conducci[on autonoma si choco es - si obtengo ruta optima es +
- chat gpt +- exp del usuario
- que es una estructura markov. eg. el mercado no es markov
*** Laboratorio
- el agente parte de un estado 0 $s_0$ y toma una acción inicial $A_0$
- el entorno devuelve la recompensa $R_1$ y un estado $s_1$
- el agente descubre por recompensas (refuerzo +/-) como obtener una
  cadena de secuencias que me lleve al objetivo
- los episodios acumulan varios cadenas de estados
  $EP_1:s_0A_0R_1, s_1A_1R_2$
*** Componentes del Agente
- política :: conjunto de acciones/decisiones ?permitidas?  que el
  agente puede tomar en un tiempo
- función de valor del Estado $v$ ::
- función de valor de acción estado $q$ ::
- $v$ y $q$ usan la recompensa para cuantificar que tan bien me va
- es posible que no existan ambas a la vez pero sí una polítca 
*** Política $\pi$
- Es un mapa desde los estados a las acciones
- El objetivo es encontrar la política óptima
- la política óptima maximiza la recompensa a largo plazo
- $\pi(s) = a$ deterministica donde las acciones son siempre las mismas
- $\pi(a|s) = P[A_t=a|s_t=s]$ Estocástica
*** Ejemplos Introductorios
- Maquina bandidos multi brazo: cada maquina recibe una moneda y da
  una recompensa que puede ser 0 o varias monedas. Cada máquina tiene
  una palanca. La hilera o arreglo de maquinas traga monedas es un
  agente. Cada una tiene una distribución que no será estimada. Se
  asume que hay infinitas monedas. Mi objetivo es maximizar el número
  de monedas. Acciones: halar la palanca. Es un problema incompleto en
  que cada episodio es más menos la misma dinámica. Es parecido al
  sistema de recomendación de redes sociales. encontrar la palanca que
  me de la maxima recompensa por ejemplo tiempo en pantalla para
  lucrar. Otra aplicación de este problema es elegir un servidor para
  mi cloud computing. En cuál la q es más pequeña para que el proceso
  tome menos tiempo.
- bandido es un traga monedas
- ensayos clínicos: se expone a pacientes a diferentes medicinas
  e.g. 3 y lo que se pretende es saber cual sana mas. 
- explotación vs exploración
*** Valores de acción estado $q$
- $k$ número de acciones
- recompensa estocástica
- la recompensa viene despues de la acción
- $q$ es la suma de recompensabas cuando tome una acción $a$ para el
  total de acciones
*** política
- la política puede usar un concepto Greedy aunque en realidad no se
  usa por limitaciones que tiene este concepto en la práctica en
  cuanto a la situación de exploración y explotación
- la política voraz parece limitar la exploración
- la política greedy ayuda a ilustrar las decisiones pero no se usa ya
  que anula la exploración
*** exploración vs explotación
- hay opciones más optimas?
- cual es el balance entre la explotación y exploración
- es decir si se hace mucha exploración se va a mucha incertidumbre en
  el sistema
- mucha explotacion ie. tomar la misma decision una y otra vez puede
  llevar a que me quede en una solución suboptima o mínimo local tal
  vez será lo que lleva el greedy
** Summary
Reinforcement Learning (RL) o aprendizaje por refuerzo es un paradigma
de aprendizaje en Machine Learning (ML) en donde se busca **maximizar
la recompensa esperada a largo plazo**. Para esto se usa un *agente*
que realiza *acciones* permitidas de acuerdo a una *política* sobre un
*entorno* y recibe una señal de retroalimentación retardada que es la
*recompensa*. Se caracteriza por afrontar problemas secuenciales ya
que el patrón de los datos cambia con el tiempo. Por esta razón
requiere de un sistema online (i.e. adaptación continua)

Machine learning es un paradigma que desarrolla programas que no
tienen instrucciones explícitas sino que aprenden mediante prueba y
error a optimizar una tarea. RL está como una técnica entre Machine
Learning y Deep Learning (DL). Está relacionado con /Control Óptimo/ y
/Programación Dinámica/.

Una característica importante de RL es que es un modelo que permite
una planeación futura. Algoritmos clásicos de ML como KNN, SVM, Redes
Neuronales,usados en problemas de clasificación y clusterización,
resuelven tareas específicas sobre un conocimiento histórico de datos
estático. Se puede decir que son esencialmente de tipo /offline/. Es
decir, aprenden de los datos obtenidos del sistema. RL es más bien de
tipo sistema Online, es decir aprende conectado al sistema. Extrayendo
datos y resultados de la interacción del agente con el entorno. Por
esto para ML clásico no importa el tiempo. En cambio en RL el tiempo
importa y de ahí que el foco de problemas de RL son procesos no
estáticos en el tiempo. Sin embargo, surge una duda. Un ejemplo de
sistemas de control son los HVAC. Sin embargo, hay que discutir la
diferencia entre predecir la siguiente temperatura (i.e. regresión)
considerando una colección de datos de temperatura y el que un agente
desarrolle técnicas para controlar la apertura y cierre de una
válvula. Podría ser una válvula controlada por PWM para mantener la
temperatura constante. RL en este sentido no se usa para predecir la
siguiente temperatura más probable. Ahora encuentro un problema,
podría usar ML clásico para predecir, supongamos el ángulo de disparo
de tiristores para controlar la temperatura? En este caso necesitaría
abordar el problema desde un esquema supervisado. Por qué no debería
hacerse con ML clásico este problema? Se nota que ML clásico no le
importa el tiempo. Sólo le importaría la relación entre el dato de
entrada y el de salida.

Algunos conceptos a considerar son:
- Entorno :: mundo o sistema en el que el agente se desenvuelve. Puede
  ser físico o virtual. Es una descripción exhaustiva del
  sistema. Puede ser determinista (i.e. misma acción y mismo estado
  producen el mismo resultado) o estocástico. Puede ser observable
  (ajedrez) o parcialmente observable (poker)
- Estado :: situación del entorno en un momento $t$. Puede ser
  discreto (rango finito de posibilidades)o continuo (rango infinito
  de posibilidades).
- Acción :: es la decisión que el agente toma para interactuar con el
  entorno. Pueden ser discretas o continuas.
- Episodio :: secuencia completa de interacciones entre un agente y su
  entorno, desde un estado inicial hasta un estado terminal.
  $Ep_1:S_0A_10R_1, S_1A_1R_2\dots$
  $Ep_1:S_0A_10R_1, S_1A_1R_3\dots$
- Política :: secuencia de decisiones que el agente ejecuta en una
  instancia de tiempo. Se relaciona con dos funciones para medir el
  éxito del agente frente al objetivo. Estas funciones usan la
  recompensa y son $\nu, q$. La política *mapea estados a
  acciones*. Define el comportamiento del agente. Le dice al agente
  que acción puede tomar en cada estado. Puede ser determinista o
  estocástica (distribución de probabilidad). $\pi(a|s)$: la
  probabilidad de tomar la acción $a$ en el estado $s$.
  - Política determinista: $\pi(s) = a$
  - Política estocástica: $\pi(a|s)  = \mathbb{P}[A_t=a|s_t=s]$
- Función valor de Estado $\nu$ (state value function):: estima cuán
  bueno es un estado para el agente en términos de la recompensa
  acumulada esperada a largo plazo. Mide la recompensa total que un
  agente puede esperar recibir a partir de un estado dado, siguiendo
  una política específica.
- Función de valor Acción Estado $q(s,a)$ (action-state value
  function):: estima que tan bueno es tomar una acción específica en
  un estado dado en términos de la recompensa acumulada esperada a
  largo plazo. Es la recompensa total que el agente puede esperar
  recibir al tomar una acción específica en un estado dado, y luego de
  seguir una política específica.


El bucle de RL es:
1. agente percibe el estado del entorno
2. agente elige una acción basada en el estado
3. el entorno responde a la acción y proporciona un nuevo estado y una
   recompensa
4. el agente aprende mediante el ajuste de la política de acciones
   para maximizar la recompensa.

*** Ejemplo de discusión
Consideremos un sistema de control de temperatura formado por una
válvula que inyecta el gas. La apertura de la válvula se controla
mediante una señal PWM. ¿Se puede resolver por ML clásico?

En una formulación ML clásica, se podría pensar que el ángulo de PWM
es una regresión. Para esto debería tener previo un registro de cuanto
incrementa la temperatura para un incremento del ángulo. Sin embargo,
se nota dos problemas:
1. si cambia el set point de control se tiene que registrar nuevos datos
2. no se considera en sí el cambio con el tiempo ni el estar conectado
   al sistema. Se observa que tratar como problema de regresión obliga
   a pensar en disponer de una tabla de datos para inferir un modelo
   que devuelva en cuanto abrir y cerrar la válvula dada la
   temperatura como entrada.
3. es difícil de modelar porque tendría que aislar el sistema
   térmicamente para obtener datos y tal vez nunca termine de exigir
   el disponer de datos. Es más, con el tiempo el mismo ángulo de pwm
   puede generar diferentes curvas de calentamiento i.e. se puede
   alcanzar el set point a distintos tiempos.
*** Objetivo de RL
Esencialmente es maximizar la recompensa esperada a largo plazo. Para
esto un enfoque es obtener una suma de recompensas. Así se puede
acumular pequeñas recompensas que acerquen al objetivo a largo
plazo. En este proceso, puede darse un sacrificio en donde no siempre
se busca maximizar la recompensa en cada paso si al hacerlo se puede
tener una recompensa a largo plazo. Esto es la planeación.

#+begin_quote
Todo objetivo puede ser expresado como la maximización de una
recompensa acumulada esperada.
#+end_quote

De ahí que es de interés describir los problemas y objetivos en
términos de la recompensa.
* 2025-03-11 Políticas
** Questions and keywords
- función de valor de acción ::
- intervalo de confianza ::
- target ::
- oldEstimate ::
- optimismo esta relacionado con el sesgo de inicio :: Entiendo que si
  existe un condicionamiento de condiciones iniciales, el problema de
  aprendizaje podria no converger? o era que el optimismo garantiza
  una exploracion inicial, pero si el problema es no estacionario
  i.e. varia con el tiempo, ya no va a buscar nuevas mejores acciones
  teniendo un mal desempe;o a largo plazo. En cambio si el problema es
  estacionario, el optimismo puede funcionar bien.
** Notes

*** Política Voraz (Greedy)
- no se puede utilizar en RL completo. Trata de explotar el
  conocimiento lo que le limita la capacidad de explorar nuevas
  acciones/politicas que mejoren a largo plazo ya que trata de obtener
  la mejor accion inmediata. Depende de disponer de una matriz de
  transicion de estados.
- una política maximiza la función de valor de acción
- greedy para una $A_t$ se aplica $1-\epsilon$
- así por ejemplo $1-\epsilon \right_arrow 80\%$ tomo una acción que
  maximice la recompensa es decir una accion muy conveniente
- el 20% de las veces $\epsilon$ hago algo nuevo (exploración)
- aleatorio significa que las acciones se toman con una distribución
  uniforme y todas las acciones con misma probabilidad
- la función de valor de la acción en un estado en tiempo $t$ es
  $Q_t = \frac{R_1+R_2+\dots+R_{t-1}}{t-1}$
- Para que $Q$ sea eficiente se plantea como una *implementación
  incremental del promedio*. Este promedio de recompensas da idea de
  $Q$
- $t$ se entiende como el número de veces que se toma una acción $a$
- Entonces el $Q$ queda como:
  $Q_{t+1}=Q_t+\frac{1}{t}(R_t-Q_t)$
- esto permite mantener en memoria valores anteriores
- en general, las estimaciones se entienden por:
  $N_{est} = Old_{est}+\alpha(Target - Old_{est})$
- el $\alpha$ está entre 0 y 1 y es el factor de olvido
- hay que definir cual es el margen de tiempo relevante para mi
  problema
- es una ventaja asumir $\alpha$ constante, porque de esa manera puedo
  hacer que en las elecciones del agente pese mas la informacion
  reciente que toda la informacion
- $Q(a) \neq 0$ inicializado con valor optimista ayuda a la exploración
- optimismo implica saber que es ser potimista en el contexto del
  problema i.e. conocer la recompensas del problema
- el optimismo funciona al inicio
- en proble no estacionarios optimismo no funciona bien

*** politica UCB extremo superior de confianza
- hacer estimaciones o calculos dentro de un intervalo de confianza
- ucb aprovecha la información que da la función de valor de la acción
  Q
- la siguiente acción es *potencialmente* la mejor
- el valor de Q se asume dentro de un intervalo de confianza, en un
  sentido optimista pegado al borde superior
- el denominador de la UCB da intervalo de confianza
- la UCB se define por
  $A_t = \underset{a}{\text{argmax}}(Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)})$

*** Algoritmos de Bandidos de Gradiente GBA
- usa soft max
- la funcion softmax da varias probabilidades
- se escoje la mayor
- la función de preferencia $H_t$ no está relacinada con la recompensa


*** Arrepentimiento
- se explica con el ejemplo de TikTok
- un arrepentimiento bajo garantiza en un tiempo crítico mantener al
  cliente enganchado
- crítico es rápido es decir que con un límite de iteraciones obtener
  el resultado

*** Bandidos Contextuales
- el ejemplo de bandidos se asemeja a cosas de marketing
- no solo se tiene información de la recompensa
- sino también se incluye información histórica de episodios pasados
  del usuario
- usar informacion histŕoica nos pone en la intersección entre
  aprendizaje por refuerzo y RL
- 
** Summary

* 2025-03-11 Procesos de Markov
** Questions and keywords
- cadenas de markov ::
- procesos con recompensa de markov MRP :: 
- procesos de markov MP ::
- planeación :: las acciones tienen consecuencias
- se puede modelar un ajuste P&ID de una planta con RL ::
- un virus con RL que podría hacer si el agente aprende a controlar el entorno ::
- filtros de kalman ::
- máquina de estado finito ::
- matriz de probabilidad de transición de estado :: $p_{ss'}=P[s_{t+1}=s'|s_t=s]$
- retorno ::
- no es claro el tema del valor del estado vs recompensa vs mi objetivo ::
- ley de esperanzas iteradas ::
- diagrama de respaldo :: 
** Notes
- bandidos es un problema incompleto con una sola acción
- en bandidos la única acción no influye en la recompensa
- accionar la palanca no afecta al futuro
- el problema del conejo lleva a recordar el problema de canivales que
  se usa con BFS
- el agente es el controlador
- el entorno es el proceso
- el entorno provee un feedback
- el agente aprende a controlar el entorno
*** Procesos de Decisión de Markov MDP
- El proceso de decisión de Markov esta compuesto de acciones, estado,
  recompensas
- la recompensa es algo que el diseñador modela y no es real
- hay varias variables que no se conocen. El RL se basa en cantidades
  estimadas
- En MDP:
  1. ningun elmento es conocido por el diseñador
  2. se trabaja con cantidades estimadas
  3. el ámbito puede ser observable completamente o no el
     entorno. Pero se considerará completamente observable en un inicio
- hay los siguientes procesos: 
- Procesos Markov que es una tupla <s,p>
- Proceso de recompensa de markov <s,p,r,gamma> gamma es el olvido hay
  una referencia a control
- proeceso de decision de markov es la tupla completa <s,A,p,r,gamma>
*** Procesos de Markov
- El estado actual contiene toda la información necesaria (propiedad de Markov)
- los procesos de markov son procesos sin memoria
- la probabilidad de estimar el mismo estado es la misma que la del
  acumulado de estados $P[s_{t+1}|s_t] = P[s_{t+1}|s_t, s_{t-1}, \dots, s_0]$
- el paso entre estados en un automata finito se la hace con
  probabilidades de transición
- cómo puedo estimar la matriz de probabilidad de transicion
- es el proceso el que me lleva a determinar las probabilidades
*** Proceso de recompensa de markov
- es una tupla de estados matriz de transicion recompensa y un factor
  de olvido, donde los estados es un conjunto finito, p la matriz de
  transición y la recompensa se define como una función que calcula el
  valor esperado dado que me encuentro en un determinado estado
- aun no se considera la acción pero de momento se obvia
- $\gamma$ es el factor de olvido en control adaptativo
- $\gamma$ es el factor de descuento con valores de 0 a 1.
- objetivo de RL es obtener la Recompensa maxima acumulada esperada
- el objetivo de RL se define como *retorno*
- los valores del problema de RL como el retorno se obtienen de los
  estados no de manera histórica
- el descuento balancea cuanto quiero tomar en cuenta el futuro
- $\gamma = 0$ es ser Miope hacia los valores futuros sólo me interesa
  lo que pasa en el siguiente estado no a futuro
- $\gamma = 1$trato de incluir lo más que pueda la información futura
- $\gamma= 1$ es como estimar lo más lejos en el tiempo pero esto
  genera problema para estimar adecuadamente. se vuelve mas incierto
*** El descuento
- desde un punto de vista matemático para evadir problemas de
  recompensa infinita es necesario tener un factor de descuento
- introduce modelado de la incertidumbre
- es un balance entre recompensas a largo y corto plazo (inspiración
  financiera o psicologica)
- $\gamma = 0.01$ implica centrarse en recompensas inmediatas
*** Función del valor del estado $v(s)$
- indica el valor a largo plazo del estado en el que me encuentro
- mientras más alto es mejor
- matemáticamente es el retorno esperado empezando desde el estado $s$
- Ejemplo
  - Se requiere una secuencia de estados
  - $\gamma = 0.5$
- se requiere hacer mas cadenas
- se toma el mejor valor posible de la esperanza (promedio)
*** Ecuación de Bellman
- se usa la ley de esperanzas iiteradas para
- el valor esperado se asume lineal
- el valor esperado no tiene un solo valor
- lo anterior se asume
- la Recompensa inmediata es la respueta del sistema (repsuesta a entrada paso?)
- esta ecuacion es ver un paso adelante
- es una estimacion o busqueda de un paso al futuro
*** Diagramas de Respaldo
- se usan para ver de manera grafica el proceso de estado accion recompensa
- lineas indican la trayectoria seguida
- burbujas que indican estados
- puntos negros que indican acciones
- Mediante el uso de algoritmos se trata de obtener los diferentes
  valores que describen el proceso
- la solución de los valores de estado se pueden resolver por medio de
  ecuaciones lineales
** Summary

* 2025-03-12 Procesos de Decisión de Markov
** Questions and keywords
- Decisiónes de Markov ::
- diagrama de respaldo :: ayuda a seguir el proceso que sigue el
  algoritmo. Cambia segun las acciones a tomar
- MDP  :: markov decision process??? 
- MRP :: markov reward process???
- El retorno ::  donde se definió
- demostracion de la ley de esperanzas iterativas ::
- recompensa inmediata :: $R_{t+1}$
- valor con descuento del siguiente estado :: $\gamma q_{\pi}(s_{t+1},
  A_{t+1})$ y $\gamma v_{\pi}(s_{t+1})$
- politica uniforme :: todos los estados tienen la misma
  probabilidad. Si tengo dos estados cada estado tiene 0.5 si son 3
  entonces 1/3
** Notes
- Los procesos de decision markov es una tupla comprendida de los
  estados, las acciones, la matriz de probabilida de transición de
  estados, la recompensa y el descuento $<S, A, P_{ss'}, R, \gamma>$
- Pss' será redifinida porque depende de la acción
- $P_{ss'}^a\mathbb{P}[s_{t+1}=s'|s_t=s, A_t=a]$
- La recompensa sera tambien en funcion del etado y la accion
- $R_s^a = \mathbb{E}[R_{t+1}|s_t=s, A_t=a]$
*** Políticas Estocásticas
Una política es una distribución de aciones dado un estado en el que
me encuentro en un tiempo t

$\pi(a|s)=\mathbb{P}[A_t=a|s_t=s]$
- La política define el comportamiento
- usa el estado actual o estado markoviano.
- son estacionarios en el tiempo $t$
- En cada episodio la política es invariante en el tiempo. por ejemplo
  puede variar $\epsilon$ greedy con el tiempo pero no la política.
- para un proceso de decision de markov MDP que consta de $<S, A,
  P_{ss'}, R, \gamma>$ + Política $\pi$ se redefine la matriz de
  transición y la recompensa como

  $P_{ss'}^{\pi} = \sum_{a \in A}\pi(a|s)P^a_{ss'}$
  $R_s^{\pi} = \sum_{a \in A}\pi(a|s)R_s^a$
*** Valor de Estado $v(s)$ y Valor de Acción Estado $Q$
Se redefinen con respecto a la política. $n$ es el número de estados y
$m$ el número de acciones

Función valor estado: es el retorno esperado del estado siguiendo la
policía $\pi$ (valor esperado) G es el retorno

$V_{\pi}(s) = \mathbb{E}_{\pi}[G_t|s_t=s] \in \mathbb{R}^n$

Funcion Valor accion estado> es el retonro esperado G despues de tomar
la acción $a$ en el estado $s$ usando la política $\pi$

$q_{\pi}(s)=\mathbb{E}_{\pi}[G_t|s_t=s, A_t=a] \in \mathbb{R}^{n \times m}$
*** Ecuación de la esperanza o valor esperado de Bellman
Es la recompensa del siguiente esatdo mas el valor condescuento del
siguiente estado

$v_{\pi}(s) = \mathbb{E}[R_{t+1}+ \gamma v_{\pi}(s_{t+1})|s_t=s]$

$q_{\pi}(s,a) = \mathbb{E}[R_{t+1}+ \gamma q_{\pi}(s_{t+1}, A_{t+1})|s_t=s, A_t=a]$


La secuencia que se ejecuta siempre es S -> A -> R es decir en un
estado tomo una accion y recibo una recompensa. Este bucle se repite.

Estando en un estado s y tomando una accion a puedo basado en la
accion puedo estimar el q del estado y la acción

$$v_{\pi}(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$$


$$q_{\pi}(s,a) = R_s^a+\gamma\sum_{s'\in s}P_{ss'}^av_{\pi}(s')$$
- dependiendo de las acciones que toomo el estado destino no siempre es fijo
- el entorno decide a que estado llego desde una acción
- no se sabe si las acciones me llevan a la misma respuesta
- la q pi sa se puede reemplazar en la v pi s
- revisar resultado en diapositaiva 9 de grabacion de 12 de marzo
*** Ejercicio 12 de marzo diapositiva 10
- politica uniforme
- $\gamma = 1$
- se busca hallar el valor en el estado c3
** Summary
* 2025-03-13 Programación Dinámica
** Questions and keywords
- problemas de evaluación MDP ::
- problema de control de un proceso de decsision de markov MDP ::
- porque la recompensa no es conocida :: pense que si era conocida
  porque ponemos el objetivo en terminois de la recompensa
- proceso iterativo ::
- equacion esperanza bellman ::
- ecuación optimalidad de bellman ::
- principio de optimalidad :: que existe una subestructura en la que
  podemos dividr el problema. los subproblemas son superpuestos,
  reusables e iterables. Puedo iterar una funcion para ir entre
  cualquier nodo del problema
- evaluacion sincrona ::tiene dos copias d ela funcion de
  valor. cuando termna el for loop tiene el nuevo valor . EN cada
  estado se evalua el valor de v y en cada iteracion se ajusta el
  valor de la funcion v
- evaluación en su lugar (in place) ::
- checkar grid world dynamic programing demo standord ::
- evaluación política :: es la predicción. consiste en cuantizar que
  tan buena es la política.
- PD :: es un paradigma de optimización propuesto por Bellman que
  consiste en dividir un problema grande en subproblemas.
- problema de planeacion :: es hacer la prediccion y el control
  . donde prediccio es calcular $v$ y $q$ y el control obtener $\pi_*$
- iteracion de politica ::  en cada itereacion encuentro una neva
  politica $\pi'$ que es la greedy, es decir la accion que me dio el
  valor maximo de los estados.
- teorema de mejoramiento de la politica :: 

** Notes
- Un proceso de decisión de Markv tiene $S, A, P, R, \gamma + \pi$
- En la realidad la recompensa y la matriz de transición y la
  recompensa son desconocidas. A excepción de que estemos modelando.
- En esta clase se asume que $P$ y $R$ son conocidos.
- la evaluación de la política es cuantizar que tan buena es una política
- evaluación politica input: \pi
- output de la evaluacion politica v_{\pi}(s), q_{\pi}(s,a). Que son
  las funciones de valor. A través de ellas medimos.
- El problema completo de RL tiene dos partes Evaluación de POlítica
  que es la prediccion y la mejora de la política (control)
- la mejora de la política o control trata de encontrar una política
  óptima $\pi_*$. Esto es maximizar la recompensa.
- El proceso de politica optima tiene por IN: MDP y como salida la
  política optima $\pi_*$y los estimados de $v_*$ y $q_*$
- resolver el problema completo de RL no es sencillo así se conozca
  $P_{ss'}$ y $R$. Para resolver este problema se recurre a la
  Ecuación de la esperanza de Bellman. 
- el problema de la ecuacion de esperanza de bellman es que tiene un
  costo computacional alto. Sobre todo para problemas reales. Porque
  el costo computacional esta relacionado al tamaño lineal del
  sistema. Por esta razón se requiere procesos iterativos-
- La ecuación optimalidad de bellman es no lineal y no tiene solución cerrada
- para estimar la ecuacion de esperanza de bellman y la optimalidad de
  bellman es necesario procesos iterativos. Es una manera para ayudar
  a  resolver la ecuación de Bellman
- Bellman propone la programación dinámica para resolver
  computacionalmente las ecuaciones reduciendo el costo computacional.
- Por ejemplo si se quiere ir de NY a los Angeles pasando por Chicago,
  y en cuentro dos caminos Optimos entre NY-CH y CH-LA, entonces el
  optimo general es la suma de los optimos mas pequeños
- consiste la prog dinamica en dividir el problema ensubproblemas o suboptimos
- unir dos caminos optimos o rapidos debe garantizar una entrega mas rapida
- no es seguro que el óptimo global sea la suma de los suboptimos
- la programacion dinamica consiste en
  1. dividir en subproblemas: la subestructura de un problema debe ser
     visible o ya existen. Si la subestructura existe cumplimos con el
     principio de optimalidad
  2. resolver subproblemas
  3. unir las soluciones
- condiciones para plantear la prog dinámica
  1. debe existir una subestructura i.e. el problema es divisible
  2. la substructura debe cumplir elprincipio de optimalidad
  3. los sub problemas deben ser superpuestos lo que permite re
     utilzar las soluciones i.e. son reusables e iterables
- Los diagramas de estado accion muestran que RL tiene una sub
  estructura, ya que una accion nace de un estado y llegar a otro
  estado fue consecuencia de una accion. i.e. la ecuación se
  mantiene. Asi la funcion $v_t$ esta realacionda con la $v_{t+1}$. Es
  mas hay una solucion lineal para v haciendo $[v]=[A^{-1}][b]$
- El RL es:
  - estructurado porque tiene subproblemas o substructuras e.g. el
    diagrama de respaldo
  - puede ser iterado

- es riesgoso acercarse a estados con valores negativos
*** Problema de evaluacion o prediccion
- se denomina por prediccio porque se trata de estimar que va a pasar
  en el siguiente estado. Pero la ecuacion se escribe de una manera
  invertida. Por esta razon para que tenga sentido se convierte en una
  regla de actualizacion:
  $V_{k+1}(s)=\sum_a\pi(a|s)\sum_{s,r}P(s',r|s,a)[r+\gamma V_k(s')]$
- Aqui hay que considerar que $k+1$ es el estado que estoy viendo y
  $k$ el que transcurrio es una manera programatica de verlo
- Se inicializa la funcion con valores optimos $v_0$ cuando $k \to
  \infty$ alcanzamos la funcion real: $v_k \to v_{\pi}$
- La evaluacion de la politica es aplicar la ecuacion de bellman
*** Implementacion de la evaluacion
- evaluacion sincrona: 
- evaluacion in place o en su lugar: se ajusta cuando hay cambios en
  el barrido de estados. inmediatamente se ajusta la funcion de
  estado. por eso ajusta en el mismo lugar. no espero a ajustar cuando
  todos los estados cambien. converge mas rapido es mejor en la practica
- evaluacion sincrona tiene 2 copias de la v_{\pi} va de acuerdo con la matematica
- evalucion sicron aactualiza fuera del loop.
- en su lugar es que cuando cambio uno de los valores cambia, entonces
  inmediato cambio todos los valores. es mas rapida. Es decir si tres
  estado de 50 cambian, cambio los tres y la diferencia se queda igual.

*** Ejemplo numerico de calculo con PD
- algoritmo de valuacion:
  - In: $\pi$,
  - $\delta$ es un numero pequeño
  - inicializar los estados $v(s)$ con algún numero excepto los estados terminales.
  - en estaos terminales se da el valor de $v(terminal)=0$ Un control de
  temperatura no tiene un estado terminal termina al rato de desconectar la maquina
- los valores iniciales de $v(s)$ pueden usar optimismo
- en el video de la clase grabada hay un ejemplo de mundo cuadricula:
  las acciones son up, down, left, right. La recompensa es -1 para
  todos los estado no terminales. se asmume politica uniforme, es
  decir que todas las acciones tienen una probabilidad de 0.25. y
  determinista quiere decir que la accion elegida se cumple. El agente
  va a tratar de encntrar la manera mas rapida de ir de su posicion
  inicial a los estados terminales. Dikstra y A*(google maps) sn
  algoritmos usados para ir de un punto A a un punto B. recordar que
  se evalua todos los etados con todas las acciones mas acciones es
  mas costo. Si todos los estados cambian de valor es evaluacion
  sincrona.  porque no considero el cambio de valor y uso los valores
  anteriores por eso cuando salgo del loop sobre los estados recien
  actualizo los valores. si considera el cambio significa que se
  actualizo el valor y con eso calcular otro valor seria en su lugar. 
- en el mundo cuadricula el agente ha de aprender la manera más rápida
  de llegar al estado terminal
- buscar gridWorld dynamic programming demo.
#+begin_example

loop
    loop for all S
    v obtiene el valor de V(s)
    calculo v(s) con bellman
    calculo una diferencia delta entre la v nueva y la v vieja
    terminar si el delta es menor que un valor
#+end_example
*** mejoramiento de la politica
- cada que evaluamos iteramos sobre todos los estados e ingreso a otro
  for loop para mejorar la politica . se alterna entre evaluacion y
  control y se llega a valores optimos de v y de pi. Se usa la
  optimilidad de bellman. la manera mas sencilla de hallar una
  politica optima es usar greedy. voy mejorando la funcion d valor y
  la politica. el tener los dos procesos prediccin y control permite
  hacer una mejora de la politica rapidamente. 
- y se actualiza la politica con la optimalidad de bellman
- es una alternacion entre evaluacion y control (actualizar politica)
- la manera mas sencilla de encontrar una politica optima es usar greedy
*** teorema de mejoramiento de la politica
dado $\pi$ en un proceso iterativo y una siguiente iteracion tal que
mi siguiente politica sea greedy $\pi' = greedy(v_{\pi}) =
\text{argmax}_{a in A}q_{\pi}(s,a)$ Al tomar greedy es como asumir que
la accion no influye y hace que V y q sean iguales
Es decir una vez que se fija la accion v y q son iguales

Es decir tomar greedy de la funcion de evaluacion de los estados, es
igual a tomar el argumento maximo de la q para una accion fija. 


** Summary

* 2025-03-16 RL David Silver
** Questions and keywords
- Reward :: all goals can be described by the maximization of expected cumulative reward
- Goal :: select actions to maximize total future reward
- history :: sequence of observations, actions and rewards $H_t =
  A_1,O_1,R_1, \dots, A_t, O_t, R_t$
- agent :: something that acts on the environment
- state :: is the information used to detrmine what happenes next. It
  is a function of the history $S_t = f(H_t)$. It is a summary of
  all that has happened and helps to predict what comes next. State is
  any function of history.
- environment state :: the information within then environment and decides
  what comes next.
- agent_state :: it is the internal representation of the agent. a set
  of numbers that captures what is going on. It is information gathered
  by the agent to pick next action. It is function of history.
- markov state or information state :: significa que el estado
  siguiente depende exclusivamente del estado anterior, se puede
  obviar todos los estados anteriores i.e. $P[S_{t+1}|s_t]
  =P[S_{t+1}|S_1, S_2, \dots S_t]$
- full observability :: full observability could it be agent state is
  equal to environment satet?
- partial observability :: poker player. The agent state is different
  from the environment state. agent must build its ownstate representation
- policy :: how the agent picks its actions.
- value function :: how good is a state and or action. How much reward
  do we expect to get by being in a state or taking an action. It is
  the prediction of the **expected future reward**
- $\gamma$ :: discount factor
- model :: prediction of what the environment does next
- exploration ::
- explotation ::
- prediction :: evaluate the future given a policy
- control :: optimize the future. Find the best policy
- MDP :: Markov Decision Process
- optimal control :: ??
** Notes
- Reward could be delayed
- Actions may have long term consequences. We need planing.
- all goals can b described by the maximisation of expected cumulative reward
- agent selects actions
- environment selects observations and rewards
- environment state is not visible to the agent
- Markov state the future is independent of the past given the
  present. El estado presente contiene suficiente informacion para
  poder hacer predicciones hacia el futuro: $H_{1:t}\to S_t \to
  H_{t+1:\infty}$.
- En un Markov State, once the state $S_t$ is known, the history may be
  thrown away
- a markov state is always the environment state
- what happens next depends on representational state
*** Agent componentes
- tiene una Policy, value function, model
- polocy is a map from state to action
- policy deterministic $a=\pi(s)$
- stochastic policy helps to make random explorative decisions
  $\pi(a|s) = \mathbb{P}[A=a|S=s]$
- the value function is the prediction of the expected future reward:
  $v_{\pi}(s)=\mathbb{E}_{\pi}[R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\dots|S_t=s]$
*** The model
- predicts what the environment will do
- it has a *State Transition Matrix/Model* $\mathcal{P}$ that predicts the next
  state i.e. the dynamics. $\mathcal{P}_{ss'}^a =
  \mathbb{P}[S'=s'|S=s, A=a]$
- it has a *Rewards Matrx* $\mathcal{R}$ predicts the next immediate
  reward $\mathcal{R}_s^a = \mathbb{E}[R|S=s, A=a]$
- it is necessary to have this matrices or data because it is the
  equivalent to model the environment.
- RL bases on model free problems
*** Categorizing RL Agents
- **Value based**
  - No Policy
  - It has Value function
- **Policy based**
  - Policy
  - no value function
- **Action Critic**
  - Policy
  - Value Function
- Model free: it has no model but represents the model through policy
  and or value function
- Model based: First episode make a model and also could use POlicy
  and Value functions.
*** Problems in RL
- Learning and Planning
- RL: first tires to learn how the model works and then goes to planing
- Planning: the model of the environment is known
- Exploration and Explotation: RL is like trial and error
  learning. The agent should discover agood policy.
- exploration: give up some reward to get more informatio about the environment
- exploitatino: exploit the informationknown to maximise reward
- In RL we need to solve the **prediction problem** to **(control)
  optimize the policy**
*** Markov Decision Process MDP
- MDP the current state completely charecterises the process
- Almost all RL problems can be formalised as MDPs
- Optimal control deals with continuous MDPs
- 
** Summary
*** Introduction
El aprendizaje por refuerzo es una especia de trial and error i.e. de
aprendizaje por error. Para esto, el agente de RL tiene que que
observar el *entorno*, tomar una *accion* y recibir una respuesta a
traves de una señal de recompensa. La *politica* define el
comportamiento del agente i.e. como escoge las acciones. De ahi que el
agente forma una representacion interna que obtiene el agente. Para
poder realizar esto, el objetivo a alcanzar por el agente debe ser
expresado como la maximización de la recompensa esperada. De ahi que
el agente utiliza como componentes: 1. politica, 2. una funcion de
valor del estado/accion y 3. modelo. La idea de RL es obtener la
*politica optima* es decir aquella que nos conduzca al objetivo con la
mayor recompensa posible. La policy es un mapa de los estados y las
acciones. Puede ser determinista, donde una accion se deriva de un
estado especifico o estocastica, en donde se puede tomar decisiones
explorativas. Un estado puede ser del entorno o del agente. El del
entorno es desconocido para el agente. El estado del agente tiene la
representacion que hace el agente del entorno luego de explorarlo. La
funcion de valor accion estado estima la accion o estado enfuncion de
la recompensa y permite con sus valores decidir o predecir que accion
tomar luego. Predecir el siguente estado es el problema de prediccion
mientras que optimizar que hacer a futuro es el problema de
control. En RL, los agentes enfrentan varios problemas como el de
exploracion vs explotacion. La exploracion es que tanto puedo
sacrificar mis recompensas inmediatas para poder descubrir
alternativas nuevas que a largo plazo me conduzcan a una mayor
ganancia. La explotacion es en cambio obtener la maxima recompensa a
partir del conocimiento que se tiene. 
* 2025-03-18 Renta de Autos Jack
** Questions and keywords
- Poisson ::
- Funcion de Probabilidad FP ::
- no me queda claro como leer los diagramas ::
- no me queda claro porque es la optima :: dice que en 4 iteraciones
  se obtiene la poltiica optima y que se vio en clase en cual?
** Notes
- cada lote tiene un maximo de 20 carros
- esta relacionado a la optimizacion lineal entera
- accines rentar y devolver
- accion es mover los autos
- por cada renta gano +10
- **maximizar rentas**
- si no tengo autos para dar a una persona por no disponer de autos
  pierdo (recompensa negativa)
- se puede mover los autos entre las dos pilas con R = -2
- el algoritmo ha de aprender la distribucion de autos se requiere en
  cada locacion
- se usara una funcion de probabilidad discreta Distribucion de
  Poisson
*** Modelado
- Se hara un modelado del #de rentas/devoluciones
- $\lambda$: es valor esperado y varianza de la distribucion de poisson
- $\lambda$: cuantos personas rentan un auto en localidad 1
- $n$: numero de rentas o devoluciones
- Funcion de Probabilidad FP:  $P(X=n)=\frac{\lambda^{n}}{n!}e^{-\lambda}$
- Se asumen conocimientos previos por acopio de datos:
  - Renta loc 1: $\lambda = 3$
  - Renta loc 2: $\lambda = 4$
  - Retorno loc 1: $\lambda = 3$
  - Rentorno loc 2: $\lambda = 2$
- Truncamiento de la función para valores mayores que 11 la prob es 0
  i.e. mas de 11 solicitudes
- maximo de numero de movimientos de local 1 a local 2 es 5.
*** Entorno
- acciones: que tantos carros puedo mover. Es un vector. + de loc 1 a
  loc 2 y - de loc 2 a loc1.
  $acciones  = [-maxMov, \dots, -2,-1,0,1,2,\dots, maxMov]$
- estados es un vector que contiene el numero de autos por localidad $estados = [cars_1, cars_2]$
- la funcion de valor sera un vector i.e. el valor de estado es una
  matriz de  $20 \times 20$ que contran todos los valores de estados
*** Politica
- Ida basica de la politica: Si yo tengo /x/ num de autos en loc1, entonces debo tener /y/ en loc2
- $\pi_0$ politica inicial ningun auto se mueve
- los ejes son el numero de autos en cada locacion
- la 4 es la politica optima
* 2025-03-18 Métodos Monte Carlo e Iteracion de Politica
** Questions and keywords
- P :: matriz transicion
- GPI :: Iteracion de poltica generalizada
- PD :: Programacion denamica
- en su lugar tiene 2 copias? ::
- cual es el espacio de stados del ajedrez ::
- MCMC :: Metodos Monte Carlo
- aprendizaje por fuerzo libre de modelo :: es un RL con Montecarlo
- experiencia ::  son los datos que obtengo del proceso u entorno
- el retorno esta relacionado a la recompensa ::
- MC :: montecarlo
- MC 1era visita ::  utiliza solo el valor de la primera vez que uso
  un estado
- MC cada visita :: voy a reemplazar el valor anterior cada vez que
  visito ese estado. cada visita actualizamos el vaor cada vez que
  pasamos por el estado
- First Visit MC :: 
** Notes
- Métodos de Montecarlo: no se necesita asumir P, R
- Iteracion de política: asume que conocemos P, R
- Iteracion de valor: asume que conocemos P, R
*** Policy Iteration estimación de $\pi$
- se divide en dos partes Policy Evaluation y Policy Improvement
- $\Delta<\theta$ da una estacón buena de V(s)
- Solo cuando obtengo la buena estimación paso a policy improvement
- El algoritmo de iteracion de politica el paso 2 es el de evaluacion
  y usa la ecuacion de la esperanza de bellman.
- Luego en el paso tres hace la mejora con la ecuacion de la
  optimalidad de bellman
- 
*** Iteración de valor
- toma una sola iteracion de la estimacion de la funcion de valor (del
  bucle) pero devuelve solo el valor que maximiza la funcion i.e. la
  que en esa iteración con esa política da el mejor valor de V
- Ya no considera todas las acciones (media ponderada) sino que tomo
  la que es mejor $v_{k+1}= max \sum p[r+\gamma V(s')]$.
- iteracion de valor en un solo paso hace la estimacion de la accion
  optima y de la politica optima.
- la ecuacion con max usa la optimalidad de bellman. Este maxeo
  permite de manera conveniente no ingresar al otro bucle de
  ptimizacion de la politica
- es combinar la parte de predicción y control en un solo paso
- para balancear entre iteración de política e iteración de valor para
  no solo maximizar constantemente y hacer una sola iteración o gastar
  computacionalmente haciendo todo el proceso computacional se usa
  GPI> iteracion de politica generalizado. es no irse a los extremos
  sino hacer un menor numero de iteraciones con lo cual se reduce el
  costo computacional.
*** Algoritmos sincronos de Programacion Dinamica PD
- Esperanza de bellman hace predicción y se llama algoritmo de
  Evaluacion iterativa de la politica
- Valor esperado de bellman + politica greedy permite hace predicción
  y control. Esto se denomina iteracion de politica. hace prediccion y control.
- la ecuacion de optimalidad de bellman es el algoritmo de iteración
  de valor. Como hago una iteración y tomo el max hago solo control.
- estimacion de valor reduce mas. hace casi solo control. 
- La complejidad de todos tres algoritmos en terminos de tiempo es
  $O(mn^2)$ con
  - m numero de acciones
  - n numero de estados
- escalan peor que cuadraticamente porque mientras mas estados tengo
  mas complicado se vuelve.
- Si se considera q $O(m^2n^2)$ porque la probabilidad tambien depende
  de las acciones.
- en este enfoque se hace un barrido por todo el espacio de estados
- esta es el equivalente a la actualizacion sincrona donde hay dos copias
*** Programación Dinámica Asíncrona PDA
- En todos los algoritmos se realiza un barrido de los estados o del
  espacio de estados
- En programación dinámica asíncrona  los estados se actualizan en
  cualquier orden y con a información disponible en t
- P.D. Asíncrona es el equivalente a actualizar V(s) en su lugar
- PD asíncrona reduce el computo. estrictamente necesario para
  problemas reales
- para que exista convergencia es necesario que **visite** todos los
  estados. como puedo converger si no he visto todo el problema. es
  si es que he visto algunas veces los estados puedo converger.
- Ejemplos:
  1. Juego d estrategia con varios batallones de un ejercito: no
     visitar la reserva si estoy lidiando con los soldados de forntera
  2. Ruta mas rápida
  3. arreglo de desgaste de piezas. e.g. una pieza que se da;a a cada
     rato vs cada a;o
- los estados importantes se aprenden no se definen
*** Cosas que permite la PDA
- la PDA permite visitar los estados mas importantes
- se puede ignorar casi completamente ciertos estados no relevantes
  para el problema
- dada una política actualizar los mas los estados que la política visitada
- algunas partes del problema cambian mas lento que otros, por ejemplo
  analizando problemas variables en el tiempo. POr ejemplo la
  manufactura de una pieza. 
- PDA permite que me centre en los estados mas importantes ya que no
  estoy obligado a barrer todos el espacio de estados ni en un orden especifico
- PDA tambien permite ignorar casi completamente (porque la condicion
  es que visitemos todos los estados) los estados no relevantes para
  el problema.  Ejemplo si tengo que encontrar una ruta entre dos
  puntos y hay puntos completamente fuera de la zona de interes los
  puedo obviar
- PDA puede aprovechar la politica para actualizar mas los estados que
  la politica por si misma obliga a visitar
- PDA permite dar una prioridad a los estados a revisar en un problema
  variable en el tiempo
- no se sabe cual es el estado importante. si se conoce sobre el
  problema se puede dar una prioridad al estado. solo son los estados
  que mas peso tienen en el problema. Esto se aprende.
*** Metodos MonteCarlo
- su idea principal es estimar distribuciones que no conocemos de
  manera eficiente.
- EL RL es similar al problema de los imanes. en donde se coloca un
  juego de imanes cubiertos por una hoja que no nos permite ver que
  ellos estan colocados. lo que nos ayuda montecarlo es a tomar
  acciones para descubrir sobre el proceso. se distribuye
  uniformemente limallas de acero. lo que da idea sobre los campos
  magneticos que estan gobernando el proceso. 
- los proceso a aprender asemejar a los imanes a un arreglo de imanes
- se cubre los imanes en una hoja de papel
- aplicando acciones y montecarlo se va descubriendo como opero
- distribuyendo uniformemente el material correcto
- montercarlo permite estimar magnetic field lines
- montecarlo es dar un montón de acciones y ver la respuesta del
  proceso esto me permite conocer como se comporta
- desde la perspectiva de RL, se puede considerar que el problema a
  conocer es una caja negra con entradas y salidas. Se pueden medir
  las respuetas a las entradas. El sistema va a filtrar la
  entrada. Para conocer sobre la caja negra necesito ingresar inputs
  adecuadas. Es decir acciones adecuadas. porque en base a la
  respuesta del proceso se va a estimar la política 
*** Condiciones del Metodo Monte Carlo
1. Generar los Input con la Política i.e. la política se usa para
   generar las acciones.
2. Observar los output es decir las salidas del sistema. La única
   salida que tengo del sistema son R las recompensas
3. procesar
4. se requiere muchos datos para que funcione bien
5. condiciones extra: elegir los inputs de manera uniforme
   i.e. uniformemente en el sentido probabilístico de la palabra es
   decir cada acción debe ser elegida con la misma probabilidad
Usar los métodos de montecarlo se denomina RL libre de Modelo. No se
necesita saber la matriz de transición de estados $P_{ss'}$. También
es desconocida la Recompensa $R$. Estas son reemplazadas con
experiencia que se obtiene de la exploración del entorno (sistema).
*** Estrategia libre de modelo
- Implica que P y R son desconocidas
- El conocmiento de P y R se reemplaza con experiencia. i.e. en un
  proceso real no son conocidas.
- experiencia viene de la exploración del entorno
- por tanto se van a recoger datos y estimar ciertas cantdades y se
  obtendrá la experiencia
- La Recompensa se puede ver
- Método de Montecarlo permite aproximar el Retorno
- MMC son buenos para aproximar Esperanzas, promedios, por ejemplo en
  $v_{\pi}(s)=\mathbb{E}_{\pi}[G_t|s_t=s]$ y
  $q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|s_t=s, A_t=a]$
- La esperanza ayuda a encontrar el entorno.
- Ejemplo de maquina de estado finito de estudiar. en MC significa que
  el grafico es desconocido. 
- Con MC se da cadenas de estado
- se obtiene un promedio de los estados visitados
- y ese va ser los valores de estado
- usando el grafico se dan dos cadenas. en la primera pasada se da una
  cadena c1, c2, c3, paso, dormir que da +4. en la segunda se pasa dos
  veces por c1. como lo que se quiere tener es el valor del estado c1,
  se puede tener dos valores en la segunda cadena. Por esta razon se
  usa puede dfinir dos enfoques:
  - c1, IG, IG, c1, c2, dormir que da -8 desde el primer c1 y -4 desde
    el ultimo
  - 1era visita solo el valor de la primera vez que uso un estado (+4-8)/2
  - cada visita: actaulizamos el valor cada vez que pasamos por el
    estado (+4-8-4)/3
*** First visit MC 
1. Iniciar una politica
2. estimar los retornos
3. generar un episodio siguiendo la política i.e. interactuar con el entorno
4. una vez que tengo las recompensas se hace un bucle para cada uno de
   los pasos del episodio de T pasos
5. calculo el retorno
6. obtengo el promedio o media de los retornos que puede ser una media
   incremental

note que el bucle va de reversa desde el ultimo paso hasta el
primero. esto se hace por eficiencia. 
*** Algoritmo de prediccion evaluacion
- iniciar la politica
- iniciar V con valores arbitrarios
- inicar los retoros como lista vacia
- dentro de un lazo eterno para cada episoodio genero una cadena y
  y ponto el retorno a 0. El episodio tiene T pasos
- for cada paso del episodio escrito de ordencontrario calculo el
  retorno con el factor gamma
- solo si st ya se paso no se actualiza sino actualizo los valores de
  V y optengo los promedios. el proceso se invierte porque de esta
  manera es mas amigable computacionalmente que ir en el orden
  episodico. porque los valores ya estaran calculados.
** Summary
* 2025-03-20 Control MC en y fuera de la politica
** Questions and keywords
- en que orden se hace evaluacon y mejora ::
- para que funcione MC :: debo visitar todos los pares (s,a) para
  garantizar la exploracion al menos una vez
- politica determinista :: no sucede la exploracion?
- algoritmo de inicions que exploran (Exploring Starts ES) :: se
  generan cadenas que voy a seguir pero el primer estadoes random y
  uniforme i.e. misma probabilidad para todos los pares de estado
  accion. en un bucle eventualmente se visitaran todos los etados.
- en politica  :: es aprender haciendo i.e. aprendo la politica pi
  usando la experiencia de la politica pi.
- fuera politica  :: es copiarle a otro. Voy a copiar la politica
  optima. Es decir, aprender la politica pi usando la experiencia de
  otra politica /b/. Esta estrategia es la que se usa por default
  - $\pi$: politica objetivo
  - $b$ politica de comportamiento
- cobertura :: cuando se trabaja fuera pi hay que asegurar la
  cobertura. La cobertura dice que cada par (s,a) que pueda ocurrir
  enla Pi target debe ser eexplorado al menos ocasionalmente por la
  politica de comportamiento (i.e. la que estoycopiando) la /b/
- Politica :: es una distribucion es una probabilidad. Recordar
- problema completo rl :: cuando P y R son desconocidos tambien
- GPI :: iteracion de politica generalizada. Hallo un buen valor de V
  y hago unas cuantas iteraciones y obtengo la mejora de la politica
- exploring starts ES :: se refiere a cadenas generadas pero el primer
  estado es aleatorio y uniforme. lo que sucede es que eventualmente
  luego de varias o muchas iteraciones se exploraran todos los
  estados. es parecido al optimismo. despues del estado aleatorio se
  sigue la politica propuesta. eventualmente se habran visitado todos
  los pares de accion estado y de esta manera se asegura la exploracion.
- $\pi$ :: politica objetivo target
- $b$ :: politica de comportamiento. se encarga de exploracion
- cobertura :: cada para (s,a) que pueda ocurrir en la politica
  objetivo debe ser explorado al menos ocasionalmente por la politica
  de comportamiento /b/. es asegurar la exploración
- muestreo de importancia :: tecnica general del area de stadistica y
  probabilidad que se usa para estimar valores esperados o esperanzas
  matematicas de una distribucion dada otra distribucion diferente.
- $\rho$ :: $\pi/b$
** Notes
- montecarlo saca episodios. La clase anterior se vio esencialmente evaluacion
- en esta clase se realiza control
- se estudia conceptos de hacer control dentro y fuera de politica
- muestreo de importancia
- recordar que P, R no son conocidas se obtienen de explorcion
  mediante montecarlo que devuelve valores esperados
- la exploracion es requerida cuando no se conocen P y R ya que greedy
  explota el conocimiento por lo que no es muy real. se reemplazan con
  los datos o la experiencia.
- MC es bueno para calcular valores esperados i.e. la media de una distribucion.
- hacer gpi es decir una o muchas iteraciones para hacer evaluacion y mejora
- no se puede hacer GPI en el porblema completo de RL porque R y P son
  desconocidas.
- porque se puede maximizar la funcion de valor estado con respecto a
  la accion porque P y R son conocidas
- $\pi'(s)$ politica luego de una iteracion
- $\pi'(s) = \text{argmax}_a[Q(s,a)]$. En este caso la politica greedy
  si funciona. Para empezar Q se ve como un objeeto matematico de 2 o
  mas dimensiones porque tiene el estado y las acciones e.g. una
  matriz de acciones x estados. esto deja ver cual es la mejor accion
  que puedo tomar para un estado en un t.
- El espacio de accion estado es mas grande que el espacio de los
  estados. El espacio de s es un vector.
- para usar MC es necesario garantizar la exploracion.
- en una politica determinista la exploracion no sucede. Para asegurar
  la exploracion tenemos el algoritmo inicios que exploran y epslons soft
- hallar valores y registrar en memoria
- El espacio accion estado (s,a) es mayor siempre que el espacio s
- recordar la condicion para que funcione montecarlo
*** Exploring starts
- Este algoritmo hace control y evaluacion.
- 1 a 4 es la inicializacion
- iniciar una politica que inice ddesde cada estado comience con cada estado
- retornos vacios
- linea 6 escogo un estado inicial aleatorio
- linea 7 genera un episodio con la politica y el estado aleatorio escogido
- linea 8 inicio el retorno en 0
- linea 10 calcula los retornos es la evaluacion
- linea 11 es el concetpo de primera visita.
- linea 12 acumula los retornos i.e. un array en 3 dimensiones
- linea14 es la mejora de la politica
- lineas 8,9,10,11,12 es la evaluacion su corazon esta en 10
- linea 11 es la primera visita
- si se omite la linea 11 seria en cambio el algoritmo de Cada visita
- Es siempre posible sobre todo enproblemas reales escoger un inicio
  al azar??? : haycasos en donde es imposible aplicar por eemplo n un
  auto no se podria iniciar sin prender elcarro . ES es como el
  optimismo,sise puede selo pone para que ayude
- como no siempre se puede usar inicios que exploran se requiere otras
  maneras para ayudar la exploracion.
*** Politicas $\epsilon$-soft
- las polticas greede son un subconjunto de epsilonsoft
- es una version mas general de $\epsilon$- greedy
- $\epsilon$-greedy es que un porcentaje del tiempo hago una accion la
  mas ambiciosa y   el resto del tiempo una accion randomica
- es una politica estocastica que asegura que la probabilidad de
  tomaruna accion dado un estado es mayor que 0 para todas las
  acciones. es decir que todas las s,a van a pasar dadas suficientes iteraciones
- son politicas estocasticas que para todos los pares s,a  estas tiene
  al menos una probabilidad de ocurrir epsion sobre la cardinalidad de
  las aciones del estado
- Defnicino: $\pi$ es $\epsilon$-soft si todos los pares s,a tienen al
  menos $\frac{\epsilon}{\Delta(s)}$ de prob de ocurrir
- es decir que con cierta probabilidad se asegura que todas las
  acciones de todos los estados
- Si tengo un estado con 3 acciones. la cardinalidad del conjunto de
  aciones seria 3. si epsilon es 3 entonces la politica epsilon soft
  asegura que las acciones tendran una probabilidad pasar al menos
  mayor a 0.1/3.
- cada estado tiene un conjunto diferente de acciones
- el deltas es la carrdinlaidad del conjunto de acciones del estado:
  cuantas acciones tiene el estado? cada estado tiene un diferente
  numero de acciones
- sobre el algoritmo este requiere inicializar arbitrariamente una
  politica epsilon soft
- cuando voy a usar MC genero un episodio y obtengo el retorno que es
  la evaluacion. El algoritmo usa primera visita. 
- si quito la linea de primera visita se tiene el algortimo de CADA
  VISITA
- como se aplica lo greedy es tomar el 90% la mejor accion y un 10% ir
  a una accion random. La diferencia con epsilon greedy son los
  coeficientes. a=A*  es 90% de mejora $a \neq A^*$ es el 10% de tomar
  cualquier otra accion
- la politica epsilon greedy es optima?
- las politicas épsilon soft no son optimas.... ?siguen explorando
  luego de hallar un óptimo. Siguen explorando luego de varias iteraciones.
- epsion soft luego de varias iteraciones sigue explorando
- es mas si el porcentaje exploracion es bajo sigue explorando asi sea
  un valor peque;o
- lo deseado es que sea estocastica y determinista. $\epsilon$-soft es
  buena si no podemos usar ES
*** Control en (dentro) Política y Fuera de Política (Black Jack)
- son maneras de obtener la politica optima es un paradigma
- la politica optima es $\pi_*$
- control en politica es todo lo que hemos hecho hasta ahora
- ver en keywords. es decir se aprende usando la experiencia de la
  politica
- en control fuera de politica es copiarle a otro. alguieen ya
  encontro la politica optima y la uso. aprendo la politica pi usando
  la experencia de otra politica /b/
- la politica /b/ se queda explorando mientras que $\pi$ alcanza la
  optimalidad. 
- por default se usa politica fuera de politica y sus ventajas son:
  1. xq se deja la exploracion a la politica b y la politica $\pi$
     alcanza la optimalidad. esto es una ventaja
  2. podemos ganar experiencia de lo que otro agente hace. el agente
     puede ser una persona. Es decir el agente puede estar aprendiendo
     de una persona que se coloca en el lazo de entrenamiento. ChatGPT
     uso esto para entrenar sus habilidades conversacionales mediante
     puntajes que daban las personas a las respuestas. Es decir
     usaron fuera de política para esto.
  3. permite reusar la experienciad de políticas antiguas
  4. usando una sola politica de comportamiento /b/ podemos entrenar
     varias politicas óptimas u políticas objetivos $/pi$
- no free lunch: las desventajas de Fuera de Politica
  - Problema de la cobertura
  - si $\pi(a|s)>0$ implica $b(a|s)>0$
  - si eso no sucede no estamos explorando o no tenemos datos de
    /(s,a)/ y no sabemos que ocurre si tomamos este par
- la idea de tener dos politicas tiene por problema que: usar los
  datos de otra distribucion para estimar la distribucion buscada en
  principio no es posible. las distribuciones son distintas. 
*** Muestreo de Importancia (Importance Sampling)
- Ejemplo: estimar la altura de personas enla provincia del guayas
- segun profesor no eslogico usar los datos de una;o anterior para
  estimar los del a;o 205
- las politicas son distribuciones. 1:39:47
- es una tecnica estadistica/probabilista para estimar esperanzars o
  valores estimados de una distribucion dada usando otra distribucion.
- demistracin en el video de muestreo de importancia fuera de Pi
- loque se observa es que la esperanza en la distribucion pi es la
  misma que la en /b/ pero con una correccion que es la razon de
  muestreo $\mathbb{E}_{\pi}[X] = \mathbb{E}_b[X\rho]$
- para la demostracion son distribuciones evaluadas en una accion para
  tener sus valores. 

** Summary

* 2025-03-25 Aprendizaje Diferencia Temporales (TD $\lambda$)
** Questions and keywords
- Sortear los laboratorios
- donde esta el temario?
- semana para entregar tareas
- $C(s,a)$ es un acumulador
- /b/ es unapolitica on un a cobertura de $\pi$
- que es hacer control y que es hacer estimacion
- juego resuelto :: en el que el computador siempre gana e.g. ajedrez bagamon
- bootstraping :: relacionado a modelo de hopfield
- modelo de hopfield :: grafos
- montecarlo es un subconjunto o caso especial de diferencias temporales
- TD :: diferneicas temporales
- $\alpha$ :: constante en el TD
- lamina Acerca del Aprendizaje TD :: preguntas de la evaluacion siguiente
- bootstrapping :: apoyarse de algo más sencillo para ejecutar algo
  más complejo
- overfitting :: sobreajustando los parametros del modelo. no se
  generaliza bien el modelo a los datos. varianza alta. overfitting
  tiene que ver con varianza alta. 
- underfitting :: el modelo necesita mas no linealidad. Sesgo alto se
  relaciona con underfitting
- vision unificada de RL ::
- busqueda exhaustiva :: es recorrer todo el espacio de estado
** Notes

- la clase se enfoca en TD0
*** Muestreo de Importancia
- se basa en encontrar una razon entre la politica y la b i.e. $\rho = \frac{\pi(x)}{b(x)}$
- tomo una politica que me dice unas probabilidades sobre las acciones
  y obtengo un arreglo de recompensas $R=[1,3,1]$ Se toma la accion 1
  dos veces y la accion 3, una vez. (1,85%), (2,5%), (3,5%), (4,5%)
- b es la politica d exploracion o comportamiento
- supongamos otra distribucion de probabilidad en que la politica toma
  40% del tiempo tomoa la accion 2, la 1 el 30%, 3 el 10%, 4 el 20%.
- Para obtener el retorno con la política $\pi$ seria el promedio de
  recompensas con la politica d ecomportamiento?

  $G_{\pi} ¿ = 1/n\sum Rp$
- La politica de comportamiento la pone el diseñador y sabe cuantas
  veces sucede cada accion.
- usando la politica /b/ de comportamineto se obtiene los retornos $G_b$
- $\rho$ es un ajuste
- los pasos a diferentes estados se analizan como la probabilidad de
  la accion dado el estado $\rho(A|s)$.
- la $\rho$ acumulada es la multiplicacion de los $\rho$
  $\rho_{t:T-1}=\prod_{k=t}^{T-1}
  \frac{\pi(A_k|s_k)}{b(A_k|s_k)}=\rho_1 \rho_2 \rho_3 \rho4$
- en /V(s)/ se divide por elnumero de veces que pase por cada
  estado. $\rho$ es difernte para cada estado. no se divide para la
  longitud completa del episodio. $\tau$
*** Dilema del sesgo/varianza o entre el bias/varianza
- imagine dos conjuntos de datos uno para /b/ y otros para /pi/. los
  datos van a arrastrar hacia el /b/ 
*** Muestreo de importancia ponderado
- si solo tengo una muestra , la estimacion del estado devuelve el retorno
- hay un sesgo hacia el cojunto de datos de b
- $v(s) = \frac{\rho_{t:T-1}G_t}{\rho_{t:T-1}}$
- El muestreo de importancia ordinario estima $v_{\pi}(S)$ y tiene una
  alta varianza y si la cadena de estados no es la suficiente larga la
  estimacion del valor puede divergir
- El muestreo de importancia ponderado esta sesgado hacia /b/ al
  inicio. pero con mas iteracines converge a $v_{\pi}(s)$ y tiene baja
  varianza. los valores van a estar cercanos uno del
  otro. i.e. varianza se va a 0 conforme mas iteraciones y por tanto
  converge al valor esperado. Por esta razon se usa en la practica
*** Control con MC
- Control implica que la poltica va a estar cambiando
- la idea de **cobertura** es si existe la probabilida dde que pase en
  $\pi$ tambien debe pasar para o en la politica /b/
- control con MC no es effectivo
  - es lento en conveger
  - valores de /q(s,a)/ tienen alta varianza
- Montecarlo es para sólo para tareas episódicas. pero no todas  las
  tareas son episódicas.
- montecarlo no usa la informacion disponible inmediatamente
  $G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} + \dots$
- el problema de que no explote la informacion es que no podria
  funcionar conectado a sistemas en linea
- Montecarlo espera al finaldel episodio para actualizar valores
- TD reporta observando cada estado
- TD hace peque;as actualizaciones para llegar al valor final
- 
*** aprendizaje por diferencias temporales
- funciona con episodios incompletos
- esta relacionado al bootstrapping
- hace peque;as actualizaciones en vez de MC que espera al final del episodio
- TD(0) usa la recompensa inmediata luego de tomar una acción
- en el algoritmo de TD el parametro $\alpha$ es importante
- cada paso que se da de cada accion se actualiza el valor de v
- a pesar de que tengo dos bucles uno de episodios y dentro de ese los
  pasos ya o tengo que calcular retornos.
- ya no necesito de preocuparme de si fuera de politica o si organizar
  los valores desde t-1. sino que se hace constantemente la actualizacion.
- TD usa el concepto de bootstraping
- bootstraping usa el estimado de /V(s')/ para hallar /V(s)/
- TD aprende sin necesidad de ver el resultado. no necesita llegar al
  otro estado sino observar. es muy conveniente
- TD puede aprender tareas sobre la marcha i.e. aprendizaje online
- TD puede aprender tareas no episodicas
- TD puede aprender tareas continuas e.g. control de temperatura
- tD puede aprender de secuencias de stado incompletas
- TD converge mas rapido que MC
- 2025-03-27
- no usa el retorno sino estimacion del siguienteestado
- es como cortar a montecarlo
- *caminata aleatoria* usa una politica random
  - si usa MC tarda mucho en aprender por la exploracionque tiene que
    hacer ya que **actualiza completamente al final de cada episodio**
  - al usar TD, como *actualiza en cada paso del algoritmo*, el tiempo
    es menor
- el parametro $\alpha$ en la aproximacion incremental para TD esla
  velocidad de convergencia
- en montecarlo de igual manera si $\alpha$ es alto se converge mas
  rapido pero con mas error
- si $\alpha$ es bajo, voy a converger mas lento pero con menos error
- POr que parece que TD se acerca mas rapido al verdadero valor?
  revise la curva de $\alpha=0.5$. Solo significa que MC es mas lento
  no erróneo
*** Sesgo varianza
- que tiendo a tomar mis decisiones en funcion de la politica b
- se analiza en esta seccion en su totalidad wrt a MC y a TD
- esta relacionado con *overfitting* y *underfitting*
- en Montecarlo el retorno tiene bastantes datos de cada episodio por
  loque se relaciona con el underfitting es decir un sesgo bajo
- en TD como no ve los datos sino un estimado de los datos tiene sesgo
- en TD tiene baja varianza
- en TD, si el modelo cambia un poco yo puedo ajustarme por labaja varianza
- es necesario estar entre overfitting y underfitting.
- revisar tabla del dilema sesgo/varianza 2
- Montecarlo sesgo vs varianza
  - tiene alta varianza y bajo sesgo al iniciar
  - buena convergencia
  - insensible a las condiciones iniciales
- TD
  - tiene algo de sesgo al inicio
  - mas eficiente
  - es mas sensible a las condiciones iniciales
*** Eficiencia y Utilidad TD para RL
- MC converge a la solución con mínimo error medio cuadrático RMS
- MC lo que hace es el mejor aproximado de los retornos en el sentido
  de minimos cuadrados
- TD0 converge ala solucion de maximo verosimilidad MLE
- TD0 implicitamente halla el mejor vector de parametros $\theta$ para la
  funcion probabilistica $(\cdot| \theta)$ que explica mis
  muestras. esto consiste en maximizar el logaritmo de la
  distribucion.
- TD0 al ser un modelo probabilistico explota de mejor manera la
  propiedad de markov
- TD0 se ajusta a los modelos de RL
*** vision unfiicada de RL
- son dos ejes muestreo vs bootstraping
- la idea de bootstraping es actualizar usando un estimado
- la exploracion completa de un episodio hace que TD0 pase a MC
- MC no hace bootstraping ya que no hace estimaciones. mira todo el episodio
- la idea de muestreo es obtener muestras mediante la exploracion y
  estima un valor esperado. MC sí saca valores esperados. PD no saca
  muestras. TD sí saca muestras.
- busqueda exhaustiva casi que nunca recomendable

** Summary
* 2025-03-27 sarsa y  q learning 
** Questions and keywords
- sarsa :: para control libre de modelo
- q learning :: para control libre de modelo
- idea del q learning ::
- completar el proyecto hasta el 2 semanas despues de la proxima clase
  del martes:: 
** Notes
- Estaremos viendo algorimos: Sarsa y Qlearning
- la construccion del sistema es lo mas laborioso
*** Control con Aprendizaje TD
- SARSA aprendizaje en política
- Aprendizaje Q es para fuera de politica
- en politicas sin modelo necesito exploracion
- hay iteraciones por vlor e iteracion por politica
*** Sarsa
- estado accion recompensa estado accion siguentes
- considera transiciones s a s' y A a A'
  $Q(s_t, A_t)=Q(s_t, A_t)+\alpha[R_{t+1}+\gamma Q(s_{t+1}, A_{t+1}) -
  Q(s_t, A_t)]$
- Trabaja con el estado actual $Q(s_t, A_t)$
- target : $R_{t+1}+\gamma Q(s_{t+1}, A_{t+1})$
- error es la diferencia con Q a tiempo t
- SARSA cuando llega al siguiente estado solo ve el valor estimado de
  la siguiente accion no ejecuta la siguiente accion
- en el algoritmo el paso 8 que usa greedy es la mejora
- en el algoritmo el paso 9 es la evaluacion. esta al revez que
  normalmente evaluamos valores iniciales y de ahi mejoramos
- esto es porque sarsa requeire del siguiente estado
- sarsa observa *solo una** accion y tomo su estimado
- 
*** Aprendizaje Q
- en aprendizaje Q de todo elconjunto de acciones tomo la que maximiza
  Q. esto se conoce como **idea q learning**
- sarsa esta relacionado conla ecuacion de valor esperado de bellman y
- Q esta relacionado con la ecuacion de optimalidad de bellman
** Summary
* 2025-04-01 Metodos tabulares
- MC tiene sesgo de 0
- MC actualiza los valores q y v al final del episodio
- TD actualiza en lamarcha
- TD tiene una baja varianza
- En la practica queda usar TD
- Una manera intermedia entre TD y MC es TD en n pasos
- mientras mas pasos se toman mejor es el estimado del retorno
*** Metodos de aproximacion
- v y q se vuelven funciones con parametros
- se libera de tener una matriz de memoria
*** Metodos de evaluacion del curso
1. abrir las evaluaciones. Ma;ana se sube evaluacion final. Las
   evaluaciones tienen 2 intentos. para todas. Promedio de intentos. 30%
2. Proyecto: uno de los laboratorios comentado. todo lo referente al
   problema resolviendo y todo lo referente al aprendizaje por
   refuerzo. que se note la revisión del material. No se comenta el
   codigo relacionado a las graficas. 60%
   - Ejecucion 40%
   - Comentarios 60%: esto es la mejora de la politica. esto es
     evaluacion, etc. esta es ecuacion de bellman.
3. Entregar hasta el lunes 07-04-2025. 23:59 se sube el archivo en
   formato de jupyter
* Sutton Book
** k-armed multi bandid problem
- greedy :: cuando se selecciona la accion cuyo valor es el mejor
- explotacion :: usar el conocimiento de los valores de las acciones
  para maximizar beneficion o recompensa esperada en un paso
- exploracion :: seleccionar una accion no greedy. puede producir la
  mayor recompensa total a la larga.
- how do they use the law of large numbers :: $Q_t(a) \leftarrow q(s)$
  para que converga al valor verdadero
- greedy :: seleccinar la accion o una de las accnes con el mas alto
  valor estimado del valor de la accion en un paso t $A_t =
  \underset{a}{\text{argmax}} Q_t(a)$. Una accoin gredy simepre explota el
  conocimiento disponible o actual para maximizar la recompensa
  inmediata
- Cada accion tiene un valor experado o medio de recompensa
- $A_t$ :: Accion tomada en un tiempo t
- $R_t$ :: Recompensa correspondiente a $A_t$ en /t/
- $q_*(a)$ :: Valor esperado de una accion /a/ $q_*(a) \doteq \mathbb{E}[R_t|A_t=a]$
- $Q_t(a)$ :: valor estimado de la accion
- Para estimar $Q_t(a)$ (i.e. el valor de la accion) partimos de que
  se la puede definir como el promedio de las recompensas obtenidas de
  la accion seleccionada
  #+begin_export latex
  \begin{equation}
    \label{eq:def-action-value}
    Q_t(a)=\frac{\text{suma de recompensas cuando se toma}}{\text{numero
      de veces que se toma a}} = frac{\sum R_i\cdot A_i}{\sum A_i}
  \end{equation}


  #+end_export
- El conflicto entre exploracion y explotacion, en control se llama el
  conflicto entre estimacion y control
- La estimacion del valor de una accion $Q_n$, luego de ser
  seleccionada $n-1$ veces se define :
  $$Q_n \doteq \frac{\sum_{i=1}^{n-1}R_i}{n-1}$$
- La implementacion incremental de $Q_{n+1}$ es ::
  $$Q_{n+1}=Q_n+\frac{1}{n}[R_n-Q_n]$$
- $\alpha = \frac{1}{n}$
- En general, la regla de actualizacion es:  $NewEstimate \gets
  OldEstimate + StepSize[Target - OldEstimate]$ 
** Markov Decision Process
* TODO [57%]
- [X] Ingresar al Moodle y revisar bibliografía y Temario
- [ ] Implementar el laboratorio o hacer el proyecto
- [X] Atender clase de bandidos en aula virtual
- [X] Responder las preguntas de la semana 1
- [X] Solicitar en el foro la demostración de la implementación
  incremental de Q
- [ ] solicitar una descripcion del diagrama de respaldo de los
  ejemplos no sé que está modelando parece un estudiante.
- [ ] solicitar revisar el cuestionario semana 2
