#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil expand-links:t f:t
#+options: inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+options: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+title: Inteligencia Artificial Generativa
#+date: 2024-10-02
#+author: Lenin G. Falconí
#+email: lenin.falconi@epn.edu.ec
#+language: es
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.1 (Org mode 9.7.5)
#+cite_export:

#+begin_comment
* Lecture
** Questions and keywords
** Notes
** Summary
#+end_comment

* Resources:
- [[https://www.davidsilver.uk/teaching/][UCL Course on RL]]
- [[https://www.youtube.com/watch?v=2pWv7GOvuf0][YouTube videos]]
- [[https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf][Reinforcement Learning Algorithms Csaba Szepesvari]]
  - 
* 2025-03-06 Introducción Aprendizaje por Refuerzo
** Questions and keywords
- Reinforcement Learning :: paradigma de aprendizaje en Machine Learning
- Machine learning :: es una disciplina de la Inteligencia Artificial
  en donde se busca que una maquina aprenda a través de datos
  históricos sin necesidad de una programación explícita.
- paradigma :: marco conceptual que engloba una manera de representar
  o concebir el dominio de un problema. Machine Learning se puede
  considerar un paradigma en la medida que propone una manera
  alternativa de abordar el desarrollo de sistemas informáticos
  (e.g. sin una programación explícita). Reinforcement Learning
  también se puede considerar un paradigma de los tipos de
  aprendizaje, ya que a diferencia del supervisado y no supervisado,
  RL esencialmente se interesa por la interacción de un agente en un
  entorno.
- Agente :: programa que aprende por prueba y error obtiene una manera
  óptima de realizar una tarea
- Inteligencia Artificial ::  cualquier máquina (software) que imita
  /función/ que realiza un ser humano. Campo de las ciencias de la
  computación que busca crear sistemas capaces de resolver tareas
  mediante el aprendizaje a través de datos. Característica que le
  permite funcionar como una inteligencia de origen no humano. Si bien
  puede imitar ciertas capacidades humanas, su objetivo no es replicar
  la inteligencia humana. Más bien creo que lo mejor es la
  *identificación automática de patrones* en los datos.
- programación dinámica :: enfoque para resolver problemas de
  optimización complejos mediante la división en subproblemas más
  pequeños y superpuestos donde las soluciones se almacenan para
  evitar recalculo innecesario. Depende de memorización o tabulación.
- planeación al futuro :: se refiere a sistemas que cambian con el tiempo
- aprender con sistema online :: el sistema aprende y se adapta de
  manera continua mientras interactúa con su entorno o recibe nuevos
  datos. Actualiza su modelo en tiempo real. En general, la aplicación
  se da en sistemas donde el modelo debe actualizarse con los datos
  del entorno ya que los patrones de los datos cambian con el tiempo.
- sisteam offline :: aprende con un conjunto de datos estático y luego
  se despliegan. Su mejora se hace enfrentando nuevamente ajustes a
  través de datos estáticos tipo snapshot.
- no me queda claro que se pueda usar en sistemas de recomendación RL ::
- escenario o entorno ::
- comportamiento emergente ::
- cómo se enlazan las acciones de los agentes con los dispositivos electrónicos :: en
  clase se habla de controlar un volante, podría ser una válvula, cómo
  traduzco lo que me devuelve el agente en señales que puedan ir a los
  actuadores
- estado :: ¿tiene relación con los automatas finitos o maquinas de estado?
- rolling horizonte:: ????
- cuál es el dilema de la exploración y la explotación :: en RL
- puede RL enfrentar problemas NP completos como el de la mochila o el
  traveling salesman
- se puede usar RL para problemas de optimización lineal :: replantear
  la recompensa para que las soluciones esten dentro del conjunto de enteros
- es muy indicado para problemas distribuidos en nodos
- valor esperado :: esperanza pero asumio como el promedio para el
  problema o media aritmética
- esperanza :: 
** Notes
- RL es un paradigma de aprendizaje de machine learning.
  - No hay un supervisor sino una señal de recompensa o reward
  - El feedback o retroalimentación es retrasado
  - El tiempo importa. Los datos son secuenciales no de tipo  i.i.d.
- No hay instrucciones explícitas
- las reglas se descubren
- las estrategias desarrolladas para RL son métodos iterativos
  esencialmente
- Está relacionado con teoría del control(óptimo) y programación
  dinámica
- recibe tanto conceptos de machine learning como de deep learning
- Machine learning clásico:
  - KNN
  - Regresión
  - Clasificadores (e.g. regresión logística)
  - SVM
- Aprendizaje clásico no tiene la idea de *planeación a futuro*
- Cuándo usar RL:
  - EL se usa cuando se puede aprender con el sistema online
  - RL se usa cuando el tiempo importa y los procesos no son estáticos
    sino que cambian con el tiempo
  - RL se usa cuando los problemas son de horizonte continuo/infinito
    en el tiempo
- Por ejemplo sistemas de temperatura que varán con el tiempo
- Sistemas de recomendación manejan aprendizaje por refuerzo
- El objetivo del RL es *maximizar la recompensa esperada a largo plazo*
- es encontrar un conjunto de acciones que de la máxima recompensa
*** Formalismo
- Agente: aplicación o software o maquina que **interactúa** con un
  **entorno**
- El agente ejecuta acciones.
- El entorno: es una descripción del sistema (i.e. sensores)
- El estado: descripción exhaustiva por el entorno
- Recompensa: manera de evaluar que tal va
- Tareas: hay tareas continuas e.g. manejar el automóvil. La suspende
  el usuario Otro ejemplo sistemas HVAC de control de temperatura
- Tareas episódicas: son tareas con objetivos que se debe cumplir en
  ciertas iteraciones y repetimos hasta obtener buen
  rendimiento. E.g. video juegos
- Entornos:
  - parcialmente observables: poker con el paso del tiempo gano
    información pero hay limitación
  - completamente observable: e.g ajedrez. Cada que se mueve las
    piezas cambia el estado
- La recompensa se puede ver en un objetivo principal que puede
  dividirse en pequeñas recompensas. La suma de recompensas lleva al
  objetivo de ganar. Pero hay casos en que puede existir un sacrificio
  de mini objetivos con el objeto de ganar.
- la planificación es sacrificar pequeñas recompensas con la idea de
  obtener o asegurar la victoria
- Sobre las recompensas, todo objetivo se puede expresar como la
  maximizaron de una recompensa acumulada *esperada*
- se debe encontrar una manera de expresar el problema a resolver en
  función de una recompensa.
- recompensa acumulada -> planeación
- recompensa acumulada esperada -> valor esperado (probabilidades)
*** Ejemplos de recompensas
- hvac: medir el confort edel usuario maximizar si se gasta mucha
  energia puede ser negativo
- en video jeugo si gano es + si pierdo -
- en la conducci[on autonoma si choco es - si obtengo ruta optima es +
- chat gpt +- exp del usuario
- que es una estructura markov. eg. el mercado no es markov
*** Laboratorio
- el agente parte de un estado 0 $s_0$ y toma una acción inicial $A_0$
- el entorno devuelve la recompensa $R_1$ y un estado $s_1$
- el agente descubre por recompensas (refuerzo +/-) como obtener una
  cadena de secuencias que me lleve al objetivo
- los episodios acumulan varios cadenas de estados
  $EP_1:s_0A_0R_1, s_1A_1R_2$
*** Componentes del Agente
- política :: conjunto de acciones/decisiones ?permitidas?  que el
  agente puede tomar en un tiempo
- función de valor del Estado $v$ ::
- función de valor de acción estado $q$ ::
- $v$ y $q$ usan la recompensa para cuantificar que tan bien me va
- es posible que no existan ambas a la vez pero sí una polítca 
*** Política $\pi$
- Es un mapa desde los estados a las acciones
- El objetivo es encontrar la política óptima
- la política óptima maximiza la recompensa a largo plazo
- $\pi(s) = a$ deterministica donde las acciones son siempre las mismas
- $\pi(a|s) = P[A_t=a|s_t=s]$ Estocástica
*** Ejemplos Introductorios
- Maquina bandidos multi brazo: cada maquina recibe una moneda y da
  una recompensa que puede ser 0 o varias monedas. Cada máquina tiene
  una palanca. La hilera o arreglo de maquinas traga monedas es un
  agente. Cada una tiene una distribución que no será estimada. Se
  asume que hay infinitas monedas. Mi objetivo es maximizar el número
  de monedas. Acciones: halar la palanca. Es un problema incompleto en
  que cada episodio es más menos la misma dinámica. Es parecido al
  sistema de recomendación de redes sociales. encontrar la palanca que
  me de la maxima recompensa por ejemplo tiempo en pantalla para
  lucrar. Otra aplicación de este problema es elegir un servidor para
  mi cloud computing. En cuál la q es más pequeña para que el proceso
  tome menos tiempo.
- bandido es un traga monedas
- ensayos clínicos: se expone a pacientes a diferentes medicinas
  e.g. 3 y lo que se pretende es saber cual sana mas. 
- explotación vs exploración
*** Valores de acción estado $q$
- $k$ número de acciones
- recompensa estocástica
- la recompensa viene despues de la acción
- $q$ es la suma de recompensabas cuando tome una acción $a$ para el
  total de acciones
*** política
- la política puede usar un concepto Greedy aunque en realidad no se
  usa por limitaciones que tiene este concepto en la práctica en
  cuanto a la situación de exploración y explotación
- la política voraz parece limitar la exploración
- la política greedy ayuda a ilustrar las decisiones pero no se usa ya
  que anula la exploración
*** exploración vs explotación
- hay opciones más optimas?
- cual es el balance entre la explotación y exploración
- es decir si se hace mucha exploración se va a mucha incertidumbre en
  el sistema
- mucha explotacion ie. tomar la misma decision una y otra vez puede
  llevar a que me quede en una solución suboptima o mínimo local tal
  vez será lo que lleva el greedy
** Summary
Reinforcement Learning (RL) o aprendizaje por refuerzo es un paradigma
de aprendizaje en Machine Learning (ML) en donde se busca **maximizar
la recompensa esperada a largo plazo**. Para esto se usa un *agente*
que realiza *acciones* permitidas de acuerdo a una *política* sobre un
*entorno* y recibe una señal de retroalimentación retardada que es la
*recompensa*. Se caracteriza por afrontar problemas secuenciales ya
que el patrón de los datos cambia con el tiempo. Por esta razón
requiere de un sistema online (i.e. adaptación continua)

Machine learning es un paradigma que desarrolla programas que no
tienen instrucciones explícitas sino que aprenden mediante prueba y
error a optimizar una tarea. RL está como una técnica entre Machine
Learning y Deep Learning (DL). Está relacionado con /Control Óptimo/ y
/Programación Dinámica/.

Una característica importante de RL es que es un modelo que permite
una planeación futura. Algoritmos clásicos de ML como KNN, SVM, Redes
Neuronales,usados en problemas de clasificación y clusterización,
resuelven tareas específicas sobre un conocimiento histórico de datos
estático. Se puede decir que son esencialmente de tipo /offline/. Es
decir, aprenden de los datos obtenidos del sistema. RL es más bien de
tipo sistema Online, es decir aprende conectado al sistema. Extrayendo
datos y resultados de la interacción del agente con el entorno. Por
esto para ML clásico no importa el tiempo. En cambio en RL el tiempo
importa y de ahí que el foco de problemas de RL son procesos no
estáticos en el tiempo. Sin embargo, surge una duda. Un ejemplo de
sistemas de control son los HVAC. Sin embargo, hay que discutir la
diferencia entre predecir la siguiente temperatura (i.e. regresión)
considerando una colección de datos de temperatura y el que un agente
desarrolle técnicas para controlar la apertura y cierre de una
válvula. Podría ser una válvula controlada por PWM para mantener la
temperatura constante. RL en este sentido no se usa para predecir la
siguiente temperatura más probable. Ahora encuentro un problema,
podría usar ML clásico para predecir, supongamos el ángulo de disparo
de tiristores para controlar la temperatura? En este caso necesitaría
abordar el problema desde un esquema supervisado. Por qué no debería
hacerse con ML clásico este problema? Se nota que ML clásico no le
importa el tiempo. Sólo le importaría la relación entre el dato de
entrada y el de salida.

Algunos conceptos a considerar son:
- Entorno :: mundo o sistema en el que el agente se desenvuelve. Puede
  ser físico o virtual. Es una descripción exhaustiva del
  sistema. Puede ser determinista (i.e. misma acción y mismo estado
  producen el mismo resultado) o estocástico. Puede ser observable
  (ajedrez) o parcialmente observable (poker)
- Estado :: situación del entorno en un momento $t$. Puede ser
  discreto (rango finito de posibilidades)o continuo (rango infinito
  de posibilidades).
- Acción :: es la decisión que el agente toma para interactuar con el
  entorno. Pueden ser discretas o continuas.
- Episodio :: secuencia completa de interacciones entre un agente y su
  entorno, desde un estado inicial hasta un estado terminal.
  $Ep_1:S_0A_10R_1, S_1A_1R_2\dots$
  $Ep_1:S_0A_10R_1, S_1A_1R_3\dots$
- Política :: secuencia de decisiones que el agente ejecuta en una
  instancia de tiempo. Se relaciona con dos funciones para medir el
  éxito del agente frente al objetivo. Estas funciones usan la
  recompensa y son $\nu, q$. La política *mapea estados a
  acciones*. Define el comportamiento del agente. Le dice al agente
  que acción puede tomar en cada estado. Puede ser determinista o
  estocástica (distribución de probabilidad). $\pi(a|s)$: la
  probabilidad de tomar la acción $a$ en el estado $s$.
  - Política determinista: $\pi(s) = a$
  - Política estocástica: $\pi(a|s)  = \mathbb{P}[A_t=a|s_t=s]$
- Función valor de Estado $\nu$ (state value function):: estima cuán
  bueno es un estado para el agente en términos de la recompensa
  acumulada esperada a largo plazo. Mide la recompensa total que un
  agente puede esperar recibir a partir de un estado dado, siguiendo
  una política específica.
- Función de valor Acción Estado $q(s,a)$ (action-state value
  function):: estima que tan bueno es tomar una acción específica en
  un estado dado en términos de la recompensa acumulada esperada a
  largo plazo. Es la recompensa total que el agente puede esperar
  recibir al tomar una acción específica en un estado dado, y luego de
  seguir una política específica.


El bucle de RL es:
1. agente percibe el estado del entorno
2. agente elige una acción basada en el estado
3. el entorno responde a la acción y proporciona un nuevo estado y una
   recompensa
4. el agente aprende mediante el ajuste de la política de acciones
   para maximizar la recompensa.

*** Ejemplo de discusión
Consideremos un sistema de control de temperatura formado por una
válvula que inyecta el gas. La apertura de la válvula se controla
mediante una señal PWM. ¿Se puede resolver por ML clásico?

En una formulación ML clásica, se podría pensar que el ángulo de PWM
es una regresión. Para esto debería tener previo un registro de cuanto
incrementa la temperatura para un incremento del ángulo. Sin embargo,
se nota dos problemas:
1. si cambia el set point de control se tiene que registrar nuevos datos
2. no se considera en sí el cambio con el tiempo ni el estar conectado
   al sistema. Se observa que tratar como problema de regresión obliga
   a pensar en disponer de una tabla de datos para inferir un modelo
   que devuelva en cuanto abrir y cerrar la válvula dada la
   temperatura como entrada.
3. es difícil de modelar porque tendría que aislar el sistema
   térmicamente para obtener datos y tal vez nunca termine de exigir
   el disponer de datos. Es más, con el tiempo el mismo ángulo de pwm
   puede generar diferentes curvas de calentamiento i.e. se puede
   alcanzar el set point a distintos tiempos.
*** Objetivo de RL
Esencialmente es maximizar la recompensa esperada a largo plazo. Para
esto un enfoque es obtener una suma de recompensas. Así se puede
acumular pequeñas recompensas que acerquen al objetivo a largo
plazo. En este proceso, puede darse un sacrificio en donde no siempre
se busca maximizar la recompensa en cada paso si al hacerlo se puede
tener una recompensa a largo plazo. Esto es la planeación.

#+begin_quote
Todo objetivo puede ser expresado como la maximización de una
recompensa acumulada esperada.
#+end_quote

De ahí que es de interés describir los problemas y objetivos en
términos de la recompensa.
* 2025-03-11 Políticas
** Questions and keywords
- función de valor de acción ::
- intervalo de confianza ::
- target ::
- oldEstimate ::
- optimismo esta relacionado con el sesgo de inicio :: Entiendo que si
  existe un condicionamiento de condiciones iniciales, el problema de
  aprendizaje podria no converger? o era que el optimismo garantiza
  una exploracion inicial, pero si el problema es no estacionario
  i.e. varia con el tiempo, ya no va a buscar nuevas mejores acciones
  teniendo un mal desempe;o a largo plazo. En cambio si el problema es
  estacionario, el optimismo puede funcionar bien.
** Notes

*** Política Voraz (Greedy)
- no se puede utilizar en RL completo. Trata de explotar el
  conocimiento lo que le limita la capacidad de explorar nuevas
  acciones/politicas que mejoren a largo plazo ya que trata de obtener
  la mejor accion inmediata. Depende de disponer de una matriz de
  transicion de estados.
- una política maximiza la función de valor de acción
- greedy para una $A_t$ se aplica $1-\epsilon$
- así por ejemplo $1-\epsilon \right_arrow 80\%$ tomo una acción que
  maximice la recompensa es decir una accion muy conveniente
- el 20% de las veces $\epsilon$ hago algo nuevo (exploración)
- aleatorio significa que las acciones se toman con una distribución
  uniforme y todas las acciones con misma probabilidad
- la función de valor de la acción en un estado en tiempo $t$ es
  $Q_t = \frac{R_1+R_2+\dots+R_{t-1}}{t-1}$
- Para que $Q$ sea eficiente se plantea como una *implementación
  incremental del promedio*. Este promedio de recompensas da idea de
  $Q$
- $t$ se entiende como el número de veces que se toma una acción $a$
- Entonces el $Q$ queda como:
  $Q_{t+1}=Q_t+\frac{1}{t}(R_t-Q_t)$
- esto permite mantener en memoria valores anteriores
- en general, las estimaciones se entienden por:
  $N_{est} = Old_{est}+\alpha(Target - Old_{est})$
- el $\alpha$ está entre 0 y 1 y es el factor de olvido
- hay que definir cual es el margen de tiempo relevante para mi
  problema
- es una ventaja asumir $\alpha$ constante, porque de esa manera puedo
  hacer que en las elecciones del agente pese mas la informacion
  reciente que toda la informacion
- $Q(a) \neq 0$ inicializado con valor optimista ayuda a la exploración
- optimismo implica saber que es ser potimista en el contexto del
  problema i.e. conocer la recompensas del problema
- el optimismo funciona al inicio
- en proble no estacionarios optimismo no funciona bien

*** politica UCB extremo superior de confianza
- hacer estimaciones o calculos dentro de un intervalo de confianza
- ucb aprovecha la información que da la función de valor de la acción
  Q
- la siguiente acción es *potencialmente* la mejor
- el valor de Q se asume dentro de un intervalo de confianza, en un
  sentido optimista pegado al borde superior
- el denominador de la UCB da intervalo de confianza
- la UCB se define por
  $A_t = \underset{a}{\text{argmax}}(Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)})$

*** Algoritmos de Bandidos de Gradiente GBA
- usa soft max
- la funcion softmax da varias probabilidades
- se escoje la mayor
- la función de preferencia $H_t$ no está relacinada con la recompensa


*** Arrepentimiento
- se explica con el ejemplo de TikTok
- un arrepentimiento bajo garantiza en un tiempo crítico mantener al
  cliente enganchado
- crítico es rápido es decir que con un límite de iteraciones obtener
  el resultado

*** Bandidos Contextuales
- el ejemplo de bandidos se asemeja a cosas de marketing
- no solo se tiene información de la recompensa
- sino también se incluye información histórica de episodios pasados
  del usuario
- usar informacion histŕoica nos pone en la intersección entre
  aprendizaje por refuerzo y RL
- 
** Summary

* 2025-03-11 Procesos de Markov
** Questions and keywords
- cadenas de markov ::
- procesos con recompensa de markov MRP :: 
- procesos de markov MP ::
- planeación :: las acciones tienen consecuencias
- se puede modelar un ajuste P&ID de una planta con RL ::
- un virus con RL que podría hacer si el agente aprende a controlar el entorno ::
- filtros de kalman ::
- máquina de estado finito ::
- matriz de probabilidad de transición de estado :: $p_{ss'}=P[s_{t+1}=s'|s_t=s]$
- retorno ::
- no es claro el tema del valor del estado vs recompensa vs mi objetivo ::
- ley de esperanzas iteradas ::
- diagrama de respaldo :: 
** Notes
- bandidos es un problema incompleto con una sola acción
- en bandidos la única acción no influye en la recompensa
- accionar la palanca no afecta al futuro
- el problema del conejo lleva a recordar el problema de canivales que
  se usa con BFS
- el agente es el controlador
- el entorno es el proceso
- el entorno provee un feedback
- el agente aprende a controlar el entorno
*** Procesos de Decisión de Markov MDP
- El proceso de decisión de Markov esta compuesto de acciones, estado,
  recompensas
- la recompensa es algo que el diseñador modela y no es real
- hay varias variables que no se conocen. El RL se basa en cantidades
  estimadas
- En MDP:
  1. ningun elmento es conocido por el diseñador
  2. se trabaja con cantidades estimadas
  3. el ámbito puede ser observable completamente o no el
     entorno. Pero se considerará completamente observable en un inicio
- hay los siguientes procesos: 
- Procesos Markov que es una tupla <s,p>
- Proceso de recompensa de markov <s,p,r,gamma> gamma es el olvido hay
  una referencia a control
- proeceso de decision de markov es la tupla completa <s,A,p,r,gamma>
*** Procesos de Markov
- El estado actual contiene toda la información necesaria (propiedad de Markov)
- los procesos de markov son procesos sin memoria
- la probabilidad de estimar el mismo estado es la misma que la del
  acumulado de estados $P[s_{t+1}|s_t] = P[s_{t+1}|s_t, s_{t-1}, \dots, s_0]$
- el paso entre estados en un automata finito se la hace con
  probabilidades de transición
- cómo puedo estimar la matriz de probabilidad de transicion
- es el proceso el que me lleva a determinar las probabilidades
*** Proceso de recompensa de markov
- es una tupla de estados matriz de transicion recompensa y un factor
  de olvido, donde los estados es un conjunto finito, p la matriz de
  transición y la recompensa se define como una función que calcula el
  valor esperado dado que me encuentro en un determinado estado
- aun no se considera la acción pero de momento se obvia
- $\gamma$ es el factor de olvido en control adaptativo
- $\gamma$ es el factor de descuento con valores de 0 a 1.
- objetivo de RL es obtener la Recompensa maxima acumulada esperada
- el objetivo de RL se define como *retorno*
- los valores del problema de RL como el retorno se obtienen de los
  estados no de manera histórica
- el descuento balancea cuanto quiero tomar en cuenta el futuro
- $\gamma = 0$ es ser Miope hacia los valores futuros sólo me interesa
  lo que pasa en el siguiente estado no a futuro
- $\gamma = 1$trato de incluir lo más que pueda la información futura
- $\gamma= 1$ es como estimar lo más lejos en el tiempo pero esto
  genera problema para estimar adecuadamente. se vuelve mas incierto
*** El descuento
- desde un punto de vista matemático para evadir problemas de
  recompensa infinita es necesario tener un factor de descuento
- introduce modelado de la incertidumbre
- es un balance entre recompensas a largo y corto plazo (inspiración
  financiera o psicologica)
- $\gamma = 0.01$ implica centrarse en recompensas inmediatas
*** Función del valor del estado $v(s)$
- indica el valor a largo plazo del estado en el que me encuentro
- mientras más alto es mejor
- matemáticamente es el retorno esperado empezando desde el estado $s$
- Ejemplo
  - Se requiere una secuencia de estados
  - $\gamma = 0.5$
- se requiere hacer mas cadenas
- se toma el mejor valor posible de la esperanza (promedio)
*** Ecuación de Bellman
- se usa la ley de esperanzas iiteradas para
- el valor esperado se asume lineal
- el valor esperado no tiene un solo valor
- lo anterior se asume
- la Recompensa inmediata es la respueta del sistema (repsuesta a entrada paso?)
- esta ecuacion es ver un paso adelante
- es una estimacion o busqueda de un paso al futuro
*** Diagramas de Respaldo
- se usan para ver de manera grafica el proceso de estado accion recompensa
- lineas indican la trayectoria seguida
- burbujas que indican estados
- puntos negros que indican acciones
- Mediante el uso de algoritmos se trata de obtener los diferentes
  valores que describen el proceso
- la solución de los valores de estado se pueden resolver por medio de
  ecuaciones lineales
** Summary

* 2025-03-12 Procesos de Decisión de Markov
** Questions and keywords
- Decisiónes de Markov ::
- diagrama de respaldo :: ayuda a seguir el proceso que sigue el
  algoritmo. Cambia segun las acciones a tomar
- MDP  :: markov decision process??? 
- MRP :: markov reward process???
- El retorno ::  donde se definió
- demostracion de la ley de esperanzas iterativas ::
- recompensa inmediata :: $R_{t+1}$
- valor con descuento del siguiente estado :: $\gamma q_{\pi}(s_{t+1},
  A_{t+1})$ y $\gamma v_{\pi}(s_{t+1})$
- politica uniforme :: todos los estados tienen la misma
  probabilidad. Si tengo dos estados cada estado tiene 0.5 si son 3
  entonces 1/3
** Notes
- Los procesos de decision markov es una tupla comprendida de los
  estados, las acciones, la matriz de probabilida de transición de
  estados, la recompensa y el descuento $<S, A, P_{ss'}, R, \gamma>$
- Pss' será redifinida porque depende de la acción
- $P_{ss'}^a\mathbb{P}[s_{t+1}=s'|s_t=s, A_t=a]$
- La recompensa sera tambien en funcion del etado y la accion
- $R_s^a = \mathbb{E}[R_{t+1}|s_t=s, A_t=a]$
*** Políticas Estocásticas
Una política es una distribución de aciones dado un estado en el que
me encuentro en un tiempo t

$\pi(a|s)=\mathbb{P}[A_t=a|s_t=s]$
- La política define el comportamiento
- usa el estado actual o estado markoviano.
- son estacionarios en el tiempo $t$
- En cada episodio la política es invariante en el tiempo. por ejemplo
  puede variar $\epsilon$ greedy con el tiempo pero no la política.
- para un proceso de decision de markov MDP que consta de $<S, A,
  P_{ss'}, R, \gamma>$ + Política $\pi$ se redefine la matriz de
  transición y la recompensa como

  $P_{ss'}^{\pi} = \sum_{a \in A}\pi(a|s)P^a_{ss'}$
  $R_s^{\pi} = \sum_{a \in A}\pi(a|s)R_s^a$
*** Valor de Estado $v(s)$ y Valor de Acción Estado $Q$
Se redefinen con respecto a la política. $n$ es el número de estados y
$m$ el número de acciones

Función valor estado: es el retorno esperado del estado siguiendo la
policía $\pi$ (valor esperado) G es el retorno

$V_{\pi}(s) = \mathbb{E}_{\pi}[G_t|s_t=s] \in \mathbb{R}^n$

Funcion Valor accion estado> es el retonro esperado G despues de tomar
la acción $a$ en el estado $s$ usando la política $\pi$

$q_{\pi}(s)=\mathbb{E}_{\pi}[G_t|s_t=s, A_t=a] \in \mathbb{R}^{n \times m}$
*** Ecuación de la esperanza o valor esperado de Bellman
Es la recompensa del siguiente esatdo mas el valor condescuento del
siguiente estado

$v_{\pi}(s) = \mathbb{E}[R_{t+1}+ \gamma v_{\pi}(s_{t+1})|s_t=s]$

$q_{\pi}(s,a) = \mathbb{E}[R_{t+1}+ \gamma q_{\pi}(s_{t+1}, A_{t+1})|s_t=s, A_t=a]$


La secuencia que se ejecuta siempre es S -> A -> R es decir en un
estado tomo una accion y recibo una recompensa. Este bucle se repite.

Estando en un estado s y tomando una accion a puedo basado en la
accion puedo estimar el q del estado y la acción

$$v_{\pi}(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$$


$$q_{\pi}(s,a) = R_s^a+\gamma\sum_{s'\in s}P_{ss'}^av_{\pi}(s')$$
- dependiendo de las acciones que toomo el estado destino no siempre es fijo
- el entorno decide a que estado llego desde una acción
- no se sabe si las acciones me llevan a la misma respuesta
- la q pi sa se puede reemplazar en la v pi s
- revisar resultado en diapositaiva 9 de grabacion de 12 de marzo
*** Ejercicio 12 de marzo diapositiva 10
- politica uniforme
- $\gamma = 1$
- se busca hallar el valor en el estado c3
** Summary
* 2025-03-13 Programación Dinámica
** Questions and keywords
- problemas de evaluación MDP ::
- problema de control de un proceso de decsision de markov MDP ::
- porque la recompensa no es conocida :: pense que si era conocida
  porque ponemos el objetivo en terminois de la recompensa
- proceso iterativo ::
- equacion esperanza bellman ::
- ecuación optimalidad de bellman ::
- principio de optimalidad :: ???
- evaluacion sincrona ::
- evaluación en su lugar (in place) ::
- checkar grid world dynamic programing demo standord :: 
** Notes
- Un proceso de decisión de Markv tiene $S, A, P, R, \gamma + \pi$
- la recompensa y la matriz de transicion son desconocidas
- la evaluación de la política es cuantizar que tan buena es una política
- evaluación politica input: \pi
- output de la evaluacion politica v_{\pi}(s), q_{\pi}(s,a)
- la mejora de la política o control trata de encontrar una política
  óptima
- el problema de la ecuacion de esperanza de bellman es que tiene un
  costo computacional alto.
- La ecuación optimalidad de bellman es no lineal y no tiene solución cerrada
- para estimar la ecuacion de esperanza de bellman y la optimalidad de
  bellman es necesario procesos iterativos
- Bellman propone la programación dinámica para resolver
  computacionalmente las ecuaciones reduciendo el costo computacional.
- consiste la prog dinamica en dividir el problema ensubproblemas o suboptimos
- unir dos caminos optimos o rapidos debe garantizar una entrega mas rapida
- no es seguro que el optimo global sea la suma de los suboptimos
- la programacion dinamica consiste en
  1. dividir en subproblemas
  2. resolver subproblemas
  3. unir las soluciones
- condiciones para plantear la prog dinámica
  1. debe existir una subestructura i.e. el problema es divisible
  2. la substructura debe cumplir elprincipio de optimalidad
  3. los sub problemas deben ser superpuestos lo que permite re
     utilzar las soluciones i.e. son reusables e iterables
- El RL es:
  - estructurado porque tiene subproblemas o substructuras e.g. el
    diagrama de respaldo
  - puede ser iterado
- evaluacion sincrona tiene 2 copias de la v_{\pi} va de acuerdo con la matematica
- evalucion sicron aactualiza fuera del loop
- en su lugar es que cuando cambio uno de los valores inmediato cambio
  todos los valores. es mas rapida
- es riesgoso acercarse a estados con valores negativos
- 
*** Ejemplo numerico de calculo con PD
- In: $\pi$
- $\delta$ es un numero pequeño
- en estaos terminales se da el valor de $v(terminal)=0$ Un control de
  temperatura no tiene un estado terminal termina al rato de desconectar la maquina
- los valores iniciales de $v(s)$ pueden usar optimismo
- en el video de la clase grabada hay un ejemplo de mundo cuadricula
- en el mundo cuadricula el agente ha de aprender la manera mas rapida
  de llegar al estado terminal
*** mejoramiento de la politica
- cada que evaluamos iteramos sobre todos los estados
- y se actualiza la politica con la optimalidad de bellman
- es una alternacion entre evaluacion y control (actualizar politica)
- la manera mas sencilla de encontrar una politica optima es usar greedy
*** teorema de mejoramiento de la politica
dado $\pi$ en un proceso iterativo y una siguiente iteracion tal que
mi siguiente politica sea greedy $\pi' = greedy(v_{\pi}) =
\text{argmax}_{a in A}q_{\pi}(s,a)$ Al tomar greedy es como asumir que
la accion no influye y ase que V y q sean iguales
Es decir una vez que se fija la accion v y q son iguales

** Summary

* 2025-03-16 RL David Silver
** Questions and keywords
- Reward :: all goals can be described by the maximization of expected cumulative reward
- Goal :: select actions to maximize total future reward
- history :: sequence of observations, actions and rewards $H_t =
  A_1,O_1,R_1, \dots, A_t, O_t, R_t$
- agent :: something that acts on the environment
- state :: is the information used to detrmine what happenes next. It
  is a function of the history $S_t = f(H_t)$. It is a summary of
  all that has happened and helps to predict what comes next. State is
  any function of history.
- environment state :: the information within then environment and decides
  what comes next.
- agent_state :: it is the internal representation of the agent. a set
  of numbers that captures what is going on. It is information gathered
  by the agent to pick next action. It is function of history.
- markov state or information state :: significa que el estado
  siguiente depende exclusivamente del estado anterior, se puede
  obviar todos los estados anteriores i.e. $P[S_{t+1}|s_t]
  =P[S_{t+1}|S_1, S_2, \dots S_t]$
- full observability :: full observability could it be agent state is
  equal to environment satet?
- partial observability :: poker player. The agent state is different
  from the environment state. agent must build its ownstate representation
- policy :: how the agent picks its actions.
- value function :: how good is a state and or action. How much reward
  do we expect to get by being in a state or taking an action. It is
  the prediction of the **expected future reward**
- $\gamma$ :: discount factor
- model :: prediction of what the environment does next
- exploration ::
- explotation ::
- prediction :: evaluate the future given a policy
- control :: optimize the future. Find the best policy
** Notes
- Reward could be delayed
- Actions may have long term consequences. We need planing.
- all goals can b described by the maximisation of expected cumulative reward
- agent selects actions
- environment selects observations and rewards
- environment state is not visible to the agent
- Markov state the future is independent of the past given the
  present. El estado presente contiene suficiente informacion para
  poder hacer predicciones hacia el futuro: $H_{1:t}\to S_t \to
  H_{t+1:\infty}$.
- En un Markov State, once the state $S_t$ is known, the history may be
  thrown away
- a markov state is always the environment state
- what happens next depends on representational state
*** Agent componentes
- tiene una Policy, value function, model
- polocy is a map from state to action
- policy deterministic $a=\pi(s)$
- stochastic policy helps to make random explorative decisions
  $\pi(a|s) = \mathbb{P}[A=a|S=s]$
- the value function is the prediction of the expected future reward:
  $v_{\pi}(s)=\mathbb{E}_{\pi}[R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\dots|S_t=s]$
*** The model
- predicts what the environment will do
- it has a *State Transition Matrix/Model* $\mathcal{P}$ that predicts the next
  state i.e. the dynamics. $\mathcal{P}_{ss'}^a =
  \mathbb{P}[S'=s'|S=s, A=a]$
- it has a *Rewards Matrx* $\mathcal{R}$ predicts the next immediate
  reward $\mathcal{R}_s^a = \mathbb{E}[R|S=s, A=a]$
- it is necessary to have this matrices or data because it is the
  equivalent to model the environment.
- RL bases on model free problems
*** Categorizing RL Agents
- **Value based**
  - No Policy
  - It has Value function
- **Policy based**
  - Policy
  - no value function
- **Action Critic**
  - Policy
  - Value Function
- Model free: it has no model but represents the model through policy
  and or value function
- Model based: First episode make a model and also could use POlicy
  and Value functions.
*** Problems in RL
- Learning and Planning
- RL: first tires to learn how the model works and then goes to planing
- Planning: the model of the environment is known
- Exploration and Explotation: RL is like trial and error
  learning. The agent should discover agood policy.
- exploration: give up some reward to get more informatio about the environment
- exploitatino: exploit the informationknown to maximise reward
- In RL we need to solve the **prediction problem** to **(control)
  optimize the policy**
** Summary
*** Introduction
El aprendizaje por refuerzo es una especia de trial and error i.e. de
aprendizaje por error. Para esto, el agente de RL tiene que que
observar el *entorno*, tomar una *accion* y recibir una respuesta a
traves de una señal de recompensa. La *politica* define el
comportamiento del agente i.e. como escoge las acciones. De ahi que el
agente forma una representacion interna que obtiene el agente. Para
poder realizar esto, el objetivo a alcanzar por el agente debe ser
expresado como la maximización de la recompensa esperada. De ahi que
el agente utiliza como componentes: 1. politica, 2. una funcion de
valor del estado/accion y 3. modelo. La idea de RL es obtener la
*politica optima* es decir aquella que nos conduzca al objetivo con la
mayor recompensa posible. La policy es un mapa de los estados y las
acciones. Puede ser determinista, donde una accion se deriva de un
estado especifico o estocastica, en donde se puede tomar decisiones
explorativas. Un estado puede ser del entorno o del agente. El del
entorno es desconocido para el agente. El estado del agente tiene la
representacion que hace el agente del entorno luego de explorarlo. La
funcion de valor accion estado estima la accion o estado enfuncion de
la recompensa y permite con sus valores decidir o predecir que accion
tomar luego. Predecir el siguente estado es el problema de prediccion
mientras que optimizar que hacer a futuro es el problema de
control. En RL, los agentes enfrentan varios problemas como el de
exploracion vs explotacion. La exploracion es que tanto puedo
sacrificar mis recompensas inmediatas para poder descubrir
alternativas nuevas que a largo plazo me conduzcan a una mayor
ganancia. La explotacion es en cambio obtener la maxima recompensa a
partir del conocimiento que se tiene. 
* 2025-03-18 Renta de Autos Jack
** Questions and keywords
- Poisson ::
- Funcion de Probabilidad FP ::
- no me queda claro como leer los diagramas ::
- no me queda claro porque es la optima :: dice que en 4 iteraciones
  se obtiene la poltiica optima y que se vio en clase en cual?
** Notes
- cada lote tiene un maximo de 20 carros
- esta relacionado a la optimizacion lineal entera
- accines rentar y devolver
- accion es mover los autos
- por cada renta gano +10
- **maximizar rentas**
- si no tengo autos para dar a una persona por no disponer de autos
  pierdo (recompensa negativa)
- se puede mover los autos entre las dos pilas con R = -2
- el algoritmo ha de aprender la distribucion de autos se requiere en
  cada locacion
- se usara una funcion de probabilidad discreta Distribucion de
  Poisson
*** Modelado
- Se hara un modelado del #de rentas/devoluciones
- $\lambda$: es valor esperado y varianza de la distribucion de poisson
- $\lambda$: cuantos personas rentan un auto en localidad 1
- $n$: numero de rentas o devoluciones
- Funcion de Probabilidad FP:  $P(X=n)=\frac{\lambda^{n}}{n!}e^{-\lambda}$
- Se asumen conocimientos previos por acopio de datos:
  - Renta loc 1: $\lambda = 3$
  - Renta loc 2: $\lambda = 4$
  - Retorno loc 1: $\lambda = 3$
  - Rentorno loc 2: $\lambda = 2$
- Truncamiento de la función para valores mayores que 11 la prob es 0
  i.e. mas de 11 solicitudes
- maximo de numero de movimientos de local 1 a local 2 es 5.
*** Entorno
- acciones: que tantos carros puedo mover. Es un vector. + de loc 1 a
  loc 2 y - de loc 2 a loc1.
  $acciones  = [-maxMov, \dots, -2,-1,0,1,2,\dots, maxMov]$
- estados es un vector que contiene el numero de autos por localidad $estados = [cars_1, cars_2]$
- la funcion de valor sera un vector i.e. el valor de estado es una
  matriz de  $20 \times 20$ que contran todos los valores de estados
*** Politica
- Ida basica de la politica: Si yo tengo /x/ num de autos en loc1, entonces debo tener /y/ en loc2
- $\pi_0$ politica inicial ningun auto se mueve
- los ejes son el numero de autos en cada locacion
- la 4 es la politica optima
* 2025-03-18 Métodos Monte Carlo e Iteracion de Politica
** Questions and keywords
- P :: matriz transicion
- GPI :: Iteracion de poltica generalizada
- PD :: Programacion denamica
- en su lugar tiene 2 copias? ::
- cual es el espacio de stados del ajedrez ::
- MCMC :: Metodos Monte Carlo
- aprendizaje por fuerzo libre de modelo :: es un RL con Montecarlo
- experiencia ::  son los datos que obtengo del proceso u entorno
- el retorno esta relacionado a la recompensa ::
- MC :: montecarlo
- MC 1era visita ::  utiliza solo el valor de la primera vez que uso
  un estado
- MC cada visita :: voy a reemplazar el valor anterior cada vez que
  visito ese estado. cada visita actualizamos el vaor cada vez que
  pasamos por el estado
- First Visit MC :: 
** Notes
- Metodos de Montecarlo: no se necesita asumir P, R
- Iteracion de politica: asume que conocemos P, R
- Iteracion de valor: asume que conocemos P, R
*** Policy Iteration estimacion de $\pi$
- se divide en dos partes Policy Evaluation y Policy Improvement
- $\Delta<\theta$ da una estmacion buena de V(s)
- Solo cunado obtengo la buena estimacion aso a policy improvement
*** Iteracion de valor
- toma una sola iteracion del bucle pero devuelve solo el valor que
  maximiza la funcion i.e. la que en esa iteracion con esa politica da
  el mejor valor de V
- es combinar la parte de prediccion y conrol en un solo paso
- para balancear entre iteracion de politica e iteracino de valor para
  no solo maximizar constantemente y hacer una sola iteracion o gastar
  computacionalmente haciendo todo el proceso computacional se usa
  GPI> iteracion de politica generalizado
*** Algoritmos sincronos de Programacion Dinamica PD
- Esperanza debellman hace prediccion y se llama algoritmo de
  Evaluacion iterativa de la politica
- Valor esperado de bellman + politica greedy permite hace prediccion
  y control. Esto se denomina iteracion de politica
- la ecuacion de optimalidad de bellman es el algoritmo de iteracion
  de valor. Como hago una iteracion y tomo el max hago solo control.
- La complejidad de todos tres algoritmos en terminos de tiempo es
  $O(mn^2)$ con
  - m numero de acciones
  - n numero de estados
- escalan peor que cuadraticamente
- Si se considera q $O(m^2n^2)$
*** Programacion Dinamica Asincrona
- En todos los algoritmos se realiza un barrido de los estados o del
  espacion de estdos
- En prgramacion dinamica asincrona  los estados se actualizan en
  cualquier orden y con a informacion disponible en t
- P.D. Asincrona es el equivalente a actualizar V(s) en su lugar
- PD asincrona reduce el computo. estrictamente necesario para
  problemas reales
- para que exista convergencia es necesrio que **visite** todos los estados
- Ejemplos:
  1. Juego d eestrategia con varios batallones de un ejercito: no
     visitar la reserva si estoy lidiando con los soldados de forntera
  2. Ruta mas rapida
  3. arreglo de desgaste de piezas. e.g. una pieza que se da;a a cada
     rato vs cada a;o
- los estados importantes se aprenden no se definien
*** Cosas que permite la PDA
- la PDA permite visitar los estados mas importantes
- se puede ignorar casi completamente ciertos estados no relevantes
  para el problema
- dada una politica actualizar los mas los estados que la politica visitada
- algunas partes del problema cambian mas lento que otros
*** Metodos MonteCarlo
- su idea principal es estimar distribuiones que no conocemos de
  manera eficiente
- los proceso a aprender asemejar a los imanes a un arreglo de imanes
- se cubre los imanes on una hoja de papel
- aplicando acciones y montecarlo se va dsecubriendo como opero
- distribuyendo uniformemente el material correcto
- montercarlo permite estimar magnetic field lines
- montecarlo es dar un monton de acciones y ver la respuesta del
  prceso esto me permite ocnoncer coomo se ocmporta
*** Condiciones del Metodo Monte Carlo
1. Generar los Input con la Politica
2. Observar los output es decir las salidas delsistema. La unica
   salida que tengo del sistema son R las recompensas
3. procesar
4. se requeire muchos datos apa que funcione bien
5. condiciones extra: elegir los inputs de manera uniforme
   i.e. uniformemente en el sentido probabilisto de la palabra es
   decir cada acciondebe ser elegida conlamisma probablidad
*** Estrategia libre de modelo
- Implica que P y R son desconocidas
- El conocmiento de P y R se reemplaza con experiencia
- experiencia viene de la exploracion del entorno
- por tanto se van a recoger datos y estimar ciertas cantdades y se
  obtendra la expericina
- La Recompensa se puede ver
- Metodo de Montecarlo permite aproximar el Retorno
- MMC sonbuenos para aproximar Esperanzas, promedios
- Con MC se da cadenas de estado
- se obtiene un promedio de los estados visitados
- y ese va ser los valore de estado
*** First visit MC
1. Iniciar una politica
2. estimar los retornos
3. generar un episodio siguiendo la politica i.e. interactuar conelentorno
4. una vez que tengo las recompensas se hace un bucle para cada uno de
   los pasos del episodio de T pasos
5. calculo el retorno
6. obtengo el promedio o media de los retornos que ouede ser una media
   incremental

note que el bucle va de reversa desde el ultimo paso hasta el
primero. esto se hace por eficienca. 
** Summary
* 2025-03-20 Control MC en y fuera de la politica
** Questions and keywords
- en que orden se hace evaluacon y mejora ::
- para que funcione MC :: debo visitar todos los pares (s,a) para
  garantizar la exploracion al menos una vez
- politica determinista :: no sucede la exploracion?
- algoritmo de inicions que exploran (Exploring Starts ES) :: se
  generan cadenas que voy a seguir pero el primer estadoes random y
  uniforme i.e. misma probabilidad para todos los pares de estado
  accion. en un bucle eventualmente se visitaran todos los etados.
- en politica  :: es aprender haciendo i.e. aprendo la politica pi
  usando la experiencia de la politica pi.
- fuera politica  :: es copiarle a otro. Voy a copiar la politica
  optima. Es decir, aprender la politica pi usando la experiencia de
  otra politica /b/. Esta estrategia es la que se usa por default
  - $\pi$: politica objetivo
  - $b$ politica de comportamiento
- cobertura :: cuando se trabaja fuera pi hay que asegurar la
  cobertura. La cobertura dice que cada par (s,a) que pueda ocurrir
  enla Pi target debe ser eexplorado al menos ocasionalmente por la
  politica de comportamiento (i.e. la que estoycopiando) la /b/
- Politica :: es una distribucion es una probabilidad. Recordar
** Notes
- en esta clase se realiza control
- se estudia conceptos de hacer control dentro y fuera de politica
- muestreo de importancia
- recordar que P, R no son conocidas se obtienen de explorcion
  mediante montecarlo que devuelve valores esperados
- la exploracion es requerida cuando no se conocen P y R ya que greedy
  explota el conocimiento por lo que no es muy real
- hacer gpi es decir una o muchas iteraciones para hacer evaluacion y mejora
- no se puede hacer GPI en el porblema completo de RL. porque se puede
  maximizar la funcion de valor estado con respecto a la accion porque
  P y R son conocidas
- $\pi'(s)$ politica luego de una iteracion
- $\pi'(s) = \text{argmax}_a[Q(s,a)]$
- hallar valores y registrar en memoria
- El espacio accion estado (s,a) es mayor siempre que el espacio s
- recordar la condicion para que funcione montecarlo
*** Exploring starts
- 1 a 4 es la inicializacion
- iniciar una politica que inice ddesde cada estado
- retornos vacios
- linea 12 acumula los retornos i.e. un array en 3 dimensiones
- linea14 es la mejora
- lineas 8,9,10,11,12 es la evaluacion su corazon esta en 10
- linea 11 es la primera visita
- si se omite la linea 11 ser[ia en cambio el algoritmo de Cada visita
- Es siempre posible sobre todo enproblemas reales escoger un inicio
  al azar??? : haycasos en donde es imposible aplicar por eemplo n un
  auto no se podria iniciar sinprender elcarro . ES es como el
  optimismo,sise puede selo ponepara que ayude
*** Politicas $\epsilon$-soft
- las polticas greede son un subconjunto de epsilonsoft
- es una version mas general de $\epsilon$- greedy
- es una politica estocastica que asegura que la probabilidad de
  tomaruna accion dado un estado es mayor que 0 para todas las
  acciones. es decir que todas las s,a van a pasar dadas suficientes iteraciones
- Defnicino: $\pi$ es $\epsilon$-soft si todos los pares s,a tienen al
  menos $\frac{\epsilon}{\Delta(s)}$ de prob de ocurrir
- es decir que con cierta probabilidad se asegura que todas las
  acciones de todos los estados
- cada estado tiene un conjunto diferente de acciones
- el deltas es la carrdinlaidad del conjunto de acciones de un estado
- sobre el algoritmo este requiere inicializar arbitrariamente una
  politica epsilon soft
- cuando voy a usar MC genero un episodio
- si quito la linea de primera visita se tiene el algortimo de CADA
  VISITA
- la politica epsilon greedy es optima?
- las politicas epsiolon soft no son optimas.... ?siguen explorando
  luego de allar un optimo?
- epsion soft luego de varias iteraciones sigue explorando
- es mas si el porcentaje exploracion es bajo sigue explorando asi sea
  un valor peque;o
*** Control en Politica y Fuera de Politica
- son maneras de obtener la politica optima es un paradigma
- la politica optima es $\pi_*$
- control en politica es todo lo que hemos hecho hasta ahora
- ver en keywords
- por default se usa politica fuera de politica:
  1. xq se deja la exploracion a la politica b y la politica $\pi$
     alcanza la optimalidad. esto es una ventaja
  2. podemos ganar experiencia de lo que otro agente hace. el agente
     puede ser una persona.
  3. permite reusar la experienciad e politicas antiguas
  4. usando una sola politica de comportamiento /b/ podemos entrenar
     varias politicas optimas u politicas objetivos $/pi$
- no free lunch: las desventajas de Fuera de Politica
  - Problema de la cobertura
  - si $\pi(a|s)>0$ implica $b(a|s)>0$
  - si eso no sucede no estamos explorando o notenemos datos de
    /(s,a)/ yno sabemos que ocurre si tomamos este par
*** Muestreo de Importancia (Importance Sampling)
- Ejemplo: estimar la altura de personas enla provincia del guayas
- segun profesor no eslogico usar los datos de una;o anterior para
  estimar los del a;o 205
- es una tecnica estadistica/probabilista para estimar esperanzars o
  valores estimados de una distribucion dada usando otra distribucion.
-  demistracin en el video de muestreo de importancia fuera de Pi

** Summary

* 2025-03-25 Aprendizaje Diferencia Temporales (TD $\lambda$)
** Questions and keywords
- Sortear los laboratorios
- donde esta el temario?
- semana para entregar tareas
- $C(s,a)$ es un acumulador
- /b/ es unapolitica on un a cobertura de $\pi$
- que es hacer control y que es hacer estimacion
- juego resuelto :: en el que el computador siempre gana e.g. ajedrez bagamon
- bootstraping :: relacionado a modelo de hopfield
- modelo de hopfield :: grafos
- montecarlo es un subconjunto o caso especial de diferencias temporales
- TD :: diferneicas temporales
- $\alpha$ :: constante en el TD
- lamina Acerca del Aprendizaje TD :: preguntas de la evaluacion siguiente
- bootstrapping :: apoyarse de algo más sencillo para ejecutar algo
  más complejo
- overfitting :: sobreajustando los parametros del modelo. no se
  generaliza bien el modelo a los datos. varianza alta. overfitting
  tiene que ver con varianza alta. 
- underfitting :: el modelo necesita mas no linealidad. Sesgo alto se
  relaciona con underfitting
- vision unificada de RL ::
- busqueda exhaustiva :: es recorrer todo el espacio de estado
** Notes

- la clase se enfoca en TD0
*** Muestreo de Importancia
- se basa en encontrar una razon entre la politica y la b i.e. $\rho = \frac{\pi(x)}{b(x)}$
- tomo una politica que me dice unas probabilidades sobre las acciones
  y obtengo un arreglo de recompensas $R=[1,3,1]$ Se toma la accion 1
  dos veces y la accion 3, una vez. (1,85%), (2,5%), (3,5%), (4,5%)
- b es la politica d exploracion o comportamiento
- supongamos otra distribucion de probabilidad en que la politica toma
  40% del tiempo tomoa la accion 2, la 1 el 30%, 3 el 10%, 4 el 20%.
- Para obtener el retorno con la política $\pi$ seria el promedio de
  recompensas con la politica d ecomportamiento?

  $G_{\pi} ¿ = 1/n\sum Rp$
- La politica de comportamiento la pone el diseñador y sabe cuantas
  veces sucede cada accion.
- usando la politica /b/ de comportamineto se obtiene los retornos $G_b$
- $\rho$ es un ajuste
- los pasos a diferentes estados se analizan como la probabilidad de
  la accion dado el estado $\rho(A|s)$.
- la $\rho$ acumulada es la multiplicacion de los $\rho$
  $\rho_{t:T-1}=\prod_{k=t}^{T-1}
  \frac{\pi(A_k|s_k)}{b(A_k|s_k)}=\rho_1 \rho_2 \rho_3 \rho4$
- en /V(s)/ se divide por elnumero de veces que pase por cada
  estado. $\rho$ es difernte para cada estado. no se divide para la
  longitud completa del episodio. $\tau$
*** Dilema del sesgo/varianza o entre el bias/varianza
- imagine dos conjuntos de datos uno para /b/ y otros para /pi/. los
  datos van a arrastrar hacia el /b/ 
*** Muestreo de importancia ponderado
- si solo tengo una muestra , la estimacion del estado devuelve el retorno
- hay un sesgo hacia el cojunto de datos de b
- $v(s) = \frac{\rho_{t:T-1}G_t}{\rho_{t:T-1}}$
- El muestreo de importancia ordinario estima $v_{\pi}(S)$ y tiene una
  alta varianza y si la cadena de estados no es la suficiente larga la
  estimacion del valor puede divergir
- El muestreo de importancia ponderado esta sesgado hacia /b/ al
  inicio. pero con mas iteracines converge a $v_{\pi}(s)$ y tiene baja
  varianza. los valores van a estar cercanos uno del
  otro. i.e. varianza se va a 0 conforme mas iteraciones y por tanto
  converge al valor esperado. Por esta razon se usa en la practica
*** Control con MC
- Control implica que la poltica va a estar cambiando
- la idea de **cobertura** es si existe la probabilida dde que pase en
  $\pi$ tambien debe pasar para o en la politica /b/
- control con MC no es effectivo
  - es lento en conveger
  - valores de /q(s,a)/ tienen alta varianza
- Montecarlo es para sólo para tareas episódicas. pero no todas  las
  tareas son episódicas.
- montecarlo no usa la informacion disponible inmediatamente
  $G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3} + \dots$
- el problema de que no explote la informacion es que no podria
  funcionar conectado a sistemas en linea
- Montecarlo espera al finaldel episodio para actualizar valores
- TD reporta observando cada estado
- TD hace peque;as actualizaciones para llegar al valor final
- 
*** aprendizaje por diferencias temporales
- funciona con episodios incompletos
- esta relacionado al bootstrapping
- hace peque;as actualizaciones en vez de MC que espera al final del episodio
- TD(0) usa la recompensa inmediata luego de tomar una acción
- en el algoritmo de TD el parametro $\alpha$ es importante
- cada paso que se da de cada accion se actualiza el valor de v
- a pesar de que tengo dos bucles uno de episodios y dentro de ese los
  pasos ya o tengo que calcular retornos.
- ya no necesito de preocuparme de si fuera de politica o si organizar
  los valores desde t-1. sino que se hace constantemente la actualizacion.
- TD usa el concepto de bootstraping
- bootstraping usa el estimado de /V(s')/ para hallar /V(s)/
- TD aprende sin necesidad de ver el resultado. no necesita llegar al
  otro estado sino observar. es muy conveniente
- TD puede aprender tareas sobre la marcha i.e. aprendizaje online
- TD puede aprender tareas no episodicas
- TD puede aprender tareas continuas e.g. control de temperatura
- tD puede aprender de secuencias de stado incompletas
- TD converge mas rapido que MC
- 2025-03-27
- no usa el retorno sino estimacion del siguienteestado
- es como cortar a montecarlo
- *caminata aleatoria* usa una politica random
  - si usa MC tarda mucho en aprender por la exploracionque tiene que
    hacer ya que **actualiza completamente al final de cada episodio**
  - al usar TD, como *actualiza en cada paso del algoritmo*, el tiempo
    es menor
- el parametro $\alpha$ en la aproximacion incremental para TD esla
  velocidad de convergencia
- en montecarlo de igual manera si $\alpha$ es alto se converge mas
  rapido pero con mas error
- si $\alpha$ es bajo, voy a converger mas lento pero con menos error
- POr que parece que TD se acerca mas rapido al verdadero valor?
  revise la curva de $\alpha=0.5$. Solo significa que MC es mas lento
  no erróneo
*** Sesgo varianza
- que tiendo a tomar mis decisiones en funcion de la politica b
- se analiza en esta seccion en su totalidad wrt a MC y a TD
- esta relacionado con *overfitting* y *underfitting*
- en Montecarlo el retorno tiene bastantes datos de cada episodio por
  loque se relaciona con el underfitting es decir un sesgo bajo
- en TD como no ve los datos sino un estimado de los datos tiene sesgo
- en TD tiene baja varianza
- en TD, si el modelo cambia un poco yo puedo ajustarme por labaja varianza
- es necesario estar entre overfitting y underfitting.
- revisar tabla del dilema sesgo/varianza 2
- Montecarlo sesgo vs varianza
  - tiene alta varianza y bajo sesgo al iniciar
  - buena convergencia
  - insensible a las condiciones iniciales
- TD
  - tiene algo de sesgo al inicio
  - mas eficiente
  - es mas sensible a las condiciones iniciales
*** Eficiencia y Utilidad TD para RL
- MC converge a la solución con mínimo error medio cuadrático RMS
- MC lo que hace es el mejor aproximado de los retornos en el sentido
  de minimos cuadrados
- TD0 converge ala solucion de maximo verosimilidad MLE
- TD0 implicitamente halla el mejor vector de parametros $\theta$ para la
  funcion probabilistica $(\cdot| \theta)$ que explica mis
  muestras. esto consiste en maximizar el logaritmo de la
  distribucion.
- TD0 al ser un modelo probabilistico explota de mejor manera la
  propiedad de markov
- TD0 se ajusta a los modelos de RL
*** vision unfiicada de RL
- son dos ejes muestreo vs bootstraping
- la idea de bootstraping es actualizar usando un estimado
- la exploracion completa de un episodio hace que TD0 pase a MC
- MC no hace bootstraping ya que no hace estimaciones. mira todo el episodio
- la idea de muestreo es obtener muestras mediante la exploracion y
  estima un valor esperado. MC sí saca valores esperados. PD no saca
  muestras. TD sí saca muestras.
- busqueda exhaustiva casi que nunca recomendable

** Summary
* 2025-03-27 
** Questions and keywords
- sarsa :: para control libre de modelo
- q learning :: para control libre de modelo
- idea del q learning ::
- completar el proyecto hasta el 2 semanas despues de la proxima clase
  del martes:: 
** Notes
- Estaremos viendo algorimos: Sarsa y Qlearning
- la construccion del sistema es lo mas laborioso
*** Control con Aprendizaje TD
- SARSA aprendizaje en política
- Aprendizaje Q es para fuera de politica
- en politicas sin modelo necesito exploracion
- hay iteraciones por vlor e iteracion por politica
*** Sarsa
- estado accion recompensa estado accion siguentes
- considera transiciones s a s' y A a A'
  $Q(s_t, A_t)=Q(s_t, A_t)+\alpha[R_{t+1}+\gamma Q(s_{t+1}, A_{t+1}) -
  Q(s_t, A_t)]$
- Trabaja con el estado actual $Q(s_t, A_t)$
- target : $R_{t+1}+\gamma Q(s_{t+1}, A_{t+1})$
- error es la diferencia con Q a tiempo t
- SARSA cuando llega al siguiente estado solo ve el valor estimado de
  la siguiente accion no ejecuta la siguiente accion
- en el algoritmo el paso 8 que usa greedy es la mejora
- en el algoritmo el paso 9 es la evaluacion. esta al revez que
  normalmente evaluamos valores iniciales y de ahi mejoramos
- esto es porque sarsa requeire del siguiente estado
- sarsa observa *solo una** accion y tomo su estimado
- 
*** Aprendizaje Q
- en aprendizaje Q de todo elconjunto de acciones tomo la que maximiza
  Q. esto se conoce como **idea q learning**
- sarsa esta relacionado conla ecuacion de valor esperado de bellman y
- Q esta relacionado con la ecuacion de optimalidad de bellman
** Summary

* Sutton Book
** k-armed multi bandid problem
- greedy :: cuando se selecciona la accion cuyo valor es el mejor
- explotacion :: usar el conocimiento de los valores de las acciones
  para maximizar beneficion o recompensa esperada en un paso
- exploracion :: seleccionar una accion no greedy. puede producir la
  mayor recompensa total a la larga.
- how do they use the law of large numbers :: $Q_t(a) \leftarrow q(s)$
  para que converga al valor verdadero
- greedy :: seleccinar la accion o una de las accnes con el mas alto
  valor estimado del valor de la accion en un paso t $A_t =
  \underset{a}{\text{argmax}} Q_t(a)$. Una accoin gredy simepre explota el
  conocimiento disponible o actual para maximizar la recompensa
  inmediata
- Cada accion tiene un valor experado o medio de recompensa
- $A_t$ :: Accion tomada en un tiempo t
- $R_t$ :: Recompensa correspondiente a $A_t$ en /t/
- $q_*(a)$ :: Valor esperado de una accion /a/ $q_*(a) \doteq \mathbb{E}[R_t|A_t=a]$
- $Q_t(a)$ :: valor estimado de la accion
- Para estimar $Q_t(a)$ (i.e. el valor de la accion) partimos de que
  se la puede definir como el promedio de las recompensas obtenidas de
  la accion seleccionada
  #+begin_export latex
  \begin{equation}
    \label{eq:def-action-value}
    Q_t(a)=\frac{\text{suma de recompensas cuando se toma}}{\text{numero
      de veces que se toma a}} = frac{\sum R_i\cdot A_i}{\sum A_i}
  \end{equation}


  #+end_export
- El conflicto entre exploracion y explotacion, en control se llama el
  conflicto entre estimacion y control
- La estimacion del valor de una accion $Q_n$, luego de ser
  seleccionada $n-1$ veces se define :
  $$Q_n \doteq \frac{\sum_{i=1}^{n-1}R_i}{n-1}$$
- La implementacion incremental de $Q_{n+1}$ es ::
  $$Q_{n+1}=Q_n+\frac{1}{n}[R_n-Q_n]$$
- $\alpha = \frac{1}{n}$
- En general, la regla de actualizacion es:  $NewEstimate \gets
  OldEstimate + StepSize[Target - OldEstimate]$ 
** Markov Decision Process
* TODO [57%]
- [X] Ingresar al Moodle y revisar bibliografía y Temario
- [ ] Implementar el laboratorio o hacer el proyecto
- [X] Atender clase de bandidos en aula virtual
- [X] Responder las preguntas de la semana 1
- [X] Solicitar en el foro la demostración de la implementación
  incremental de Q
- [ ] solicitar una descripcion del diagrama de respaldo de los
  ejemplos no sé que está modelando parece un estudiante.
- [ ] solicitar revisar el cuestionario semana 2
